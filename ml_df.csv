subreddit,body
MachineLearning,"Bottom line is, not everyone can do what IT and CS does. Computer science is a 4 year degree for a reason. Go ahead and shove scientists, engineers, and accounts into java bootcamps. Some will swim, but there will be some who sink right to the bottom. Don't feel bad about maintaining a script that does all the work for you. As far as my customers are concerned, I'm a warlock who has mad a pact with an silicone rock to keep everything working. Don't kick back though. Maintain your competency. Get new certifications."
MachineLearning,"You are not alone with this. Because of the ease of internet access, things are moving very fast and everyone wants a piece of it. So, if you really want to learn ML I can advice you the following -

1) Read and learn from the following three books in the given order -

1.1) Book: Introduction to Probability by Dimitri Bertsekas and John Tsitsiklis
Download link: http://vfu.bg/en/e-Learning/Math--Bertsekas_Tsitsiklis_Introduction_to_probability.pdf

1.2) Book: An Introduction to Statistical Learning by Robert Tibshirani
Download link: https://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf

1.3) Book: Probabilistic Machine Learning: An Introduction by Kevin Patrick Murphy
Download link: https://probml.github.io/pml-book/book1.html

2) After reading and learning from the last book. Start coding the concepts from scratch in both R and Python. The easiest way to do is to break a concept into simple divide, multiple, add and substract. You can do this by solving a numerical for the concept of interest just like you must have done in college or school. Perform a google search for numerical problems associated with a particular ML concept, let's say regression, and see how it is done step by step. Do the same thing in R and Python and later try to make your code more general so that it could work on any data input.

3) Learn SQL, from a course or a book, doesn't matter. But learn SQL and do a small project on it.

That's all. It will take you around 6 months to do it all. If it takes you 2 years or more then don't get disheartened. Build your base, try to see the common sense behind the concepts which you are learning and then they will become more obvious and natural to you with time.

Lastly, github, docker, hive etc. are just tools. Some people need them, so they use them and vice versa. These tools are good to have and trust me github is awesome, but only if you need it. 

Also, don't learn what you will not use and use what you have learned. Otherwise it is very easy to forget everything. 

Machine Learning is not hard but because of so much attention it is getting, during any job interview you are expected to have the skills equivalent to a statistician with 10 or more years of experience. Which is ridiculous in a way. Because once you will get the job you will realise that most work will be mechanical. People still use linear regression more than ANN because it is simple and it works in most situations. So why use ANN.

All the best."
MachineLearning,"This sounds familiar to me. Feeling like an imposter is a known thing. The truth is you are doing exactly what you should be doing at this point. What's next is still in the future for you, but you will inhibit your progress of you don't appreciate your abilities. You're fine, doing things others only dream about. The fact that you don't value it only shows it's easy for you. That doesn't devalue it. It's clear by your comments that you are destined for more. Lighten up and keep an open mind and heart. You're not done by any means. Be optimistic. You are unique, valuable and still growing. Relax, you have decades ahead."
MachineLearning,OP is dumb...jeez.
MachineLearning,This is getting so old. Please verify my skills reddit! Please tell me my work is hard. Ugh! And the sad thing is it's working.
MachineLearning,"Soo true. Sir the points of a story are not units of time, yeah but how long will it take?"
MachineLearning,"You’re currently in tech limbo. You know enough to understand things and do damage , but you don’t know your stuff well enough where you can build it on your  own. You need to keep practicing. If you can understand other people’s work, you are in a good place. Other people’s work can give you sufficient examples to draw from. If you’ve seen it once, you can understand it. The trick to no longer being an imposter, is for you to be able to build similar things that others have done, but by yourself (or nearly by yourself).

Do some more practice exercises, know your vocabulary better, and learn how to explain the major concepts. The great thing is, the better you get at it, the more confident you become, and the more fun it is, and the more you get to really apply your skills. Your work truly starts to become your craft, and your craft becomes your art.

Good luck!"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Also, most jobs in tertiary are bullshit so relax"
MachineLearning,"&gt;Can someone hand you a data set and you can train a model with it without going to stack overflow for every single line of code? Congrats, you are in the top 5%! 

Importantly, it's also fine to use SO for it as well. Sometimes you forget the specific syntax of something. If you understand what the code you are cribbing is doing, that's already waaaay better than most people currently trying to do ML."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yeah most smart people can learn what you know in 3 months. BUT in your workplace there arent many smart people for this area. 
Think like this, they are monkeys that excel is super duper nice. You are a god among them.
Nothing will change that, nothing."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Attention is all you need, is not all you need - is all you need."
MachineLearning,Can’t wait to try this out .. have a few titan rtx’s for processing! Looks rad
MachineLearning,"absolutely yes. we work in the medical imaging domain, and we implement virtually all new methods that are related to detection/segmentation with VERY noisy labels and self-supervised pre-training"
MachineLearning,"Congrats, you're a tool in the toolbox guy. 

Interviews with PhDs will be excruciating because academics will hate you and interviews with MBAs will be mind numbing because they don't understand you.

Good luck. If you come up with an answer to this problem let me know but I've stopped apply to data science jobs because of this experience and just weave it in to my current gig when it's convenient for me."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Why don’t you create your own projects and become the person who other people copy?
MachineLearning,"Yea just 2.5 years ago I was hardly getting into stats and had just taken my first regression class and learned basic R.

I started getting into deep neural nets not too long ago and 5 years ago I thought the people doing this were genuises! Its only now I’m like wow this is easier and less fancy than I thought."
MachineLearning,I suggest you go to YouTube and watch a few videos of GaryVee. It will help you get more perspective. Hope it helps!
MachineLearning,"I generally start with tried and true methods, but once I've hit the limit I start exploring the latest methods (and inventing new approaches) if there is budget and a business case for it."
MachineLearning,"This. I'm in that ""match making"" industry. Interviews are dehumanizing cage matches. Here is my honest advice. Turn down your work intensity 25%. You are obviously knocking it out of the park there.

Next, plan to use that 25% of your time blocked off in calendar regaining or learning those new skills.

Get a certification if possible. Write papers if possible. They all give you an edge over other candidates. Train for interviews like you would a marathon.

Finally, get a new job. The best time to get a new job is when you have a job. The second best time is when you dont.

4 years is an eternity, and you wont learn staying put.

Good luck, fren"
MachineLearning,[deleted]
MachineLearning,"Well I've been studying machine learning for four years and the only thing I really understand is linear regression. I can follow a tutorial and get the code working even if it requires some tweaking, but I cannot get similar results with another toy data set. 

Machine learning is extremely difficult and I don't think anyone is going to push the envelop unless they are a math genius. 

My interest in artificial intelligence has inspired me to learn a lot more math, some statistics, Python and R, and a lot of computer science. I feel like I'm a much more sophisticated programmer, given my background in web development. 

The only thing I manage to be inventive at is creative coding using Processing. I often find clever ways to recreate geometric designs because I am a very visual person."
MachineLearning,"Hi, did you receive both email and a call or there will be only a call but no email for the application results?"
MachineLearning,"During interviews:  
* We want you to be technically strong.  
* You should know correct design principles.  
* You should be able to build scalable systems.  
* Your focus should be on quality code.  
* You should write clean and maintainable code.  


During actual work:  
* The faster you deliver, the better you are considered.  
* The more number of hours you put in, the better you are considered.  
* You should work fast.  
* You need to deliver it faster.  
* Why is your task not completed in the sprint?  
* You can't work late in the evenings? You have a family? Seriously? You should have thought about it before joining us!  
* Who cares about the quality? Lets promote XYZ, he sends so many mails, he speaks a lot during meetings, he is always available even during nights."
MachineLearning,"Most people, including highly accomplish ones, feel like imposters too.  No easy way."
MachineLearning,"I can exactly feel what you are feeling. It's almost like I am reading my own thoughts here. I have my undergraduate and my masters degree in materials science and have been working now for almost 3 years doing the same thing which you are doing. And heck, my mentor here even said the same thing about functional programming (""why don't you write monolithic codes?"") and sad thing is, he is pretty much thick to any logic apart from his own even if you explain him. Half of my time goes in explaining him how things work and all, which anyway I think he just asks to test my understanding (even though he has none of it). No suggestions here, but just letting you know, I feel what you are going through."
MachineLearning,"Instead of using sklearn, given you have free time in the afternoons, try to spend some time building these models from scratch. This will help with the imposter syndrome as well as the interview aspect. When I was starting out I choose one model a week which I would build from scratch using numpy to understand the nuts and bolts. 

Also instead of comparing yourself to other people, compare yourself to your own past. If you are learning a new technique or a new method to make your processes more efficient then you’re on the right path. Keep striving to be the best you can be at all times but remember you’ll have to take things one step at a time."
MachineLearning,"&gt; handsome amount of money

&gt; in academia
 
I jest"
MachineLearning,"I studied to become an emt in the past and one day i reallised how things i though were trivial like where is located your spleen, the name of the bone in your forearm or which of your lung is bigger was in fact something that no one knows unless they studied it. I know it is not the same thing when comparing to peers, but not everyone knows every subject you know and not everyone approach problem the same way you do even if they know more than you. Anyway, given enough time (talking from hours to years to study for the task) almost anyone can do anything, but you my friend can do ML much faster with much greater quality and this has enormous value."
MachineLearning,"I often feel like this too. Most of my work is taking Transformers and heavily modifying them for industry specific or task specific work, but I don't consider it all that complex. I'm always conscious of the fact that my entire job is implementing or modifying SOTA models which someone else invented, but I never actually invent any of those new SOTA models. The real machine learning people are the ones who invent BERT. The rest of us, me included, are just fungible worker bees who can be replaced at any moment. Or at least that's how I feel."
MachineLearning,What's your supervisor's PhD in?
MachineLearning,"&gt;'Tell me how KNN exactly works'

""Check out the wikipedia page. It's well documented.""

In our team's interviews, we try to parse for understand of how and when things apply, not algorithms or information. The information is out there, explanations are out there, comparisons are out there. But what's not out there is figuring out which algorithm applies to our problem space - and what the tradeoffs are."
MachineLearning,It's pretty useful the machine learning. I'm a student and I did amazing things with it.
MachineLearning,This comment deserves top marks!!
MachineLearning,"If you are constantly looking at cutting edge research and the kind of stuff that comes up on this thread you probably aren’t comparing apples to apples. I’m at a FANNG company and recently started on a project with a new research team. There are people fairy high up with PhDs and ML background that hardly know how to make a model in something like Tensorflow or PyTorch (beyond a cookie cutter example I’m pretty sure they copied and pasted), but they are still getting the job done and have been promoted to those levels.

Unless you are at a top tier ML research institution that is focused on SOTA research that the norm. Everyone isn’t stupid, you are just smart and probably have your standards set higher than others I’m guessing. If your job was so easily replicable other people would be doing it.

I’d say re-study up on the typical questions you get in ML interviews, (i.e. How does a Logistic Regression work?) and just keep applying. It’s totally normal to forget how even simple algorithms work under the hood after being out of school for 4 years, but you seem to have the foundation, so a couple months of solid practice and you should be back at a competitive level for interviews.

My response might be assuming a bit about you, but either way, best of luck!"
MachineLearning,Can I try it too?
MachineLearning,"Dev consumer, that's a nice word for it. I'm the exact same and have tried to find a word to describe what I do.

I also thought maybe software engineer vs programmer but I'm not sure if that's accurate, probably not."
MachineLearning,"Fwiw, paraphrased from xkcd, all anyone really does with a computer is press buttons to change the configurations of lights on a screen to other configurations of lights on a screen."
MachineLearning,"theoretically speaking, anyone's job can be replaced with someone else. of course, this isn't the case for lots of reasons, in practice. if you don't fall in this category, then your probably a researcher who is designing methods for others to be implemented--except only 0.00000001% of methods designed are actually going to be used. Else, expect a nobel prize."
MachineLearning,"You know how to apply ml to a specific field, isn’t that good enough? I’m currently learning ml and my future goal is to apply it to a complex manufacturing process to optimize the output. After seeing your problem, I’m a little confused if it’s a right path. I would assume that it’s a basic requirement to understand the underlying theories when you apply an algorithm or something. And if you know how to apply ml to one specific field, it’s easy to apply it to another related field because the skill sets are similar?"
MachineLearning,"I think part of the problem is many of us have unreasonable expectations for what a data scientist should be or should know. But the field is so broad and deep that there's no way any of us could know it all. Instead we all end up specializing in what we work on day to day, and that's just natural. I've worked as a data scientist for 6 years and have a master's degree in data science and analytics, but there are many basic things in statistics or machine learning that I don't know off the top of my head, because I rarely work with them. But I'm very good at the things I do regularly, a lot of which is pretty similar to the things you described. So don't sell yourself short."
MachineLearning,In a way but it doesn't come close to what I thought I would be doing as a researcher. I feel like my research is not making any dents in the field that I'm working in. Which tbh happens to most of the research and published papers but I wish it wasn't
MachineLearning,Excited to see a laundry list of papers titled “X is not all you need”
MachineLearning,"But if you got handsome amount of money and your name on research paper, it should be good huh?"
MachineLearning,"The intuitive goal of multi-stage fine-tuning is to avoid sharp updates on body parameters. There are many factors related to this goal, like learning rate, batch size, training data size and task. So it's hard to say certain fine-tuning schedule is useful or not in a case"
MachineLearning,"I think you already answered your own question. You may not realize but for some people, if they were in your shoes, they might think ""Wow, I've made it. I'm *finally* here."" Emphasis on finally, as in, they would be comfortable working that job for the rest of their life. Alternatively, some people may have gotten that far and be dealing with serious burnout. You are neither one of those people. You are really good at your job and you have free afternoons. Keep learning ML! Or some other academic discipline. With enough study, you will be laughing at how easy the interviews are. You're clearly smart and talented. Go and get a PhD and work for the badass company/university you deserve to work at. Or, don't get a PhD, just be well-versed in your discipline

Functional programming and Github are required in institutions who don't tolerate mediocrity. I think you belong at one of those. You just need to study more. To be fair, it will be a lifestyle change but I think it's just what's required to be the kind of scientist you seem to be intimating

Thanks for sharing by the way. You have my ""dream job,"" at least as far as being able to work independently and with R. I see now it's not all roses. Find a job that encourages growth and learning. It's so important"
MachineLearning,"In my case I'm just a final year undergrad student. I dont have any publication until now. I have a moderate GPA. I guess maybe the projects and internships I did made a difference. I've interned at Nokia, Samsung and Ericsson out of which last two are AI research based roles. I would recommend you to have a really well documented GitHub repos so that the recruiter can have a look at it and understand your potential. The work you did should speak for itself. Hope this helps.  
Cheers!"
MachineLearning,"u/SocksMcRockin agree with your points of view, you must have been through this even the depression. I have the same situation, and I have to admit that the things outside of my work sucks, feeling like isolation, and all of your suggestions are good for the OP. I felt better when I did those things, even though I still have panic attacks sometimes."
MachineLearning,"I don't know much about the domain that you're operating in, but based on what I can see, the 2nd option could be naively implemented by concatenating the climate data to the latent representation of the CNN that processes the radar imagery and seeing if it helps with the forecasting accuracy (assuming it is an autoencoder). 

 I couldn't find a paper that deals with climate data explicitly but Figure 1 of [this paper](http://cs231n.stanford.edu/reports/2017/pdfs/602.pdf) has a diagram that shows a similar setup for next frame prediction with a concatenated feature vector in the latent dimension.

Good luck."
MachineLearning,[deleted]
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; I feel like an absolute impostor who knows nothing. I write scripts (75% of the time in R), clean data (but it's not THAT challenging), do some data visualization, clustering, then I do some basic ML in Python (SOM, tSNE, k-means, PLS-DA, random forests, etc.) and that's it. I just apply methods other smart people have designed.

Similar to most ML jobs

&gt;Nobody reviews my code. I don't have to upload my code to GitHub for my team to see. I don't do unit testing, I don't have to deal with Hive, Spark, Kafka, CI/CD, Hadoop, AWS, GCP, Docker. I am just a script writer. I am not even a programmer. I don't know what I am but I no longer feel like someone with a **bachelor's AND master's** degree in STEM.

The reason might be you are not in a programmer team. The good news is that you have realized your weakness and that's the start of improvement.

&gt;Whatever I do, another person can easily learn in a matter of 2-3 months and just do it. Yes, I get praised a lot at work, I do a lot of stuff for everyone, I automate tons of things, but I am just considered such a great employee because everyone else sucks at their job so much. Truth is I do basic stuff. I am not special. But other people at my work are just not experienced enough to see that. My supervisor has a PhD but she asks me why I'm using GitHub for my code. She asks ""what's the point?"". Or the other day she asked me ""why do you write functions? Why do you need functional programming? It's useless"". That's the sort of place I work in. How can I consider myself valuable?

You did a good job. People often tend to underestimate the gap between majors and the power of division of labour in industry.

&gt;I have applied for other positions. Interviews are rare, but whenever I landed an interview, I screwed big time. 'Tell me how KNN exactly works'. I start to mumble and just freak out. I don't know the formulas. Yes, I studied them during my master's, but nowadays I just need \`from sklearn.neighbors import NearestNeighbors\` and that would be it. I feel terrible.

Interviews are quite different from real work. There have been many successful programmers who failed in interviews.

&gt;I feel stuck in a rut in my current position. Is this career path even worth it? Everyone is a genius. How can a guy like me even land a job elsewhere? I'm at best just a junior data analyst. I have lost all my SWE knowledge. I don't remember anything from C++/Java. I cannot recall what heapsort was made for. All that knowledge is gone because of my mediocre job position for the past 4 years.  
&gt;  
&gt;I wonder if I wouldn't be better off just opting for a career in software engineering. I know all I want is some more progress in my career, but I no longer even know how to go forward and what to do. My afternoons is a big waste of time. I just sit there undecided, thinking for hours and hours ""whether C++ would be a better language to master or Python or perhaps ML is not that bad after all and I can pull it off by 2 hours of daily study. After all I took courses and passed them, right?"" I have lost my way, my skillset, my self, and everything and I feel more lost than ever. Don't know what to do anymore. My first mistake was transitioning from IT/CS to bioinfo, and my second mistake was to accept an academic role. Now I am not qualified for anything in industry. Just a few days ago, I had another rejection from a good company (NOT Fortune 500, but still a good one). They said I'm ""too junior"" for all of their positions. 4 years of experience and a master's degree to be called ""too junior"". I cannot believe it.

The good news is you have a clear career goal, motivation and enough time. Maybe you can start from some personal projects in the afternoons."
MachineLearning,"Data cleaning, applying LR, tSNEs, etc etc sounds easy on the surface but to a software Engineer who deal with algorithms or implementing functions, it’s a whole different ball game. Don’t undervalue yourself"
MachineLearning,"This is a great read, I was always a little confused about VQ-VAE but the explanation for VQ-VAE in contrast to standard VAE methods really cleared things up."
MachineLearning,"Anything can be ""controversial"" if you include enough idiots in the ""conversation"". That's what I meant by ""That's not even wrong!""  It used to be that ""Artificial Intelligence"" meant what the ""not even wrong"" crowd now mean by ""Artificial General Intelligence"".  They don't know what they're talking about.  Those that do have their internal ""controversies"", such as the debate over various (rigorous) measures like Schmidhuber's speed prior or Levin search as alternatives to the algorithmic information size prior.  So, yeah, there is legitimate ""controversy"" in that sense, but other than that, it's sort of like the ""controversy"" over ""algorithmic bias"" which, again, once had a useful technical definition.  Then it was politically appropriated and now no one knows what they're talking about anymore when they use the phrase. 
 Those who _do_ haven't yet caught on to the fact that they had better come up with a new phrase for ""algorithmic bias""."
MachineLearning,"if you can’t find satisfaction at your workplace, so be it. pursue your own ventures. 

the world is your oyster"
MachineLearning,"The culture at your current position definitely raises eyebrows and is probably a large reason you dont feel like you're doing anything valuable. The comment about not needing github and functions is really strange

&gt; Yes, I studied them during my master's, but nowadays I just need `from sklearn.neighbors import NearestNeighbors` and that would be it. I feel terrible.

If you're trying to find a better position, it doesn't make sense to me to say you feel terrible about not knowing formulas, but then say it's because you only need to do X or Y on the job. There is an obvious difference between knowing *libraries* and knowing the underlying concepts. You need to prepare for these things if you're serious about interviewing, and a good litmus test for whether you understand something well is whether you can explain the concepts in absolute simple terms as if you were talking to a 5 year old

&gt;They said I'm ""too junior"" for all of their positions. 4 years of experience and a master's degree to be called ""too junior"". I cannot believe it.

If you can't explain KNN, all the YOE in the world isn't going to help

I definitely feel for you with regards to imposter syndrome, but you also need to get your shit together"
MachineLearning,"I'd mention that Weights and Biases is quite a step up from MLFlow for experiment tracking and visualization - very easy to create dashboards and reports, and log a variety of rich data types. Their ""sweeps"" feature allows for very easy creation of hyperparamer sweeps through a web UI or yaml file. The sweeps invert control so that clients subscribe and receive hyperparamers from the server so you don't need a top down orchestration directing this process and you can allocate additional compute later if you want. It also supports raytune if you like. 

It also handles all data/file/model persistance and visualization in a really nice interface. 

So for us weights and biases, coupled with very simple Kubeflow pipelines workflows that could easily be a k8s Job that just run a wandb command handle all of our training needs. 

For serving, I really like KFServing for very quick scaling under load and for a super simple deployment manifest and micro batching. We're looking into using BentoML as a standard format for packaging and serving diverse model architectures."
MachineLearning,"I don't think you've giving yourself enough credit.   I've been coding for ..... forever....  I have a BSEE, MSEE and MSIE.   I work at a lab with hundreds of PhDs and thousands of scientists.    I agree with another poster that if the other people at your lab could do what you do, they would.   They're busy doing their own work.

You listed basically the same tasks I do.   I think I do them well, and I learn something new every single day.    Can someone hand you a data set and you can train a model with it without going to stack overflow for every single line of code?    Congrats, you are in the top 5%!   

If you think you're in a rut, it's up to you to get out of it (which is why you posted this of course).    Find some aspect of ML that you find interesting and challenge yourself with it.   I read [arxiv.org](https://arxiv.org) papers every week.    That's the goal I set for myself.   Learning new things will probably inspire you with new ideas."
MachineLearning,[deleted]
MachineLearning,Hey we’ll be hiring more people soon for the data science realm. Our interview process is light and you’ll be able to make an impact quickly. Here’s our landing page (not mobile friendly yet) https://kickback.space DM you wanna chat more
MachineLearning,"I've been where you are (different industry, same feels). Your self-confidence is on the floor and that makes it difficult to see anything clearly. You may even be suffering from depression. 

So, first up, before you change a damn thing about your job, consider how things are outside of work. Do you have a good support structure and regular, meaningful face to face interactions with friends and/or family? Are you relatively healthy - don't need to be a total gym bunny, but do you get at least some exercise in, and eat the odd vegetable here and there? 

If everything's genuinely okay outside of work - which would be a miracle given the pandemic - *then* it's time to look at progressing in your career. 

Secondly - I assure you, *most* people could not pick up your role in 2-3 months. You're in a bubble of extremely well educated people. Only around 50% of people in UK have an undergraduate degree. Less than 2% have a Master's - because these things are VERY hard to do. So you do have a lot going for you.

Thirdly - It doesn't sound like you and your boss understand each other at the moment. When she's asking why you're doing GitHub and writing functional code, what's the context to that? Is she just being critical for no reason (I've met academics, wouldn't surprise me) or is it part of a wider conversation where she wants code produced quicker but to a lower standard, and that grates against your values? The answer to that is important in determining what your next move should be, i.e. whether it's possible to progress where you are or whether jumping ship is urgently needed for your mental health."
MachineLearning,Have you looked at knowledge based systems / knowledge representation and reasoning (KRR)?
MachineLearning,"Dude, I've been there. I'm still in academia, and every time some major upset happens I start to question myself ""Am I in the right place? Isn't it too much for my skill level?"". I'll try to explain how I help myself in these moments.

&gt; Whatever I do, another person can easily learn in a matter of 2-3 months and just do it  

No, it is not like that. It is a common cognitive bias: once you understand something fully, you underestimate the effort it took you to achieve this. Think about drawing: once you are good at it, you make the strokes intuitively, and you may even say ""well, this is not hard, anyone can do it"" - but it is hard, and it takes all your previous experience to perform a task. Decision making, programming, planning - this all takes a lot more than plain coding skills.

The fact that the others are better at knowing formulas and recovering them from your memories, only shows that they apply these skills more often. You don't have to mess with those formulas every day - why do you expect it from yourself out of the blue? You don't have to prove yourself that ""I am worthy to call myself a software engineer / data scientist"" - your employer calls you that, you get your paychecks, that's it. You have no obligations to ""be the best of the best"", ""know it all"" etc. - you just do your job, help others, communicate, achieve common goals.

I know it sounds plain but the key is: ""Stop comparing yourself to the others compare yourself to yourself from the past"". Job interviews are the measurement of interview skills at this moment. How to improve it? How to make the next interview better than the last one?

And the final piece of advice: take a rest. It is easy to lose / forget your interest when you have your day-to-day duties and obligations. What do you like about your job? What makes you happy at the end of your day?"
MachineLearning,"This is seriously helpful stuff. Thank you!

p.s. I've watched your stream on twitch and wish you would stream more ML and less games but whatever makes you happy."
MachineLearning,"Yup. You have to study for interviews like they’re exams, and that means going back over material from your first and second degrees. If you keep getting stuck on basic theoretical questions, it makes sense to invest your free time in either learning or relearning the basics.  There are tons of open source PDFs of textbooks and free YouTube series that cover these topics."
MachineLearning,"The problem is when you hang around other smart people you can start to feel like this. For some perspective, I had ""analysts"" and ""estimators"" who made $300k come to my course and they weren't even able to code. They didn't understand very basic statistics and data concepts, and just generally seemed to be going on feel. The bar is low. So yes, some 15 Russian prodigy could do a better job, but you are clearly valuable to your organization."
MachineLearning,"If it makes you feel better I finished my masters in bioinformatics and computational biology last semester and I just landed a job as a data science engineer focusing on NLP, machine learning with sklearn, and lots of deep learning and working with neural networks.

 If I’m being honest I didn’t learn jack in my masters program. Most of what I learned about RNAseq and tSNE, PCA, and all that came from doing research with a professor. And even then I taught myself most of that by going through tutorials. The same is true now. To get this data science job I have now, I watched “data science with python” videos on LinkedIn learning and studied supervised vs unsupervised learning and other concepts just to get the job. Now I find myself studying constantly to pick up deep learning. The key is just to put some time into it however you learn best. Take the time to find a tutorial that works for you. Watch an intro, then maybe find an hour and a half to 2 hour tutorial video on deep learning and machine learning and go from there. I promise you if you were able to make sense of some of the bioinformatics tools, with some time you’ll be able to pick up the concepts related to ML/DL. 

My only fear right now is I’ll get too comfortable making money and working with these new models that I won’t go back to complete my actual goal of obtaining a PhD in bioinformatics and applying what I’m learning now. But it’s still early"
MachineLearning,"Find a new job, you seem pretty well rounded."
MachineLearning,Do you work in Academia or government?
MachineLearning,"Imposter syndrome is real. I'm in the process of getting my bachelor's, and it seems like everyone around me has been coding since they were 12, whereas I just started a few years ago. 
Even though we're at vastly different points, I feel like I understand, at least a little.
I think you probably aren't giving yourself enough credit. You've done the automation, you're familiar with the environment. In your current place it seems like the best you can do is just work to improve what you can, and if you can land a better work environment, you'll pick it back up quickly."
MachineLearning,"Reality check. Interviews are gruelling competitions between people. Go teach yourself the tech for the job you want. This is the Information Age, you can learn whatever you want if you prepare enough."
MachineLearning,thats sus man
MachineLearning,"This. The money and skill is in the business value of applying the knowledge. 

As a software dev, I am definitely not jealous of the people creating the language or developing / maintaining packages. 

I'm a dev consumer. I use things that other people have invented to create value for businesses.

OP seriously underrates themself. 

There are many ""genius"" engineers who are broke because they don't know how to create or contribute real world value or utility. In my days as a project manager, it was extremely frustrating to work with devs that couldn't understand why that part of the business mattered."
MachineLearning,most companies aren't usually rushing to publicize their edge
MachineLearning,"I would also look into [https://polyaxon.com/](https://polyaxon.com/), I have used it on AWS and GCP the free open source version:  


* Extremely easy to install on an existing cluster
* Lots of integrations with many other tools [https://polyaxon.com/integrations/](https://polyaxon.com/integrations/)
* Access control is relatively simple
* Robust feature set, yet to need anything more from it"
MachineLearning,Stop being so insecure bruh that’s all it is
MachineLearning,"This one hit hard. I feel the same but I'm still in academia. The worst part is that it works and I get publications as well as respect from my peers for my research. 

It has come to the point where sometimes I don't feel like doing anything because nothing seems like good research but a random implementation and some tweaks here and there along with running experiments. I don't feel like a researcher most of the time but a code pusher. That's it"
MachineLearning,"This is the holy grail, but be aware that the field is in a half empty/half full kind of situation(more like a quarter full, but you get the idea).

Some points for your consideration:

- Neuroscience is still at its adolescence if not infancy. There is still a lot of stuff we dont have any idea how it works and most of our current probing methods in the brain are very crude and non-specific.

- ML/DL for all its hype is still 98% applied statistics, neuroscience has served as an inspiration for previous and present research but there is not a strong link between the fields beyond the usual hype of the week. You can  do computational neuroscience to try to model the brain in a computer but I suppose you are aware that it is  a different field with a different culture, goals and methodology.

- Beware of charlatans in the AGI space, you lift a rock and dozens will scatter away.

- Check out this podcast, maybe you will find somebody working on an area of your interest: https://braininspired.co"
MachineLearning,"MLflow - closer to experiment/model tracking platform similar to Weight and Biases, Allegro/ClearML, Neptune.ai, etc. You can cleverly use it to version data as well since it can log artifacts. But it does not have any pipelining/orchestrations/etc. Super easy to install and onboarding.

AWS Sagemaker - 'easy' to use if you do not have a lot of data. Enough examples but some are not updated to the latest versions. Tracking AWS approved package versions is not easy at all, and some of the tools are not close to mature or bug free. No need to install anything. Onboarding is quick and easy. The pain begins anytime you cross into using Sagemaker's build in features/estimators."
MachineLearning,"Exactly. The real value comes in knowing how to apply machine learning solutions to real world problems. I would argue that most of what you do day to day seems simple to you, but that's just because you do it every day. And from my experience most data scientists are mostly doing the kinds of things you're talking about."
MachineLearning,"If other people at your company could learn to do what you do, they would. Your position has value to your company, especially if you’re receiving praise. Having one person that knows how to apply machine learning quickly to a variety of problems is incredibly valuable.

This sub is pretty academic - talking about cutting edge applications and architectures. My industry (and probably yours too) is just at the point of implementing the basics. At that stage a combination of industry knowledge, communication skills, and basic ML / statistical analysis ability makes a person invaluable.

Don’t do your job if you don’t like it, but don’t leave just because you aren’t serving a custom designed pytorch CNN in a kubernetes container."
MachineLearning,"I don't think warm-up is really necessary. It was used on the original BERT model, but even in Hugging Face's example code (which I think many people now base their experiments on) the warm-up defaults to 0: https://github.com/huggingface/transformers/blob/72d9e039f9c78b9e6559456310ed2221192c0815/src/transformers/training_args.py#L363-L366"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"This paper might give you some ideas:

How to Fine-Tune BERT for Text Classification?
https://arxiv.org/abs/1905.05583"
MachineLearning,"I have experience with Kubeflow, MLFlow, AWS Sagemaker and GCP AI Platform

First - none of those is complete platform. You always need some additional services to fill in the blanks. CI/CD, data storage/processing etc

Second - each of them has strong and weak points.

Kubeflow - great for devops engineers, excellent pipelines, scaling of model serving. Not so easy for Data Scientist to work with.

MLFlow - more set of libraries on top of Spark/Databricks. Great fit for Data Scientists, Data Engineers. Serving models - not so good

AWS Sagemaker - relatively easy to use if you need standard things. Tons of examples to start. It will get more and more complicated as your use case gets more or more complex use-cases.

GCP - their AI platform is more implementation of open source components (most of them are maintained by Google itself) - kubeflow, apache beam, CDAP). I have a client who is using GCP and the almost the same OSS stack on-prem."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"**PO**o = **PO**nzi Scheme

Elon explain?!?!?!?!?!"
MachineLearning,"Some resources you could look at are: https://medium.com/pytorch/training-deepspeech-using-torchelastic-ad013539682 and https://aws.amazon.com/blogs/containers/fault-tolerant-distributed-machine-learning-training-with-the-torchelastic-controller-for-kubernetes/.

Concretely, this paper has data on scaling up to 256 GPUs: https://arxiv.org/abs/2006.15704"
MachineLearning,And is it comprehensible or does it look like a freaky day dream?
MachineLearning,i think this is really funny
MachineLearning,"Warmup of the learning rate is essential to keep the model from forgetting everything, I think Ulmfit introduced that. 

Other than that I assume that a CNN has some hierarchy imposed into the model, meaning that gradually (un)freezing the model is about more/less detailed features. Transformers don't have that imposed directly, which could mean that more/less detailed information is a bit more scattered through the layers.

Disclaimer: grad student here, take this with the appropriate grain of salt"
MachineLearning,"What I don't understand is why this isn't the default. To have to know the complex output shapes makes hyper parameter tuning very difficult, especially if you change layer sizes. That alone seems like bad API."
MachineLearning,"It certainly feels that way to me. Incremental improvements are everywhere and I've yet to see a ground-breaking new way of looking at information and learning. Admittedly, I don't have one either :(."
MachineLearning,"Once up a time, wireless was young and there were all sorts fun abstract problems like what if there weren’t enough towers so phones had to pass calls peer to peer?  And what if you had to build a cell network on the moon? And what if you wanted to offload a math problem from your phone to a nearby server?  Lots of papers that as far as I can tell never had any real world impact were written.  And although cell technology has advanced to the point where this is all moot its such a nice problem that a whole ecosystem of conferences drifts onwards."
MachineLearning,"Couple things-

&gt; if the training converged then it is trivial to show that clustering the features can lead to good classification because you're simply replacing the decoder in the neural network with a different decoder (in this case, the clustering algorithm is the ""decoder"")

This would show that the autoencoder learned features that weren't just useful for reconstructing the images but doing so in a way that captures the digit labels *without seeing the labels*. That's not guaranteed at all, you'd only expect that if the autoencoder was actually learning the underlying structure.

&gt; Additionally, since nothing is held out of the clustering step, it is of course going to show good results if trained on the entire set of points

But there *is* something held out of the clustering step- the labels. If I hand you a deck of cards and ask you to group them and you do so by suit, you've ""trained"" on a full dataset. But you have absolutely no idea how ""good"" the clustering is according to me, you only have the intrinsic measures of clustering quality to go by. If I evaluate your clustering by the mutual information with the deck split by face value rather than suit, you've seen all of the data but you've done no better than random. So good correspondence between the clustering and the labels is pretty strong evidence that the autoencoder is doing what it should.

Of course, you could happen to cluster the cards by face value decently well purely by chance by tossing them in random piles. It would be pretty unlikely (we can quantify how unlikely, though) but possible. If I evaluated based on 52 cards being split into red suits and black suits, you'd have a better chance at a decent performance by tossing them into piles. If I gave you 1000 of each card, ace, 2, ..., 10 and evaluated on face value, though, it would be very apparent whether you'd picked up on the pattern I was interested in or not and you would have done so using only information about what the cards looked like, not what I thought was important about them. That's what this paper is doing. 

They probably should have done some cross validation or boostrapping with the whole process- evaluating the stability of the clustering with the autoencoder trained on a bootstrapped sample of the data, clustering the encodings of a separate bootstrapped sample, but this isn't anywhere near the kind of methodological flaw that evaluating a supervised problem on seen data is."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"That's for heavy recreational users. For them, some withdrawal effects last a month or more, but the major ones last for about a week and can be handled by titrating off. (see [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7138250/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7138250/) under Background)

Therapeutic doses are smaller and so the withdrawal effects are smaller as well. There's tolerance but it's treatable with diet (magnesium and NAC were recommended to me) and aerobic exercise (for some reason). Also, taking amphetamines can actually prevent stimulant abuse disorders in ADHD patients in the first place ([https://www.tandfonline.com/doi/abs/10.1185/030079908X280707](https://www.tandfonline.com/doi/abs/10.1185/030079908X280707)).

Actually, non-stimulant ADHD medications like guanfacine are really physically addictive and have much more unpleasant withdrawal, it takes about two weeks and messes with your heart rate."
MachineLearning,"&gt;Do you think i’ll have a chance getting admitted into a PhD at a top notch (like top 10 worldwide) after publishing some good research papers under the supervision of these professors and getting recommendations from them after finishing my masters?

The name of the University doesn't really matter so much if your research and references are great, but that's a big if.

You should also think about the quality of education you're going to receive, which I would expect to not be that great for a University in it's first year."
MachineLearning,"Yes, this makes sense. But supposing i want to compare this unsupervised method with a supervised one of my own i need to properly use the train and test partitions for the unsupervised task, is this correct?"
MachineLearning,It says invite invalid.
MachineLearning,"If I'm understanding this correctly, it's not actually an issue for the reason that the labels of the data are not given in training or used in any way other than to evaluate the resulting clustering. If I have labelled data (X, y), give you only X which you train an autoencoder on and then cluster the encodings of, you give me back the assignments of points to clusters, and I compute e.g. adjusted mutual information between the labels and the clustering, then you are doing an unsupervised problem and the labels are there purely for the *evaluation of the unsupervised model*, not in the training of a supervised model. In other words, the model does not at any point have access to information about the labels so combining the standardized test and train portions of MNIST isn't an issue because the model isn't using the labels to learn.

Now, if they're doing model selection using the labels to evaluate multiple times then the model selection procedure is ok but the numbers coming out of the evaluations on that data will be biased estimators of the model's performance on unseen data. But this is in general a much smaller issue for unsupervised models than supervised ones. It's not good practice to claim x% accuracy or whatever without the proper (nested) cross validation but here I think what appears to be shady is mostly just the author using a dataset that has standard test and train splits for a *supervised* task and combining them for use in an *unsupervised* task that is evaluated using the labels."
MachineLearning,"This is all incredibly fascinating, thank you very much for the thoughtful response. I will check out the work you listed. Are you aware of any ""map of cognitive science"" that would help in getting a feel for what is being worked on? And if I can ask for clarification, what do you mean when you say that ""thought"" falls within the realm of philosophy? Do you mean the philosophy of mind? This is very helpful, so thank you again"
MachineLearning,"I suppose that is the 'holy grail' in some sense, but my interest lies in the area of recreating the more uniquely human aspects of intelligence - e.g. all sighted mammals have a visual processing system, but (presumably) not all mammals can form the kinds of complex mental structures that humans can and which have allowed us to progress to this point as a species."
MachineLearning,"So if you want to have specific labels on samples, at some point you will have to do some manual labeling no matter what. It might not be much, it might be labelling features instead of samples, or some heuristics to give some ""soft"" labels on things, but you will have to do it. There's a lot of ways to go about that, though.

The simplest approach- and the most tedious- is just to label everything manually and train a classifier using any supervised model. Obviously not ideal, you'd like to avoid labelling everything and it's not going to be necessary to label everything in the dataset.

The other broad approach is to use a semisupervised method, where you have some data with labels and usually a majority without, and a model that can take advantage of the relationships between features in the unlabeled data to relate them to the labelled data's features, and from there, the labels themselves. The simpler the classification problem is and the clearer cut the classes are, the less labelling you'll need.

There are a few strategies for choosing samples to label manually. The simplest and worst performing is to just randomly sample some number of samples to label without regard to anything else. Easy to implement, but there's likely going to be many redundant labels to assign (meaning you label samples 1, 2, and 3 but it would have been easy to infer the label of 2 and 3 from 1 automatically) and potentially very informative samples don't get labelled. You can do better. One way is to compute some kind of ""importance"" of each sample. A particularly effective way is to construct a graph where samples are nodes and edges are weighted by the similarity between samples according to some function, and to then run PageRank on that graph and label the nodes with the largest PageRank score. This effectively labels the examples that are most important for connecting the others together in terms of features and it's a massive improvement.

The best option, and most sophisticated, is to make use of an active learning method. A model will be trained on a small set of initial labels and evaluated on the unlabeled data. Then the active learning method chooses one or more unlabeled samples that would be most useful to have labelled, based on what it already knows. A human labels those, and then the process is repeated. The performance you can get this way by only labeling the stuff that really needs the label is kind of incredible for the amount of manual annotation it requires and there are plenty of off the shelf options for adding active learning to a model.

As for the underlying classifier itself, go with the simplest first. There's no reason to build a more complex model if a simpler one will do the job. Start with a naive Bayes classifier, an SVM, and a decision tree and see how they do as a baseline. With judicious annotation I would be surprised if you can't make significant headway with one of those."
MachineLearning,"Thank you, I will check it out."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Withdrawal effects from drugs like Adderall can last for months. In another comment, you wrote:

&gt;ADHD medications are not at all addictive at normal doses. Only caffeine is.

This is another false claim. Please be much more careful with spreading false information."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks this was what I was looking for.
MachineLearning,"Awesome, I’ll check those out, thank you!"
MachineLearning,"Let me put my criticism in a little more detail:

Firstly, without an implementation and any empirical evidence, this idea might be as good as any NN architecture anyone else could come up with. Nobody knows because they haven't tried it.

Secondly, this paper kind of seems like some kind of [patent troll](https://en.wikipedia.org/wiki/Patent_troll).
In the sense that if somebody actually develops a useful architecture - even if it only vaguely resembles this idea - Hinton can come out of a hat and say ""I called it first!"", all without ever having even a working prototype.
It's been done before, and it's quite common in the patent industry."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;That should be your go-to for fine-tuning BERT-type models. Every other method either has a performance cost (e.g. freezing), 

Is there an intuition though for why freezing isn't ""needed"" for BERT-type models while for resnets it's standard practice?"
MachineLearning,"When there are multiple options, always start from the simpler ones, see where it goes and if it's not good enough then try a more complex solution.

The answer here depends on the dataset I would say. If it's a very specialized dataset, it's always better to finetune everything because the pretraining probably wasn't great enough for completely new data. Otherwise it could be much faster to only train an extra fully connected layer (that's how you're supposed to do things if the pretraining was good)"
MachineLearning,talk is cheap. maybe we should stop commenting on reddit.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for the feedback. The vast majority of the content is agnostic to the data domain. In some cases, we do discuss specific data considerations, and in section 7.2 we mention how they have been applied to various data types. Also, we do discuss methods such as DALL-E (the DALL-E paper is called ""Zero-Shot Text-to-Image Generation"") in Section 3.1.1 ""2-Stage VAEs""."
MachineLearning,"You are only talking about generating pictures, but this is not mentioned in the title, abstract, or the keywords. Also, you've missed the elephant in this room which is DALL-E."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Sure!
MachineLearning,Okay! Thank you for the help
MachineLearning,"Get an idea of what kind of data you working with, after that you can go several ways:

1) Use rule based models and see how they work

2) Use regex and label some data on your own, train classifier from this

3) Use USE + gaussian mixtures for totally unsupervised clustering

Check the metrics from all three approaches and pick the best one."
MachineLearning,"That is not what I assume, and I would ask that you have a look at the definitions, as it's all fairly straight forward."
MachineLearning,"Thanks, I'll be sure to take a look!"
MachineLearning,what would be a recent example of this?
MachineLearning,They answered! Apparently the dataset webpage is usually down. They are currently working to bring the site back up.
MachineLearning,goodness... i hope that's not me
MachineLearning,You're doing god's work here OP
MachineLearning,"Great work! You might also want to include other autoregressive method like TraDE which shows significant improvement over Normalizing Flows

 [https://arxiv.org/abs/2004.02441](https://arxiv.org/abs/2004.02441)"
MachineLearning,"Fine-tune all layers.

That should be your go-to for fine-tuning BERT-type models. Every other method either has a performance cost (e.g. freezing), incurs significantly more complexity for very minor gains (robust fine-tuning) and/or is too new to have been extensively tested."
MachineLearning,"Will do, thanks for the suggestion!"
MachineLearning,"Would love to see new AnycostGAN added to the comparison as it positioned as an improvement over StyleGan2

https://hanlab.mit.edu/projects/anycost-gan/"
MachineLearning,Downloaded right away.
MachineLearning,"Yeah, I agree that the classification regions will be intermingled in complicated ways! I wonder, however, whether that really implies that there are very many distinct regions. In 3D, for example, if you took a few long, colored strings and randomly tangled them together in a ball, every point on any string would probably be near a point on every other string, and yet each string is one connected region."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"[pyimagesearch](https://www.pyimagesearch.com/) has good resources for dealing with images.

[CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/), is a Sanford class about using CNNs for images. Which you can check out."
MachineLearning,multilabel is just like muliti-class except in last layer sigmoid is used and binary cross entropy as loss function
MachineLearning,"from my practical experience, freezing BERT and finetuning only the top layer, then unfreezing and finetuning the model hasn't really given me any noticeable improvement in training speed/accuracy compared to a model like resnet. or perhaps a better way to phrase it is that when skipping the 2nd step as you described, i have not seen any noticeable detriment when training transformer models compared to skipping the 2nd step cv models. 
i also recall thomas wolf from huggingface did some experiments on different transfer learning methods for transformer models in 2019(?), and found that finetuning top layer -&gt; finetuning whole model didn't really help, but i haven't been able source it.

as for why this is the case, i have no real good explanation. what i have noticed is that the learning rates i use to train transformer models are fairly lower compared to resnets, typically around 1e-6 to 1e-4 range. my (very unscientific) hypothesis that because the learning rate is low, the weights for the top layer isn't experiencing any drastic changes, but that's just my guess"
MachineLearning,:} tyty
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"CLIP is just being used to steer BigGAN in a desired direction in that project, and I believe the pretrained BigGAN only knows a certain number of image categories."
MachineLearning,"Hi everyone, you need to **change your region to us-east-1** so that you can find the snapshot, otherwise you cannot find any match."
MachineLearning,"Bruh, you hollering does reflect poorly on you. Just saying. I saw the discussion and at least he was civil with you and trying to engage"
MachineLearning,"&gt;you join the community slack so we can notify you once the demo is ready? (hit up ""@Mikayel

Welcome to the community :) u/MagicaItux"
MachineLearning,"You're not debating in good faith.  You're the who started the conversation because you just had to take a dig at the topic in a completely unrelated thread.  I brought up bias in ML models and you pretended that that amounted to GPT-3 hurting people's feelings on occasion.  You backed up that absurd moron who was calling for everyone to be blacklisted from the industry.  

Just because you whine about civility does not mean you are being civil, and just because you call something an opinion doesn't mean it can't be criticized or that holding it doesn't reflect poorly on you."
MachineLearning,"just throwing this out there, but 

**PO**rn ~= **PO**kemon
**PO**ppies ~= **PO**odles

that seems like a bit of an odd coincidence."
MachineLearning,"I have to read about abstract and reasoning in AI. 

any recommendation for a good paper or research material?"
MachineLearning,Why doesn't the dataset satisfy your criteria? If your idea is that *all* points in the full domain are known to the classifier why even bother with nearest neighbor? Just use the label on the point itself!
MachineLearning,I can try it to see if it works for me. Do you remember what config you used when you got the error?
MachineLearning,Thank you
MachineLearning,"I contacted the authors, site admin etc. but none of them answered!"
MachineLearning,I dunno maybe they felt rule34 material would help the model think more creatively lol
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I have now implemented various optimization methods with momentum, I must say that it has not helped much with my stagnating loss function, I keep getting subpar results, but now I have more variables to play with, lol. [These are the optimization algorithms I coded](https://amaynez.github.io/TicTacToe/optimization.html):

* Stochastic Gradient Descent with Momentum
* RMSProp: Root Mean Square Plain Momentum
* NAG: Nezterov’s Accelerated Momentum
* Adam: Adaptive Moment Estimation
* and keep my old vanilla Gradient Descent (vGD) ☺

So far SGD with momentum seems to be showing progress, but it is slow. Adam very quickly optimizes, but still gets stuck.

I will continue to work on it."
MachineLearning,"&gt; You have a misconception that any scientifically valid approach has to ""commit"" to a specific model (e.g. obtained by learning on a training set) and then evaluate on a previously unseen test set.

It is not a misconception, it is a wide spread idea in machine learning. Maybe this is not always necessary, as you mentioned, with transductive learning (not mentioned anywhere in the paper). I decided to ask here because i don't have experience with unsupervised learning and thought i could be wrong."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I keep getting this error. Do you have any idea what the issue might be:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-2-fd1ebf63d925&gt; in &lt;module&gt;()
     80 
     81 
---&gt; 82 problem = GenerationProblem(config)
     83 operators = get_operators(config)
     84 

11 frames
/usr/local/lib/python3.7/dist-packages/torch/jit/_script.py in graph(self)
    447             ``forward`` method. See :ref:`interpreting-graphs` for details.
    448             """"""
--&gt; 449             return self._c._get_method(""forward"").graph
    450 
    451         @property

RuntimeError: Method 'forward' is not defined."
MachineLearning,"This is literally my day-1 in this sub &amp; I see drama lol.My 2¢: What the dude is saying has some genuine traction. I am moving from a MSc in ML to mainstream DevOps because of the theatrics by the Ethics teams in a midsize startup in Austin TX. The recent news from Google has made it even more prominent &amp; asking for more oversight into how we even work. That's just simply bad. We have real bosses &amp; the pseudo-bosses in our own pay grade who we report. And many of them aren't core devs but linguistics majors. It sucks to be working in consumer focussed NLP projects in this scene.

tldr? No he isn't detracting. This is real. This is actually happening maybe in trickles now and could probably go up if gaslighting &amp; drama does not stop"
MachineLearning,Thank you I will read this. Though I am beginner with only primary knowledge upto pandas and deep learning. If anyone can recommend some great research papers about neural networks those which could be used in data science on superstitions I would be extremely grateful.or I could explore for some hours/days on google
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Geez seems you've made up your mind to pick a fight with this guy. Why even pretend there is a logic to be served
MachineLearning,I can't even find the entire dataset.
MachineLearning,"You have a misconception that any scientifically valid approach has to ""commit"" to a specific model (e.g. obtained by learning on a training set) and then evaluate on a previously unseen test set. This is not necessary. Read through the example in the wikipedia article on tranductive learning linked above, and you will see a case where a model can use the (unlabeled) test set as a way of better estimating input distribution, which allows for useful inductive bias. 

Nevertheless, transductive learning is typically useful when the training data are small and a large set of unlabeled test data is available. 

Now, does the authors in your case intend to use transductive learning? I doubt it, unless they specifically mentioned this technique. Does the ""accidental"" transductive learning framework help to get better result? I would say only marginally, since they already have the full MNIST training set to use, and the additional test set doesn't contribute that much marginally. Do the authors unintentionally use the test set for hyper-parameter selection as a result? Maybe, since they didn't specifically say they just use one value because of some reason, it is possible that they experimented with multiple values and just leave the best one in the notebook. The final question is truly the concern. Depending on the extent of hyperparameter search (10 vs 1000), it is possible that this serach process overfits the test set (and perhaps severely)."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"To be frank I've necer used azure ml. However, I would try to convert the timespans into unix timestamps. this way you convert the date to an integer. Binning function usually accept this data type."
MachineLearning,Usually you calculate the loss for all examples in the batch and then take the mean (or sum/max/etc.). Take a look at the pytorch docs for [BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html). The 'reduce' parameters specifies how to calculate the loss for the whole batch (default == mean).
MachineLearning,...Only the experienced ones. It's terrible for anyone who's new to the field.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"
Thank you and
I had to read some papers bout what i focus on at the time in order to implement them"
MachineLearning,"Take a step back and reevaluate your workflow.

You will, in most cases, have some data and a few questions. Is your data ready to answer these questions? Do you know how to clean and build a proper, workable dataset out of your data source(s)?

Then, how do you look for answers? There comes the analytical part. There are thousands of different problems and approaches out there. You'll get this line of though with the experience. For instance, since a genome is represented by letters, it's possible to use NLP algorithms.

Also,

&gt;sitting long ass hours just to read stuff about those 2

Are you actually implementing something?

Instead of reading about a lot of stuff, try to go *a bit deeper* in something"
MachineLearning,"yup, I was thinking there needed to be a category between superstar and red flag waving slacker and “the mentor” is a good label.

but, as long as we’re having fun how about

the dead-ender - incredibly dedicated and even brilliant faculty who unshakably specializes in a completely dead issue which was hot 20 years ago but is career death for the gullible student who wanders into their program."
MachineLearning,"When you say ""fundamental AI"", do you mean artificial *general* intelligence?"
MachineLearning,"I don't think he is doing model selection, the embedding dimension is a command line parameter with a default value. Unless he already determined this value and choose that as a default. Still, i find it suspicious since dataset splitting is machine learning 101. I believe that in order to prove that the embeddings are good for the problem he needs to keep a hold out test set. He is using UMAP algorithm to learn the manifold. I believe that you can't learn the umap representation on one set and apply it to other set using the learned representation, like you can with PCA for example. Maybe this is the reason he is doing like that. But it really annoys me that the results reported, apparently, don't use a hold out test set."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I love everything. I dont know it is because i study ML and DL at the same time. Im often tired of sitting long ass hours just to read stuff about those 2
MachineLearning,"I have also seen quite some tutorials, reloading the model for every request. There are a lot of data scientists without any software engineering skills. That's not a problem as long as they are not responsible for model deployment or worse write an article about it."
MachineLearning,"I face the same issues. I learn how the ML black box operates, then forget how I wanted to implement it. I like to think about it like teaching a class. The MLs are the students, you ask the questions and grade their responses. I get stuck trying to decide valid responses."
MachineLearning,"Reinforce the basics.

How do you feel about programming in general?

How do you feel about math and statistics in general?

Try to identify your pain points. Is it the concept? Is it the code?"
MachineLearning,"If the paper does model selection (e.g. choosing embedding dimension or learning rate) on train + test, then it is wrong (i.e. the test set information is leaked). But if it is just done once, then there is nothing inherently wrong with it. This is called [transductive learning](https://en.wikipedia.org/wiki/Transduction_(machine_learning)). Although you could argue that the model will need to be re-trained for a new test set to achieve optimal performance (rather than simply using the autoencoder to predict new test data points)."
MachineLearning,Beautiful to see this. Can not thank you enough! I was just complaining about how rare it is to see parameter efficiency comparisons being made.
MachineLearning,"You forgot one.

* The mentor - has a few students, is well versed in acquiring funding, knows the industry pretty well. Enjoys learning and creating cool stuff with her students but knows how to push them. She may have had a career change once realizing how awesome ML/DL and is mostly published in another field, perhaps not so much in yours. Has the potential of being a life long friend and mentor. 

I'm only just starting my phd application process. But I think its important to know these people exist, otherwise you may never know to look for them. 

But directly to OP: if they have ZERO publications....thats a big red flag."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Incredible review thanks for sharing, it's possible I might need to cite this in an upcoming paper too."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"huh...btw ""poke"" is usually slang for porm in Finnish 🤔"
MachineLearning,"Any company that asks DS questions for a software engineering position has their priorities wrong. Any company that does not ask system design questions for a software engineering position is bound to hire incompetent people. So both of those points would make me decline them immediately after the interview. I can build ML pipelines and put them into production, etc., but I cannot balance a binary search tree in an interview. Only one of these skills is actually needed as an ML engineer, and it is not the latter."
MachineLearning,"Maybe this is of interest then, https://pytorch.org/docs/master/generated/torch.nn.LazyConv2d.html"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You are correct, but this dataset doesn't satisfy the criteria I set out in the article.

The point is not that nearest neighbor is always the best -

The point is, rather, that nearest neighbor is perfect, if the assumptions I set out in the article are satisfied.

This dataset does not satisfy the assumptions, so the lemmas don't apply."
MachineLearning,Thanks for the comment! We do already cite this (Section 3.1.2).
MachineLearning,We don't have plans for this as this is just a review paper.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Great survey! You might also want to include hierarchical VAEs like NVAE https://arxiv.org/abs/2007.03898
MachineLearning,"Hi, unfortunately the dataset of the algorithm behind it is kept behind close doors because they are still undecided if they want to file a patent application or not."
MachineLearning,Are you going to make a video describing your findings?
MachineLearning,"Thanks! We extracted all information from the official papers to not bias results due to differences in implementations where official code isn't available (many official results train with more resources than we have available), hence why FID for instance isn't given for all methods. Sampling speeds are based on the number of function evaluations and the time for a single function evaluation."
MachineLearning,"Alright, I’m out of ideas for now as to how the paper’s results can be valid. Though, without being about to review the paper, I cannot be certain in my assessment. If you’re willing to share a link to the paper, then you can likely get a better judgment of validity."
MachineLearning,"Hello, when using Deep Reinforcement Learning is there a way to provide the terminal state to the network itself? 

E.g: in a puzzle game, when the goal position is reached (i.e done = True) the episode is finished meaning that we directly jump to the next episode (the network will never receive the state of the terminal state)."
MachineLearning,"it is the full mnist dataset. 
He is loading directly from keras.dataset.mnist.load_data()"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,""" No one would seriously consider loading a model before each request "" - oh man, I've seen things :-)

The best one was starting separate container for each request. And because it expects GPU on auto-scaling k8s cluster it in most cases creates new node, downloaded container to it, run inference and deprovision node. I had hard time to keep straight face when CTO of that company was puzzled why it takes so long time for inference of simple model."
MachineLearning,"Yes - when you at that point, probably even sooner. Not all models get there - have not done any stats, but it is well below 10% of production ones in my experience.

One positive of Flask approach is simplicity in data preprocessing. You don't want to have an API with tensors in and tensors out. It creates tight coupling of services.

In first step I do transformation in Python code within service

If the model is successful, stays in production then it is the time to build transformation layers on both sides on the model. Then move to tfserving or NVIDIA Triton.

And finally I have gRPC service where SWEs can send sentence (for NLP cases) or jpg image and get some reasonable result."
MachineLearning,And how many images are in x_train?
MachineLearning,"This is awesome, thanks for putting in the effort to sample these techniques. 

Out of curiosity: how did you test these methods? Did you write your own test suite or use open source implementations for the various models?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Please have a look at [AIME A.I. Cloud from Europe (Berlin, Germany)](https://www.aime.info/gpucloud/)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"he is doing something like this (pseudocode):

(x_train, y_train), (x_test, y_test) = load_mnist()

x = concatenate(x_train, x_test)

y = concatenate(y_train, y_test)

autoencoder.fit(x, x)      # tensorflow model

embeddings = encoder.predict(x)

cluster(x, y)"
MachineLearning,"Could it be a terminology issue? Some researchers refer to the validation set as a test set and vice versa. Since you say this done on MNIST, one clue this occurred would be if they authors state they have fewer than 60k training images. The official MNIST train/test split is 60k train/10k test. In supervised settings researchers will occasionally split the train set into a validation set of 5-10k images and leave the remaining for training. Since the approach described is unsupervised, no held out validation set is needed.

Another possibility is the use of cross-validation as an alternate means of assessing the model, which again allows training on the full data, but in discrete folds, thus technically training on the test set.

This is why I say more detail is needed to know whether or not what the researchers have done is valid. Without seeing the paper, it doesn’t make sense to jump to a conclusion. Otherwise we will likely go back and forth like this with me possible explanations where it can make sense to “train on a test set”."
MachineLearning,"How can I implement a vocal recognition system? My language has no vocal recognition and I wanna contribute. 

What math and concepts do I learn? Where do I start?"
MachineLearning,I'll try this out. Thanks.
MachineLearning,"I'm with you OP, this seems at the very least dubious.

 The model as you describe it isn't shown to generalize beyond the images it was trained on set since none were held out. Depending on the loss function of the auto encoder, if the training converged then it is trivial to show that clustering the features can lead to good classification because you're simply replacing the decoder in the neural network with a different decoder (in this case, the clustering algorithm is the ""decoder""). Additionally, since nothing is held out of the clustering step, it is of course going to show good results if trained on the entire set of points.

At the _very_ least, the authors should have held a set out for the clustering. Since the images were already seen and incorporated into the k means of the model, it's fairly trivial to see that in the absence of outliers that this model is probably going to show high accuracy."
MachineLearning,"So I do research in this area and am overwhelmed at how much there is to say in response to this question. I did my Bachelor's degree in Cog Sci, and MS in computer science, and am (hopefully) getting ready to defend my PhD in linguistics. 

First you seem a bit mixed up about whether you want to work with cognitive science or ""thought"", which I would argue falls more in philosophy. If you are more interested in the philosophy approach I can't help you because I'm not familiar with the area. If you're interested in the cognitive science side of things there is a massive amount of research going on in Computer Science, but also Cognitive Science, Psychology, and Linguistics departments. I can't possibly overview all this work here, but a few of my particular areas of interest are below.

You mention making better NLP agents but ""not getting that much closer to understanding and recreating reasoning and structure of thought."" I think you're right when talking about thought specifically, but a lot of work looks at the relationship between the type of knowledge gained by large NLP models, language models in particular, and the type of knowledge held by human speakers of the same language. The most famous example of this is Linzen et al. 2016, who looks at the ability of recurrent language models to learn long-distance grammatical agreement. This same general idea has been used to propose new linguistically-informed neural architectures (e.g. Dyer's RNN-Grammars) and to make claims about linguistic theory. 

One issue that often comes up at the intersection of Cog Sci and CS is the formal expressivity of deep learning architectures. Different aspects of the language faculty seem to be restricted in their complexity, for example syntax is context-free (actually maybe mildly context-sensitive) and phonology is at most regular. If we want deep learning models to be cognitively viable models of these aspects of language we need them to not be able to compute the same range of functions without overgenerating. We all know that neural networks are famously Turing-complete in the limit, but in practice things can shake out differently. For example simple RNNs (with a certain class of activation function) seem to in fact be regular, which means they are not viable models of syntax but might be viable models of phonology. Weiss et al. 2018 and Merril 2019 are some good places to start on this topic. 

I have one non-language example, so I'll share it in case your interested. You mention ""CompNeuro"", obviously most modern neural nets are not actually related to the structure and function of the brain, but there are a few groups of researchers looking into the idea that biologically inspired networks might perform better (it seems at the moment that they are at least more efficient, but they aren't particularly good at any task yet). Look into spiking networks if you're interested in this branch of things. By the way I have seen no work that uses spiking networks to say anything about linguistic theory (although Hopfield nets had a moment a few years ago) and would love someone to start thinking about it!

These are just issues in my very narrow field of research. If you look closely enough at the body of work in any domain of cognitive science you will find people either applying ML or doing real ML research. Find out the topic you want, see who's working on it, read their papers, and then contact them and I'm sure at least some of them will be happy to talk with you about their work!"
MachineLearning,"It depends on whether (advanced) cognition can be designed in different ways. If there is only one simple way to lead to cognition, then it is very insightful to use that knowledge for machine learning approaches.

The null hypothesis is probably that this is true since many features of biological organisms are a result of convergent evolution. It could be argued that an artificial design could be very different than that seen in nature, but the great complexity of natural cognition would make this a difficult task. It is also possible there is only one way to design a cognitive system in an advanced form.

The artificial systems, such as a running computer algorithm, are definitely more capable than natural cognition in certain tasks. However, one problem is ""transfer learning"". There are efforts to solve this, such as by Dr. Bengio and co-workers. I have contributed to some writing about similar topics at a website for [Artificial Cognition](https://cognitionmodels.com/category/artificial-intelligence/), but mainly commentary (although I have contributed original work elsewhere)."
MachineLearning,"Yes, but the model needs to accept text as an input as well. show-attend-tell receives image and outputs text. I'd like something that receives image and text, and outputs text. So something like this: [https://arxiv.org/abs/1909.02950](https://arxiv.org/abs/1909.02950), but instead of classification task, I'd like to use it for text generation task"
MachineLearning,"Yes, you can beat nearest neighbor. You have two classes ""cold"" for temperatures less than 10 and ""hot"" for temperatures greater than 10. You have two data points: (0, cold) and (15, hot). Nearest neighbor would incorrectly classify 9 as hot."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Do you mean something like the paper show-attend-tell?
MachineLearning,"That nails it. Once you have thoughput at your models, you should invest the time to build a scaling and reliable infrastructure. A good engineer uses the tools made for the use case, even if it means you have to learn a new tool."
MachineLearning,"We based DCGAN's 5 star rating on being able to train it in a couple of hours for its intended use case. While it's true this doesn't scale like ProGAN, this is reflected in the scaling criteria. It's a difficult comparison case with DCGAN only applicable to low res images and ProGAN meant for very high res images. For instance, on the largest images, ProGAN reportedly takes 96 hours while BigGAN takes 24-48 hours, while for smaller images DCGAN is much faster. Now you mention this it is somewhat misleading at a glance so we will reassess it and clarify this criteria in the text. Thanks for bringing our attention to this."
MachineLearning,"I don't know of any such trick except for using TensorFlow. In fact, I benchmarked PyTorch XLA vs TensorFlow and found that the former's performance was quite abysmal: [PyTorch XLA is very slow on Google Colab](https://github.com/pytorch/xla/issues/2247). The developers' explanation, as I understood it, was that TF was using features not available to the PyTorch XLA developers and that they therefore could not compete on performance. The situation may be different today, I don't know really."
MachineLearning,It is research in deep clustering. Instead of using a neural network to do the clustering he claim that it can learn an embedded representation using an autoencoder and substitute the clustering neural network with a simple clustering algorithm like Kmeans and search for a good manifold with the Umap algorithm for example. He uses this to do a simple classification with mnist.
MachineLearning,"Modafinil was the only one that showed a strong and statistically significant effect. Second place was noopept. Anecdotally, phenylpiracitam seemed to also produce a strong concentration effect. But I didn't take it enough to get good data due to fears about it being addictive."
MachineLearning,And?
MachineLearning,I liked your piece and look forward to the next chapter.
MachineLearning,"There was a recent post in /r/math about the most important inequality: [health &gt; math](https://www.reddit.com/r/math/comments/m0yl5l/most_important_inequality_in_mathematics/). Not ""health"" as in ""healthy lifestyle"" but rather as in ""keep yourself sane and out of a hospital"". You can apply this inequality to just about anything in life, but it seems particularly relevant here.

This is not a lesson you want to learn from personal experience. Sure, both Paul Erdos and Keith Richards may have done a lot of drugs and still be famous and successful, but they are truly outliers. When under stress, even something simple things like having a beer or two every evening can quickly turn into alcoholism and having an infringed sleep schedule can quickly turn into nightmarish insomnia. Don't try to push your body to do more than it is capable of. Your body tells you when it's tired. Listen to it. Steady slow progress is much better than oscillating between being hyperproductive and having mental breakdowns."
MachineLearning,"You need to provide more information about what the authors claim. Training on the test set is not inherently wrong; it all depends on the claims made in the paper. For example, in the [Deep Image Prior](https://en.m.wikipedia.org/wiki/Deep_Image_Prior) paper, you train on a single image (which in some sense is your test set) and assess how well the network denoises the image (amongst other tasks). Since there are other ways to assess the model, e.g. signal to noise ratio of the resultant image, having a test set is not strictly necessary for demonstrating an improvement."
MachineLearning,Just by looking at the picture I am wondering why ProGAN is less rated in Train Speed than DCGAN... due to the architecture of the progressive growing GAN the training speed should be automatically boosted or am I wrong?
MachineLearning,"u/MagicaItux, can you join the community slack so we can notify you once the demo is ready? (hit ""@Mikayel"" so he knows of you if you decide to join)."
MachineLearning,You're Welcome
MachineLearning,"Thanks for your answer.
I cannot divulge the use out of privacy considerations. A simple cat/dog recognition demo would be fine."
MachineLearning,Hiya u/logtableturntable! :) thank you so much. We respect the work the dvc community is doing! The main difference is - DVC focuses on version control based on a file system. Core of the Hub is the storage layer for storing arbitrary tensors/large arrays. For versioning we do tensor deltas which are more efficient than blob differences.
MachineLearning,"&lt;former CS professor&gt; Not a good sign.  faculty come in several flavors 

* The superstar - has tons of students, money, contacts.  Ridiculous long CV. Only downside is if you are marginal you may not get attention and get dropped
* young gun - just arrived from XYZ elite university - super-pumped to work with you.  Only downside is he or she will have no money and you might have to teach (or work drive through at McDonald’s) to survive
* the teacher - chose at some point to focus on teaching.  No money, but lots of little side projects.  Might actually be very technically competent If not very published.  If you are a bit of a project yourself and need a lot of one on one attention might be okay for a low-arc trajectory.
* the admin - abandoned technical work a million years ago for meetings and PowerPoint and hopping job to job to climb towards university president some day.  Hard pass
* the codger - has a digital clock they made counting down to retirement, half-asses everything. Again, hard pass

you might have an admin type who has money through connections and wants to add pushing through a student to the resume.  Only advisable if you are a complete self starter and fast enough to graduate before your prof takes that next hop as Associate Dean at MaryThyCousinville State U."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you for this! I just started my literature review on generative models and this is perfect
MachineLearning,"First off, cool work :)
I was curious how the data versioning that you do compares against dvc."
MachineLearning,"Hey u/Sea-Category-9446, thanks a lot for the question - sorry for the delay in the response as I was reviewing the project. 

To my best knowledge:  


1.  quiltdata feels like a convenient way to manage your s3 data. We allow GCP/Azure/local deployment. 
2. More importantly, it doesn't structure unstructured data like we do. Thanks to chunking and other nifty tricks, we make it easy to access any slice of the data, modify, version control it, and make it streamable (regardless of the size of the dataset). 
3. I see Quilt does offer some sort of preview of the files (like jpeg, pngs, jsons etc), but datasets uploaded to Hub or your S3 (working on getting rest of the clouds supported) can be almost instantly visualizable through [app.activeloop.ai](https://app.activeloop.ai) with all their bounding boxes/tensors). 

Hub does work in tandem with MinIO pretty well. If you want to give it a shot, join the community (slack.activeloop.ai) and hit ""@Vinn"" and ""@Mika"" up and they'll jump on a quick call to walk you through everything if you'd like.  Happy to support you in the implementation!"
MachineLearning,The problem with deep learning is that there is too much hype around it. It is a bag of tricks that seems to work and nobody knows exactly why. What works in one dataset is crap when tested with a different dataset.
MachineLearning,"I watched [this video](https://youtu.be/BwhBtvCNwxo) earlier that basically encouraged different approaches to machine learning. Though, they seemed in agreement that it is much riskier as time can be lost that could have been used for publishing incremental improvements - which can negatively affect a person in their employment.

I’m not very experienced in machine learning myself though."
MachineLearning,I took the official figures from both papers (Table 7 in the MAF paper and Table 1 in RealNVP). MAF compares smaller models and comes out on top but this table compares the best obtained numbers.
MachineLearning,"Yeap, it is a great source for datasets. E.g. ImageNet is impossible to download for an average human being from the official source. It is easier to download from this torrent site. Don't forget to seed back for a while if you can!"
MachineLearning,wow never seen that site
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Did you run the experiments for NLL in Table 1? Surprised to see RealNVP perform so well in comparison to MAF.
MachineLearning,If you're at the point where you need introduce autoscaling and caching you would want to look at a more efficient runtime first.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks! We were sure we cited this (it was 100% meant to go in), but yeah apparently missed it - will add it in the next revision."
MachineLearning,"Great survey!   


Re missing papers: some shameless suggestion for sec 6.4 ;-) [https://arxiv.org/abs/2002.06707](https://arxiv.org/abs/2002.06707)"
MachineLearning,That’s why I’m looking for interpolation-free approaches
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Scrapes the internet. Finds porn. **Suprised Picachu**
MachineLearning,Link to the video: [https://www.youtube.com/watch?v=M-NTkxfd7-8](https://www.youtube.com/watch?v=M-NTkxfd7-8)
MachineLearning,[https://www.youtube.com/watch?v=M-NTkxfd7-8](https://www.youtube.com/watch?v=M-NTkxfd7-8)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Looks really interesting. This would be a good way to use our MinIO.

How would you say it compares to quilt/quiltdata ?"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"They really are some of our weekly articles. It just happens to have a logical continuation so I consider it to be a series. By the way, we are currently redesigning the website to solve issues just like that."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks I'll check that !
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The image itself does not contain text so OCR would not be applicable here. A possible scenario could be image and title, and from that to generate some extended summary."
MachineLearning,You can try some text location algorithm and then apply an ocr to the located text to get a digitised version of what was in the image
MachineLearning,May I recommend [org-mode](https://orgmode.org/)? I think it meets all the needs you mentioned. It offers quite a few different plug-ins that are perfect for research work.
MachineLearning,You are most correct.
MachineLearning,"OK cool, thanks"
MachineLearning,U will have to waith till March end...those are provisional papers...camera ready will be published later.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yep I saw this, but I didn't find any mapping from IDs to actual papers.

I guess those IDs are only for the people sending their papers to know if they got approved or not.

I'm looking for actual papers names/links"
MachineLearning,"[here is the list](http://cvpr2021.thecvf.com/sites/default/files/2021-03/accepted_paper_ids.txt)
They gave the IDs of accepted papers."
MachineLearning,"Thanks!! This is really cool, and I'm bookmarking it for future reference. But how come it is so hidden? There is no indication that there are article series like this on the website or sitemap or anything."
MachineLearning,"I like CNNs a lot. Do you think they will still have use-cases for NLP? I guess in all tasks that require more complex reasoning, transfomers (trained with self-supervision and fine-tuned), will outshine everything, for a long while (forever?). 

However, in tasks that are more simple, i.e., tasks that have strong phrasal signals (e.g., SPAM/HAM classification, etc), I'd say that CNNs still have there heads up. Especially from an efficiency viewpoint."
MachineLearning,"Flask on its own is definitely not comparable with model servers. However, in my experience, when backed with uwsgi and even nginx is perfectly fine for small to medium applications."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You make some good points here. However, I'd argue that these are all very dependent on the use case. For example:

\- Techniques like pruning and quantization, although very useful, don't always provide significant value (especially for smallish models)

\- Same is true for model servers. UWSGI or Gunicorn is perfectly capable to handle big loads of traffic. For sure once you reach a certain threshold, TFX and models serves definitely worth a try

\- GPUs or TPUs are super important but not always necessary for inference. CPUs are often enough for a simple forward pass ( again it depends on the model, I'm not talking about a huge transformer here)

Thank you very much for the feedback. Perhaps I can write a few more articles covering some of the topics you mentioned"
MachineLearning,I feel the opposite. In pytorch don't have to know the input and output sizes of each layer to create a sequential layer?
MachineLearning,One thing I didn't like with pytorch is that you had to know all the input and output sizes for the layers. It was much easier in tensorflow to create sequential layers.
MachineLearning,backtickopt6
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Artificially leaving out data leads to unknown biases. If you don't want it to produce certain images then a filter should be used to throw out generated images you don't want rather than prevent it from generating those images at all.
MachineLearning,"I'm working on a project for CCG (such as `HearthStone`, `Yu-Gi-Oh!`, etc.) which does classify (or give labels) deck type when user uploads their own deck.

let's assume every cards are having their own IDs and the ID would one of alphabet character (and It should be unique). now you can assume that there are 26 cards only. 

and every player can make their own deck but you should keep in mind that basic form of deck structure are already given. players can just modify small amount of cards with their taste.

let me give you guys a example:

    Assume that there are many basic known deck types.
    every each alphabet character means card ID and order is not considered.
    every deck should have 13 cards.

    e.g.)
    
    A A B B B C C C C D E E F - This deck called 'Foo'.
    C C C C F F G G E E E Z Z - This deck called 'Bar'.
    
    // and so on...

in this circumstance, I had come up that the problem is occurred here: 

&gt; If user can modify small amount of cards on their deck while maintaining basic structure of their deck type, how can we make computer can classify given deck?


so I had to decide to use Similarity Measuring which just compare between given deck and _**pre-defined deck**_ and calculate similarity. so we can assume which type given deck has when it have high similarity. (using [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) or some else algorithm)

but the only problem is `Using deck pre-defined by developer`. we've assumed that there are just 26 cards above but in real case, there are like 10k~ cards and deck type are newly created in every 3~4 months. I don't even know which decks are available out there. there are literally too many deck types to pre-define on my own.  :')

and I know above method is called `Supervised Learning`. I think `Supervised Learning` is quite challenging for this case since when new deck type have created by user community, Program wouldn't classify the type of given deck correctly when given deck type is newly created recently. 

that's why we should define it every single time new deck type has released. I think that's the pain in the bum. totally waste of time/effort.

so finally I've decided to use `Unsupervised Learning` method which don't need pre-defined data for classification. and we know that a deck is just sequence of numeric data (Card ID). all we have to do is this:

    1. Somehow convert deck (or array of Card IDs) into euclidean space
    2. make groups (or cluster) by using K-Means or whatever algorithm
    3. Now we can just label every groups (or clusters) to define name of deck type.

This simple but this is the reason why my head is about to blow up 😂

so my question is:

&gt; How can we convert series of Card IDs into eucliean-space of clustering? Is it even possible? If this is impossible, Is there any method to group (or make cluster) series of data?"
MachineLearning,"Plus one to both mostj and ZombieLeCun.

Your background and current training suggests that business / data analyst position may be a good fit. If you have experience in computer science, data engineering (DE) might be another viable route.

I recommend [Microsoft Ignite Data Analyst Challenge](https://csc.docs.microsoft.com/ignite/registration/March2021) to see if data analytics is your liking.  I think the course content is informative and industry-oriented.

Most folks lurking in this subreddit are ""hardcore"" machine learning researchers and practitioners. In this regime, young professionals are typically PhDs.

You may get more feedback from r/datascience. Good luck!"
MachineLearning,+1 Sonnet is really for research
MachineLearning,"[Fixed formatting.](https://np.reddit.com/r/backtickbot/comments/m11986/httpsnpredditcomrmachinelearningcommentskh2b81d/)

Hello, async3619: code blocks using triple backticks (\`\`\`) don't work on all versions of Reddit!

Some users see [this](https://stalas.alm.lt/backformat/gqaz5by.png) / [this](https://stalas.alm.lt/backformat/gqaz5by.html) instead.

To fix this, **indent every line with 4 spaces** instead.

[FAQ](https://www.reddit.com/r/backtickbot/wiki/index)

^(You can opt out by replying with backtickopt6 to this comment.)"
MachineLearning,"I'm working on a project for CCG (such as `HearthStone`, `Yu-Gi-Oh!`, etc.) which does classify (or give labels) deck type when user uploads their own deck.

let's assume every cards are having their own IDs and the ID would one of alphabet character (and It should be unique). now you can assume that there are 26 cards only. 

and every player can make their own deck but you should keep in mind that basic form of deck structure are already given. players can just modify small amount of cards with their taste.

let me give you guys a example:
 

```txt
Assume that there are many basic known deck types.
every each alphabet character means card ID and order is not considered.
every deck should have 13 cards.

e.g.)

A A B B B C C C C D E E F - This deck called 'Foo'.
C C C C F F G G E E E Z Z - This deck called 'Bar'.

// and so on...
```

in this circumstance, I had come up that the problem is occurred here: 

&gt; If user can modify small amount of cards on their deck while maintaining basic structure of their deck type, how can we make computer can classify given deck?


so I had to decide to use Similarity Measuring which just compare between given deck and _**pre-defined deck**_ and calculate similarity. so we can assume which type given deck has when it have high similarity. (using [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) or some else algorithm)

but the only problem is `Using deck pre-defined by developer`. we've assumed that there are just 26 cards above but in real case, there are like 10k~ cards and deck type are newly created in every 3~4 months. I don't even know which decks are available out there. there are literally too many deck types to pre-define on my own.  :')

and I know above method is called `Supervised Learning`. I think `Supervised Learning` is quite challenging for this case since when new deck type have created by user community, Program wouldn't classify the type of given deck correctly when given deck type is newly created recently. 

that's why we should define it every single time new deck type has released. I think that's the pain in the bum. totally waste of time/effort.

so finally I've decided to use `Unsupervised Learning` method which don't need pre-defined data for classification. and we know that a deck is just sequence of numeric data (Card ID). all we have to do is this:

```txt
1. Somehow convert deck (or array of Card IDs) into euclidean space
2. make groups (or cluster) by using K-Means or whatever algorithm
3. Now we can just label every groups (or clusters) to define name of deck type.
```

This simple but this is the reason why my head is about to blow up 😂

so my question is:

&gt; How can we convert series of Card IDs into eucliean-space of clustering? Is it even possible? If this is impossible, Is there any method to group (or make cluster) series of data?"
MachineLearning,"That's why I am worrying about how really ""safe and encrypted"" the data are"
MachineLearning,"I don't have any experience with scaling PyTorch, unfortunately. I hope someone else can give some good advice."
MachineLearning,"Having used TPUs on colab for Jax, performance seems sub-par and i would rather use the GPU instead. Is there a trick to correctly utilizing the TPU’s? I heard much higher batch size but that only made training comparable to Single GPU training, not outperforming it."
MachineLearning,Thank you for the updates!
MachineLearning,"Hmm.

There are 128 time series datasets in the UCR archive \[a\] (including several ECGs). However, in spite of hundreds of papers that look at them, there is very weak evidence that CNNs are any better than *simple nearest neighbor*.

If your time series is best characterized by features, rather than shape, then perhaps a CNN is a good idea. However, I would encourage you to try simple nearest neighbor. Not only is it competitive, but it is direct and intuitive. You can often see opportunities to improve things, for  example, achieve a new invariance that is important in your domain \[b\], or trim a meaningless prefix \[c\] etc.

When it comes to ECGs...

Paper \[d\] suggests an approach  based on deep learning that it took 4.5 hours  to train the model and then took 0.05 seconds to classify each  beat. In contrast, simple nearest neighbor (the MASS algorithm) it takes 0.00000294 seconds to classify each beat, and the accuracy is better. 

&amp;#x200B;

\[a\] [https://www.cs.ucr.edu/\~eamonn/time\_series\_data\_2018/](https://www.cs.ucr.edu/~eamonn/time_series_data_2018/)

\[b\] [https://www.cs.ucr.edu/\~eamonn/Complexity-Invariant%20Distance%20Measure.pdf](https://www.cs.ucr.edu/~eamonn/Complexity-Invariant%20Distance%20Measure.pdf)

\[c\] [https://www.cs.ucr.edu/\~eamonn/psi\_DTW\_10pages.pdf](https://www.cs.ucr.edu/~eamonn/psi_DTW_10pages.pdf)

\[d\]  Ozal Yildirim et al. 2019. A new approach for arrhythmia  classification using deep coded features and LSTM networks.  Computer Methods in Biomedicine 176, 121–133."
MachineLearning,"Impressive organisation, using the feed tool sounds like a great idea to have everything in place after the time one spent to set it all up, I should think about it. Maybe not to overload yourself, but then it's much easier to pick the most interesting things too, I guess.

I hope you devote some time to rest/free time! I find that it is, apart from good amount of sleep, very important to function properly for longer period of time, to be actually productive. Also to realize that the work is not the only important thing in life. Apart from that, having one day per week completely off from work is crucial for me. In general, I think it's important to stress how much the ""not-working"" time actually affects our work itself. With the same idea in mind, I don't think you should envy those being able to sleep so short. Getting enough sleep is a prerequisite for being productive and also there is a biological need for 7.30-9h of sleep. The effects of short sleep may be not seen directly bit could manifest in some time simply affecting your general health state."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The number of people who want to build models and throw prototype jupyter notebooks over the fence and call it a day, is fairly saturated.  On the other hand, it's hard to find talented people that can manage the lifecycle of a continuing evolving ML system and think about data contracts and integration challenges between a myriad of sources all at large scale.  Arguably it doesn't get much of the limelight with all the press releases about ML advances, but it's an essential component for companies building these types of ML systems."
MachineLearning,This right here.
MachineLearning,"Honestly though, the AI deserves to know what we look like naked."
MachineLearning," the problems you're pointing out aren't solved by unit tests - that's not the purpose of unit testing. Unit tests test the reliability of code, not the validity of machine learning models. test cases to validate your model and unit testing are not the same thing"
MachineLearning,"Open zotero.

Find a paper I might be interested in, read the abstract. If sounds good, glance at the pictures/graphs. If sounds good, click the save to zotero button.

I categorize by type (book, website, blog, conference long paper, short paper/technical report, poster, news article, journal article), by field (computer science, software engineering, ML, statistics, medicine, engineering etc.), by project I'm working on and by topic (NLP, computer vision, unsupervised etc.).

Making notes/annotations is a waste of time unless you know you'll need them for a specific paper. In that case you'd scribble them down in your notepad/post it note/note taking app and transfer into your paper as a comment/TODO/outline ASAP. Trying to deeply understand a paper unless it's for a specific project is a waste of time."
MachineLearning,"I also finding structuring impossible. Insignificant bug could take three days, and screw the schedules. It even worse if it's algorithmic or mathematical deficiency or some unpredicted effecs. Weeks or months could go into garbage bin easily."
MachineLearning,"I have to strongly discourage using anaconda. I used it for many years in the past and am finally glad I broke away from it. pip and virtual environments is much cleaner and much less of a headache. Conda will randomly give problems and for newer users, the mix of conda and pip can be pretty confusing"
MachineLearning,"Well I just creade markdown files of every topic i cover and put them properly inside a heirarchy of folders. Later I also maintain these folders on github/gitlab so that I can view them online from any device as github/gitlab renders markdown files directly.


I also have a tab so sometimes I write notes convert them into pdf and then put them in the folders as mentioned above."
MachineLearning,Who on Earth knows? This is like asking people what the stock prices will look like in 10 years e.g. everyone's got an opinion
MachineLearning,u/Nopaste DVC focuses on version control based on a file system. Core of the Hub is the storage layer for storing arbitrary tensors/large arrays. For versioning we do tensor deltas which should be more efficient then blob differences.
MachineLearning,"u/fredtcaroli we do have community effort on benchmarks as shown here [https://github.com/activeloopai/Hub/tree/master/benchmarks](https://github.com/activeloopai/Hub/tree/master/benchmarks) and we would love to add TFRecords comparison. 

On another note, as opposed to TFRecords, Hub datasets are totally indexable and any slice can be accessed."
MachineLearning,"It got delayed by a *lot* of government and medical-industry employees. I got grabbed by government employees, injected, and interrogated, and no one did anything, and no newspaper ran any story about my disappearance at the hands of government-employed extortionists and thugs, so no one is expecting anything from me, let alone the results of my work

https://www.reddit.com/r/WatchRedditDie/comments/lt845f/rcogsci_is_it_just_me_or_is_every_single_science/"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I don't blame you for that. The warrant will come but not yet.
MachineLearning,I do not believe your claim without a warrant.
MachineLearning,"we will see about that sir

    │  │  I&gt; [evalB '[[opScaleVal [sinusoid a] s] x]]
    │  │  │  
    │  │  O&gt; [* [circle [* x s]] s]
    │  
    │  In [184]&gt;"
MachineLearning,"A baby can't really pass the Turing test, though.  At least not right away."
MachineLearning,The closest guess I can make is that you had a baby.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Cool!
What are the differences respect to DVC?
https://dvc.org/"
MachineLearning,"Flask is good start, this is overall excellent starting point for low to mid ML production (maybe 10-15 models in production and monthly retraining).

TF Serving/NVIDIA Triton/Seldon Core and simmilars are necessary for more complex situations (in scaling, number of models etc)"
MachineLearning,"Seems pretty cool! Just wondering, are there any benchmarks I can take a look at? Would love to use something like this, but I want to understand how it compares to reading TFRecords from S3 using TFRecordsDataset."
MachineLearning,Flask no good
MachineLearning,Your model and mine are a bit different but perhaps this will be useful: [https://github.com/wingman-jr-addon/wingman\_jr/blob/5ef392764995922b973ab649bc53776eada95ef8/processor.js#L97](https://github.com/wingman-jr-addon/wingman_jr/blob/5ef392764995922b973ab649bc53776eada95ef8/processor.js#L97)
MachineLearning,Michael Bay movies are getting weirder and weirder these days
MachineLearning,"Not the commenter, but it is so simple to see a paper online, click the extension, and bam... it is in my Notion database. Can add tags, dates, etc. Best part? If I need/want to take notes on that paper, each element in a Notion database can open to a page."
MachineLearning,"But that type of graph thinking I was referring to was non-hierarchal graphs (i.e. stochastically spatial). That can't be represented as trees and hence, that was my point."
MachineLearning,"That is indeed true, but not the reverse (which we seem to agree upon)."
MachineLearning,"There're additional generated poems available at [https://workbench.data.aliyun.com/experience.htm#/paiAbilityVenue?defaultActiveKey=m6&amp;moduleName=m6-poetry-gen](https://workbench.data.aliyun.com/experience.htm#/paiAbilityVenue?defaultActiveKey=m6&amp;moduleName=m6-poetry-gen) ! Though all samples are pre-recorded without variation, and customized inputs/prompts are not currently accepted, like in OpenAI's DALL-E blog post; however there's already a link to request M6 API access."
MachineLearning,Lots of people generate pornography in reference to pokemon haha
MachineLearning,"I don't think people have understood neural network that well, but the number should be quite high if I need to guess. Think about the existence of adversarial examples. At every almost location in the input space, you can find a small perturbation to make the prediction to change drastically. So it's likely that when you zoom into the input space it will show much finer detail (like a fractal pattern, although it should be strictly finite due to the continuity of the parametrized function)."
MachineLearning,Yes. It also generates nsfw content when given nsfw labels. Which is more expected so I didn’t mention it.
MachineLearning,I don't think I did.
MachineLearning,And all trees are graphs.
MachineLearning,Did you try generating NSFW content with it to see what it'd come up with?
MachineLearning,"As a PhD student at a big ML lab in the US and being originally from Europe myself, I hate to say it but stimulants are too common in my current department.

**I take it**, my **previous advisor** took it, my **current advisor** takes it, and around **20-30% of my cohort** takes it. Even our **director of graduate studies** takes stimulants.

To everyone who doesn't take stimulants and thinks I'm exaggerating: I had no idea so many people took it until I disclosed to some people that I was taking it myself.

**Short story for context:**

I was never diagnosed with ADHD in the first place (it's not such a popular diagnosis in Europe), but when I started feeling exhausted from overworking in my first year (up to 90 hrs/week) I was directed to a psychiatrist who prescribed me ritalin 10 min into our first appointment.

It's been 3 years since I've been on ADHD meds and eventually I disclosed that to some people near me (advisors, friends, DGS, etc), mostly because I sometimes would run out of pills (thanks to terrible university health insurance) and be completely dysfunctional until I got a refill, for example missing deadlines, forgetting/sleeping through meetings and whatnot. All the people I know in my department who take ADHD meds only disclosed it to me once I told them I myself took ritalin (and I only did that if they started asking if I was sick or burnt out during the periods I was out of pills), meaning that if I didn't take ritalin I wouldn't know any of them took ADHD meds. For example, I already heard a labmate claiming that he was 100% sure nobody in the department took ritalin.

As for whether taking it matters or not, the ""research output"" of my colleagues who take ADHD drugs (and me as well) is incomparable compared to the ones who I'm sure don't take anything, as in we have from 10 to 15 published papers while other candidates have at most 5 (we're all 4th year). We're not smarter, we just work twice as much and our advisors give us better projects."
MachineLearning,"&gt; Did anything significant happen between RNNs and transformers?

Incorporation of CNNs for NLP. CNNs were probably the transition bridge for NLP from going from RNNs to Transformers. Traditionally, we were using RNNs and variants for NLP and CNNs for vision. But simple CNNs found good success with less compute in text classification. Next CNNs were also converted to different forms of language models and seq2seq models. And generally they were often better performers too in multiple tasks. There were competitive CNN-based models even after Transformer.
I suspect the success and presence of CNN-based Seq2Seq were critical for the development of the idea of Transformers. (Imagine trying to extend the window size of CNNs from k to infinity, or rather adaptive to sequence length, and then using attention mechanism to gain the adaptivity; and you have hit upon the core of Transformers) 


&gt; aka context-free approach?

Yes, they aren't context free."
MachineLearning,Very interesting articles. Thanks for sharing!
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Maybe you are underestimating the quantity of Pokemon cosplay photos (and erotic cosplay) available?
MachineLearning,"Hey u/rando_techo, hope you're doing well today. While we do only have a paltry sum to award, there is no dressing up here. 

Many office buildings in this real estate market have previously unidentified ""vampiric"" loads--electricity consumption not tied to known systems and uses. COVID-19 taught us to question commonly held assumptions, like tenants are responsible for 60-90% of energy consumption within these buildings, and outside weather conditions are a major driver for consumption within tenant spaces. 

Given that [more than 70% of NYC's greenhouse gas emissions come from buildings](https://www1.nyc.gov/assets/sustainability/downloads/pdf/publications/TWGreport_04212016.pdf), we want to offer predictive modelers around the world the opportunity to join this conversation. That's why we've made available a real-world test set with 2.5+ years of building data, so that more than the usual suspects can help solve [the Great Energy Disconnect](https://www.urbangreencouncil.org/content/events/great-energy-disconnect). 

There will be resiliency and quality of life benefits, e.g. fewer brownout conditions and better air quality, associated with solving this challenge citywide as well. So, what do you think? Should electricity consumption in tenants spaces be strongly correlated with building-wide occupancy? How correlated is humidity with this consumption? 

Hopefully, many more voices, including first-time energy modelers, will tackle these questions through this effort. 

Best,

The PropTech Challenge team"
MachineLearning,"I just download every paper I find and claim that I know the knowledge because I possess the paper in some repository of papers.

I think the word you mean is “information,” or “notes,” or “sources.”"
MachineLearning,You assumed OP's intent without reading the article.
MachineLearning,"Non-AMP Link: [https://terrytao.wordpress.com/2008/08/07/on-time-management/](https://terrytao.wordpress.com/2008/08/07/on-time-management/)

I'm a bot. [Why?](https://np.reddit.com/user/NoGoogleAMPBot/comments/lbz2sg/) | [Code](https://github.com/laurinneff/no-google-amp-bot) | [Report issues](https://github.com/laurinneff/no-google-amp-bot/issues)"
MachineLearning,"It looks like you shared an AMP link. These should load faster, but Google's AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot). Fully cached AMP pages (like the one you shared), are [especially problematic](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).

You might want to visit **the canonical page** instead: **[https://terrytao.wordpress.com/2008/08/07/on-time-management/](https://terrytao.wordpress.com/2008/08/07/on-time-management/)**

*****

 ^(I'm a bot | )[^(Why &amp; About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon me with u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)"
MachineLearning,"I miss my days as a student. Now it is a real mess. It consists of one hour to send emails, two hours in the meetings, one to three hour to answer questions for postdocs or collaborators, and a few hours to do actual works. 

Reading Terry Tao’s blog may help: https://www.google.com/amp/s/terrytao.wordpress.com/2008/08/07/on-time-management/amp/"
MachineLearning,PM'd you
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The answer to your question is “both”; I used it regularly, as it’s needed regularly. The way I see it, ADHD doesn’t take a day off, so I don’t skip days with meds. It’s like glasses for your brain, and people still wear their glasses on weekends. ;) 

Even if I’m not working, I need to run errands, take care of myself, pay attention to my spouse and kid, do things around the house, etc.

I don’t like skipping because I get scattered, my anxiety comes back, I get back to a state where I’m entirely unable to start anything unless I’m stressed. It sucks."
MachineLearning,Thanks u/ijyliu_1998! Happy to see how we can be helpful to enable working with data and run Machine Learning efficiently!
MachineLearning,"u/B-80 We are building partnerships with storage providers and doing our best to have free open research datasets free to be stored. 

We also help companies to manage their internal data. This is where we make money. :)"
MachineLearning,"I think multi-scale in this case refers to combining features from different scales in the U-net architecture to predict final bounding boxes (e.g. in layer with resolution 24 by 24, 48 by 48, 96 by 96, and 192 by 192, etc). The assumption is that big objects are picked up by lower resolution layers and smaller objects by higher res layers, hence combining them leads to better results. Single-scale is the opposite, i.e. only use features from the highest resolution output layer."
MachineLearning,"I think it depends on the topics you are interested in. If your goal is to apply ML algorithms to certain applications then I would recommend the course https://fast.ai which helps you to get started coding ML models quickly. If your goal is to get a sense of the theory behind ML (e.g. how do convolutional neural networks work, differences in loss functions, etc.) then I can highly I recommend CS 231N for Computer Vision (which aged really well imo) and CS224N for NLP from Stanford. You can find videos of these courses on YouTube."
MachineLearning,"These are awesome, thanks!"
MachineLearning,"Hi, sounds like an interesting problem - do you know if the dataset used by the translator from your university is publicly available? I think having high quality English&lt;-&gt;German (in this case) patent pairs is crucial for this ML application"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,That is the point!
MachineLearning,"&gt;pedantic

Cool but you missed the point :("
MachineLearning,Not all graphs can be trees
MachineLearning,"Even better, there is Common Lisp code for their examples. [http://incompleteideas.net/book/first/code/code.html](http://incompleteideas.net/book/first/code/code.html)  


there are a few C examples also."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Who says people aren't nice on the internet? 😂
MachineLearning,Love you
MachineLearning,"I did this when doing my masters. I'd take a wide variety of nootropics, keep a diary of self-reported statistics, then did some statistical tests to find out if any gave me a concentration boost that was greater than my average variance between days."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Sounds cool and indeed superior to clogging up my repos or begging the sys admins to let me install git lfs (which often caps out at too small of a size anyway)!
MachineLearning,"So, just like every human."
MachineLearning,"Help to fight climate change? From your site you have the following statement: ""The purpose of this effort is to identify new modeling assumptions that account for COVID-19’s impact on occupancy levels.""

Sounds to me like you're dressing up a quest to predict a money-making aspect of a highly lucrative real estate market as a climate change issue and for a paltry sum at that."
MachineLearning,"How can you afford to store and serve ""petabyte"" scale datasets? Is this a paid service?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; I thought Sutton and barto RL may be less practical (no code) than I am used to... but I may be wrong.. ?

You are wrong. Sutton and Barto's book gives very clear pseudo code of lots of RL algorithms which I have been able to implement successfully as actual code with little very little difficulty.

If you can't translate pseudo code into actual code then you should stop learning ML and take a remedial class on programming before continuing with ML."
MachineLearning,But do you prepare your body in various ways?
MachineLearning,I missed that when I read it. Just found it as the exotic part under emotions.
MachineLearning,"I agree, that's why I only read them if they're deemed essential. Otherwise I'd usually play some video games or read some books. I try not to read books about AI/ML though. I don't want to end up like one of those researchers who know nothing outside of ML."
MachineLearning,Jordan Schlansky
MachineLearning,I said it was a consideration here and could be a future problem. Not advocating for anything specifically. Just something to be aware of.
MachineLearning,I’m not really taking a position on whether it should or shouldn’t be trained on nsfw material. But I do believe it’s useful to disclose that. Im more curious as to the odd connections it makes between mundane things like Pokémon and nsfw material. Or like what someone else posted how it won’t produce tarot cards.
MachineLearning,I mean there’s both arguments for and against such a thing learning nsfw content. I’m not really taking a position there and apologies if it seemed like I was. I do think it should be clear and disclosed though. I’m also just curious as to the connection here with Pokémon and what other weird connections connections haven’t been discovered.
MachineLearning,At some point you have to chose. Do you want a general classifier that can't classify nudity or bare skin or do you include data representing those classes. Not arguing for porn but being naked is very human and in some sense should be represented.
MachineLearning,"&gt; My concern is more that it’s generating this material in reference to Pokémon. 

Oh, from your post I thought that you may have had a problem with it learning nsfw content, and not just it confusing nsfw content and something else."
MachineLearning,"But that doesn't stop it from learning about nudity via artwork and then matching that with real human bodies. Anatomy information will also give it the information it needs to create pornographic content.

Also in your example, some of the results look like a swimsuit. I would imagine it's possible to learn what humans look like naked while being provided only clothed images of humans."
MachineLearning,This is great info. Thanks for the thread. It does sound like parts of the language aspect stayed in.  I thought of the possible photos slipping in with relation of Pokémon but I figured they would be drowned out with all the normal Pokémon content. Tarot cards is an interesting one to hear fails. There’s probably a large variety of hidden failures/weird connections we haven’t heard about too.
MachineLearning,Sounds more like a discussion for /r/ControlProblem instead.
MachineLearning,Looks great!
MachineLearning,"I don't believe current and near-future tech will allow human leven AGI due to the network complexity required but I do believe we are making huge progress on getting the basic ideas right (neuromorphic systems mimic nature's ways). There's the idea that network complexity is a limiting factor in AGI growth (intelligence limited by bandwidth), and that we are nowhere near capable at this point of creating a network with enough bandwidth capacity for AGI orders of magnitude more intelligent than us, so I think this system is somewhat naturally ""regulated"" already."
MachineLearning,"Obsidian is a good way to link together in a graph, but it is lacking in terms of LaTeX and as far as I can recall,  notion also lacks support for it. Personally, I write my notes in either Jupyter Notebooks inside visual studio for a practical course, labwork, assignments, or reproduced work. For theoretical stuff,  I write my notes in LaTeX, one pdf for each field/course."
MachineLearning,My view is that any regulation in this regard that benefits incumbents or creates barriers to entry is unacceptable.
MachineLearning,"&gt; The base query list is all words occurring at least 100 times in the English version of Wikipedia. This is augmented with bi-grams with high pointwise mutual information as well as the names of all Wikipedia articles above a certain search volume. Finally all WordNet synsets not already in the query list are added.

They also appear to have done some additional NSFW filtering and hashtag filtering: https://twitter.com/metasemantic/status/1353754850710978560

How well any of this *worked* is open to question. There are weird anomalies. RiversHaveWings for example spent a while trying to generate Tarot cards, like ""4 of Swords"". And it... just doesn't work. CLIP-BigGAN just won't generate reasonable Tarot card artwork. There must be a bazillion such images with the obvious textual caption, so you'd think it'd work great, but it fails. Given CLIP's overall uncanny intelligence, this points strongly to the dataset being defective somehow - was everything Tarot-related just thrown out? That would explain it. (This is also true of GPT-3, IMO, a lot of artifacts in its outputs can be traced to the BPE tokenization being bad or the dataset formatting being bad.) Your Pokemon/porn association might be a similar problem; perhaps they managed to blacklist most hentai porn, but Pokemon-related hentai or furry images slipped through because they were whitelisted by ""Pokemon"" being such a common word on Wikipedia? Something like that."
MachineLearning,"ya i think it's kind of getting older, kids would play hard for 10 min then fall asleep right after and we don't quite do that"
MachineLearning,"aha kinda, or more like just yolo everything as much as you can"
MachineLearning,"I don’t think regulation necessarily needs to happen in ML research (with the possible exception of some topics like CV/deepfakes). I think regulation of ML deployment needs to happen because there are several examples of it being biased and unfairly treating minorities when deployed, and this bias is usually only detected when alarm bells are raised or an external investigation happens."
MachineLearning,"your concern would be as a hypothetical user of these and i agree on it. and nice catch, of course. 
but as a reader of the papers, i don’t think this is an issue."
MachineLearning,"Yes, I dropped NLP at the LSTMs and bi-directional architectures.

It is what I was looking for. Already clapped your article. Thanks!"
MachineLearning,yep. that's what I tell my friends who are still in grad school. you should leverage this as much as possible haha
MachineLearning,Same. I'm assuming something about quality publications over quantity?
MachineLearning,"This is a Fakespot Reviews Analysis bot. Fakespot detects fake reviews, fake products and unreliable sellers using AI.

Here is the analysis for the Amazon product reviews:

&gt;**Name**: Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems 

&gt;**Company**: Aurélien Géron

&gt;**Amazon Product Rating**: 4.8 

&gt;**Fakespot Reviews Grade**: A

&gt;**Adjusted Fakespot Rating**: 4.8

&gt;**Analysis Performed at**: 02-12-2021 

[Link to Fakespot Analysis](https://fakespot.com/product/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow-concepts-tools-and-techniques-to-build-intelligent-systems) | [Check out the Fakespot Chrome Extension!](https://chrome.google.com/webstore/detail/fakespot-analyze-fake-ama/nakplnnackehceedgkgkokbgbmfghain)

*Fakespot analyzes the reviews authenticity and not the product quality using AI. We look for real reviews that mention product issues such as counterfeits, defects, and bad return policies that fake reviews try to hide from consumers.*

*We give an A-F letter for trustworthiness of reviews. A = very trustworthy reviews, F = highly untrustworthy reviews. We also provide seller ratings to warn you if the seller can be trusted or not.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"No problem! Yeah, if you specifically want a minimum 11GB of VRAM then the 3060 is the best bang for your buck."
MachineLearning,"When I follow the link from github I still see the 404.
But great resource anyway, thanks for this huge job!"
MachineLearning,"&gt;aka context-free approach?

I'm not sure what you mean by that but transformers have as much context as RNNs

&gt;Did anything significant happen between RNNs and transformers?

Don't know when you dropped RNNs but basically we had GRUs, then LSTMs, and then Bi-LSTMs. Bi-LSTMs helped a lot because you could process sentences going from left to right and from right to left, it made training more efficient.

Transformers read all words simultaneously so it's even more efficient. And empirically they had better results than RNNs on some benchmarks.

&gt;Can you point me somewhere where I can educate myself in an efficient way having experience of RNNs?

There are many guides online, I wrote one [here](https://hyugen-ai.medium.com/transformers-in-pytorch-from-scratch-for-nlp-beginners-ff3b3d922ef7) (it also refers to other tutorials). It comes with a [python file with 300 lines of code re-implementing Transformers and Bert](https://github.com/Whiax/BERT-Transformer-Pytorch) (it's hard to do shorter)

If you want the TL;DR starting from RNNs: Transformers also use word embeddings but they compare all words two by two simultaneously to build a new context-rich representation for each word. So you have one word embedding (let's say of shape (1,128)), you process that embedding with the embeddings of all others words and you build a new embedding of shape (1, 128) for your first word, except this time it's contextualized. You do that multiple times and that's it.

But some people showed that Bi-LSTMs can still compete in some situations"
MachineLearning,"Thanks for compiling that resource; that's helpful. Have you had any chances to look into scaling PyTorch on a homogeneous, data parallel cluster yet? There is a specific reason why I ask - a bit of a backstory: 

I've been using TensorFlow extensively in the last ~4 years (academic research) and have greatly appreciated the introduction of TFDS and TF2.0, even though I was extremely hesitant at the beginning, since it unfortunately required a significant rewrite of certain aspects our pre-existing infrastructure. Undoubtedly the port was well worth it, especially from a modularity/clarity point-of-view, since a lot of our distribution backend felt like an abomination of ""hacky/duck-taped/monkey-patched TF1.X"". Distribution awareness and a clear way for ""how your custom features/modules are meant to be made distribution aware"" was huge leap in reducing the time needed to make ""research/lab code"" (that works well on a single-machine with multiple GPUs) scale on a HPC.  

The only time I've gotten in touch with PyTorch was when reading through contributions of other research teams; however, as it seems, this will change soon. We've had a ""*small break through*"" while collaborating with a research group we've gotten in touch with recently; and you guessed it right: they are all in on PyTorch.

I'd love to adjust and spin-up their codebase on our HPC, but I am a bit worried about scaling PyTorch, since I am not sure how well it does in that domain. I unfortunately can't find any high-quality examples/best-practices for distributed data parallel guidelines that scale into the double/triple-digits of compute-nodes (with 4x GPUs each). If anyone has some leads on this end, or tips since you've gone through this adjustment already, I'd happy to hear your thoughts."
MachineLearning,"If you are densely taking notes around paragraphs and you feel the need to write equations, graphs, or simply picturing a lot around the text I would recommend to give a tablet a chance. I do not have a direct experience on that, but in the past I saw colleagues doing that way and it looked very interesting. Maybe Mendeley has that feature already (in that case it would be perfect I guess). Just an idea, I cannot comment more on that unfortunately."
MachineLearning,Just wondering... Is this rendered live?
MachineLearning,"From the PI on the above paper: 

&gt; Also send them: https://pubs.acs.org/doi/10.1021/acs.chemmater.0c01153
Also send them: Kulik's book: https://pubs.acs.org/doi/book/10.1021/acs.infocus.7e4001"
MachineLearning,"How exactly does a neural network train on a batch on images? I get that it uses forward pass to predict and then a back prop to fix the loss. But that is for a single image at a time no? So for a training set of 100 pictures, each epoch the neural network would do 100 forward and 100 back prop. Is there a way for the neural network to do it all on one step but in a small amount ? Like some sort of gradient descent on all 100 at the same time."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I know what you mean, I'm just being pedantic."
MachineLearning,Tuned! This is awesome.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,But you are missing the random connections and nothing has a heirarchy
MachineLearning,"I agree, it was more of a joke. Ideally you run GSD to fine tune which hyperparameters to optimize, their ranges and initial values. Or more drastic changes to the entire pipeline or whay targets to optimize or what type of nodel to use in the first place."
MachineLearning,"This one they actually discuss a bit in the recent distill paper: https://distill.pub/2021/multimodal-neurons/

I have seen one or two others while clicking through the microscope randomly, but unfortunately don't remember the IDs."
MachineLearning,Wow good find. I had looked through some of the neurons but didn’t see one like this.
MachineLearning,"A data scientist at a data-driven company probably won't ever touch Excel or PowerBi. These are more tools for (business) analysts.

A data scientist usually focuses on the modeling: creating good features, setting right parameters, doing solid evaluation, causal inference, model monitoring, etc.

A machine learning or data engineer is the new ITops. All data infra engineers wanted to be in Machine Learning, due to prestige and higher pay, so they just changed the title and kept the job the same.

If you have computer skills: can write complex SQL queries, think in code not maths, deploy pipelines in the cloud, master Python and or Java/Scala, don't get existential crisis from looking at Tensorflow code, think hacking an end-point for prediction consumption and automatic retraining by posting a .csv is a good way to spend your weekends, I think entry-level data/ML engineer is feasible.

As for data scientist: All business analysts already switched :). Modeling tools were made so accessible that now former business analysts or DevOps are running an XGBoost or auto-training H20 or DL-code from Github. Lots of PhDs (you don't really need a PhD to do data infra, but you do kinda miss your PhD when having to read a 2020 ML paper on more effective  variational inference and re-implement it as a proof-of-concept). Unless you win a Kaggle competition, or have a great social presence, or contribute to their diversity and inclusion metrics, I don't think you will end up some place great: You'll be that ""data scientist"" doing dashboards and PowerBi visualizations, because there is no data for you to work with.

Being really good at SQL and Python should open up many doors for you. Don't focus so much on the title at first."
MachineLearning,"There's definitely NSFW material in CLIP's training set (and Imagenet, for that matter), and CLIP/DALL-E knows it when it sees it.  See for example this neuron:

https://microscope-azure-edge.openai.com/models/contrastive_4x/image_block_4_5_Add_6_0/1543"
MachineLearning,"I use a time tracker (toggl) to track how much time I spend on every category each day. I time all of my on-time including things like Literature Review, Writing code, writing papers, lunch, dinner, lifting, walking my dog, etc. Also, I plan my goals on a weekly basis ( I will finish this stuff by this day on this week). 

Usually like once or twice a week I look at what my goals are and how much time per day I'm spending on each goal. Then I make adjustments. I thought I was going to finish x section of my dissertation by writing 1 hour per day? No I better bump that up to 3 hours. My lunches have been 1 hour each day? I better pay more attention and get that down to 30 minutes. Etc.

It is a little stress inducing to time everything and micromanage myself sometimes. But being behind on everything will always be more stressful. Now, at least if I'm behind on everything I'm comforted knowing where my time went."
MachineLearning,"You are looking at evaluation methods for regression. Confusion matrix is for classification.

The metric you pick to optimize for, depends on the problem specifics: Do you just want to punish wrong predictions? Something like mean average error. Do you want to punish 1 misclassification of error 200 more than 100 misclassifications of error 2? Something like mean squared average error.

r2 is also commonly used, but I quite dislike that metric for arcane reasons.

See [https://scikit-learn.org/stable/modules/model\_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html) for more regression metrics."
MachineLearning,"The article linked creates a pretty weak strawman. No one would seriously consider loading a model before each request or running without proper multithreading.

I'm not sure if OP had any link to model optimisation (weight pruning, quantisation, Dropping training features from the model).
Those optimisations are important to get right, but in my experience a correctly setup flask + gunicorn setup will obtain response time similar to what something like TF model server will get you."
MachineLearning,I'm a PhD student in Europe and I use a performance enhancing drug: coffee!
MachineLearning,GSD isn't necessary a variant of SGD. GSD is a meta-optimization (could be meta-meta optimization too) process which typically use SGD in its inner loop.
MachineLearning,you're never going to make tenure off the back of one publication. its a body of work that matters. a single success is not enough.
MachineLearning,it all depends on how you define 'win'.
MachineLearning,Reading this makes nostalgic for grad school. You have so much more control over your schedule and so much more focus on the more interesting work. I'm in the industry now and you're definitely expected to churn stuff out much more mechanically. And being expected to be prompt with the small time-sucking things like email and other communication. All the while being able to demonstrate significant progress on any given project if randomly asked about it by a manager. I don't feel like I can arrange my time in a way that would be the most effective use of it like I was able to in grad school.
MachineLearning,"I would say it's very common, if not the norm. I wouldn't expect an undergrad to teach you the subject to any real depth to make you competitive in the market. And if your desired ML career will involve (or will likely involve) doing research than a Master's *at least* is a going to be a requirement. An undergrad like a Bachelor's doesn't teach your research skills."
MachineLearning,"Hey u/MagicaItux, yes, Hub can be deployed fully locally, but the on-prem data visualization via [app.activeloop.ai](https://app.activeloop.ai) is in the enterprise version of the product. Thanks for the tip - we would love to make a guide and I'll notify you once this is available. Do you have any specific dataset in mind (we can use our default one for benchmarks)? Out of curiosity, what's your use case?"
MachineLearning,is it the industry norm to have a Masters or PhD for ML? i’m in undergrad rn
MachineLearning,"I have two questions: 

- Is this usable while keeping all the data on premise?
- Would you be able to make a guide on using this with Google's TPUs?"
MachineLearning,"I wouldn't say MoE is new given https://arxiv.org/abs/2006.16668 and https://arxiv.org/abs/2101.03961 from Google; maybe it's new with multimodal training. The Alibaba group submitted https://arxiv.org/abs/2003.13198 last March introducing InterBERT, which became the first model of the M6 series and was renamed M6-v0 this January. The paper contains a DOI link to a KDD publication that doesn't work; maybe they submitted to KDD but were rejected?"
MachineLearning,I’m not discussing vetting the dataset manually. I’m wondering what if any automatic parsing was done. Like throwing out keywords or using classifiers to remove images they wouldn’t want or noise. That’s not discussed in the paper.
MachineLearning,"Of course there are, just like there are some weird humans in any society. Nobody vetted the dataset manually."
MachineLearning,I am not sure if there’s any of the shelf solution for this. I believe the reliability of the interpolation will highly depend on how large the data gaps are along with data availability. This is a nice paper for data gaps in time series with sinusoidal patterns: https://angeo.copernicus.org/articles/34/437/2016/angeo-34-437-2016.pdf
MachineLearning,[removed]
MachineLearning,"In my opinion, PyTorch is way more intuitive than TF. In PyTorch, APIs and modules fit together very nicely in a way that they don't in TF. TF also has thing horrible version 1 vs version 2 thing so code written against TF1 works very differently from code written against TF2. 

What keeps me on TF2 is that PyTorch doesn't have good TPU support on Google Colab. TF2 + Google Colab + the free TPU gives you excellent training performance. PyTorch doesn't yet have anything that can rival it."
MachineLearning,"You're quite familiar with Tao's poems! Have you spotted that 却顾所来径 苍苍横翠微 is ""plagiarizing"" Li Bai himself?"
MachineLearning,Great! Thank you!
MachineLearning,Someone forgot to blacklist rule34 for their webscraper
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Weibo is like Twitter and owned by Sina, with most content public; maybe you were thinking of WeChat?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Where are you now and what you're working on? The hot sauce is... Probably still in the office since the pandemic lollll
MachineLearning,"I have two, one is multiple classes of single objects, the other is similar, as in single objects however these are annotated images for use with yolo"
MachineLearning,My concern is more that it’s generating this material in reference to Pokémon. And wondering whatever other connections are out there. I do think the research is great and I’m glad they released part of dalle. But the paper doesn’t mention much about what type of data they use and i think people should be aware of this in case they are interested in using it in their products.
MachineLearning,"Thanks!

I'm working with some pre-trained models that are trained on 2080ti's so they tend to require 11GB+ of VRAM."
MachineLearning,My concern is more that it’s generating these features with the input of pokemon. So there are some weird connections being made there.
MachineLearning,What does your current dataset look like? Is it images of single objects?
MachineLearning,So what? It's trained from a large corpus of images scraped from the internet. Porn and copyrighted material make up a large portion of internet images. Image copyright doesn't apply to research (it's fair use) and it's unclear that copyright restricts analysis even in non-research settings. So what's the problem?
MachineLearning,Oh heckin no. Someone think of the machine learning researcher children who will be exposed to erotic content on the internet. They might see some semblance of a nipple while finetuning the vae.
MachineLearning,"I'm not sure if this has been applied to image generation, but there has been some work on learning from human preference in the RL field which might be interesting for you. See e.g. \[[1](https://arxiv.org/pdf/2007.12904.pdf)\] or \[[2](https://arxiv.org/pdf/2003.06495.pdf)\] as a starting point."
MachineLearning,"The developer suggested that Aphantasia is the replacement for this, so you might want to give that a try."
MachineLearning,But there are a lot of ways to parse out the data. Like keywords that you remove images from and/or nudity detectors. Twitter actually categorized the images on my post automatically as inappropriate.
MachineLearning,+1
MachineLearning,Have you looked at Detection Transformer (DeTr) by Facebook?
MachineLearning,"The RTX 3060 is a very good choice when building a DL rig on a budget imho. You might also consider RTX 3060 Ti or RTX 3070 which are at a similar price range with more CUDA Cores. But if price and VRAM are your main concerns, 3060 is a premium choice for doing DL at home."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The only way they could get enough data for these models is by scraping it off the internet automatically, and there is a lot of porn on the internet."
MachineLearning,Thank you for the answer!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Basically you can't do back propagation so you can't like stick some convolution layers under a decision tree and train the whole things end to end.

(There are some papers that try to address this but i'm not sure if any of them have seen much use)."
MachineLearning,[This](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) tutorial gives a pretty good overview of different activation functions to start with. You can probably follow that up with reading [this guide](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/) for some more in-depth discussion on activation functions for neural networks.
MachineLearning,"The discussion was about how ML is a huge, dense, rapidly growing field that's almost impossible to keep up with.  Apropos of nothing, that guy complains that social justice warriors are ruining the field.  And you feel that I've derailed productive discussion.  Uh huh.  

There's no obligation to let his silly, derogatory characterization stand just because his feelings are so hurt that he has to shoehorn them in to completely unrelated discussions."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Booze is the only drug I use. Like Ian Goodfellow when he invented GANs.
MachineLearning,"I am a researcher at a company (https://www.mvtec.com/company/research/publications), currently doing research in anomaly detection. We do a mix of implementing other papers and building our own models. Even with my TensorFlow background, I find PyTorch code easier to read. To me, the API seems more consistent but also more flexible compared to TensorFlow. You just have to learn a few concepts like nn.Module and then you can start building. In TensorFlow, on the other hand, they keep adding partly redundant functionality and yet I keep hitting the same barriers caused by the computational graph. I was very optimistic about TF v2, but the fact that the graph is still prevalent (for example in tf.keras and tf.data) makes TensorFlow cumbersome to use. Interestingly, I often read that PyTorch is as fast as TensorFlow."
MachineLearning,"respectfully - You've detracted from the OP's discussion.  I'm not a moderator, and I'm not an authority to mandate what's right or wrong.  I just think what you've done is unfair to people who are interested in Machine Learning."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,There was also a article that posted on Uber this morning about utilizing Horovod on Ray (Elastic Deep Learning with Horovod on Ray): [https://eng.uber.com/horovod-ray/](https://eng.uber.com/horovod-ray/)
MachineLearning,text2image fft stopped working for me
MachineLearning,I'm academic and commercial and made the switch from TF to PyTorch. I find it easier to have more control over things.
MachineLearning,Can you please elaborate how you use notion for paper reading?
MachineLearning,"I love ya'all. Progressing knowledge for humanity, one frustrated day at a time."
MachineLearning,play hard and study hard.
MachineLearning,"Thanks for this. Out of curiosity, are you academic or commercial? I implement recently published papers in tensorflow for commercial applications, and research papers seem to be primarily using PyTorch. I'm considering learning PyTorch because of that, but didn't want to wade through a bunch of junk trainings to build a toy model with no customization. What's your favorite part of PyTorch in comparison to Tensorflow?"
MachineLearning,"It's not about production. It's about reproducibility and understanding a model's capabilities. I think it's lazy on the part of academics who want to publish in this domain to just wave off test-cases like it's some lowly task done by software lackeys for ""production"". With such beliefs, no wonder the paper growth will be exponential and reproducibility will keep suffering. Deep learning is **not** as old as Newtonian Physics. It's less than a decade since it went mainstream and it is an ""Empirically measured"" domain. Yes, there is theory but a lot of research is not theoretical!. More than 50%+ of papers on ArXiv since 2020 are using ML methods for different problems and applications!.

A model giving a 90% top 1 error on image-net would be wowed by citations. But the information is incomplete because for that model I don't know what were the failure cases and the distribution of those. A lot of papers don't mention this and why should they. They are not incentivized to diss on a method for which they found shiny metrics.

Benchmarking in DL also made it a game where researchers are chasing the metric but granular understanding is not ""exactly"" provided all the time. 

Software engineers write test cases to make understanding of functions more robust. If you are ""researching"" a fancy deep learning model you are at the end making an ""approximated function"". Good test-cases are at the heart of robust functions which are clearly understandable. And to be honest they help research too!. They ground your understanding on what you hypothesize and what is the outcome. 

Yes in a lot of cases devising them would be hard/not-possible but for things where benchmarks are established, there should be more emphasis on the failure distribution. Test cases help with that!

If a paper clearly showed you test cases wouldn't you like reading about where they failed and succeeded?"
MachineLearning,Largely non-reproducible papers...?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The math symbols represent a field that hasn’t actually grown into its own. 

Additionally they serve as a barrier to entry for a lot of folks who haven’t had the tremendous privilege of a formal maths education. This, I think, is almost certainly in some folks’ minds (including some posting on this thread).

It reminds me a lot of the computer science curriculum I encountered 25 years ago; chock full of maths for no apparent reason. Certainly no reason that ever benefited me in my professional experience. But I suspect the roots of this field being both young and based in fields rooted in high level maths have contributed to the persistence of abstruse maths despite the lack of necessity for them in actually getting things done in the field. It was the same in CS way back then.

For those having issues with them, however, I would recommend building the equations in code. That way you’ll see the inconsistently-used letters and various flavors of Greek alphabet translate readily to functions and variable names. It’s nothing a decent programmer can’t grok—just a bit like trying to read code designed to be inscrutable.

Also keep in mind that the contributions of the folks who haven’t had the formal maths education are likely to be extremely helpful, so don’t give up."
MachineLearning,"&gt; they aren't differentiable so you can't stack them up so NN's have big wins in those areas.

Can you expand on this?"
MachineLearning,"does it improve the quality of your work? I get that it gets you through whatever the next hurdle is... but has anyone noticed trends about long-term memory retention, or quality of output?"
MachineLearning,"You have to use your brain, not drugs.   After all, we can't give drugs to our machines at inference time to boost performance."
MachineLearning,"Wow, that's pretty impressive. Can I import any datasets from TF or PyTorch?"
MachineLearning,You should try it. It’s been life changing for me in my 30s.
MachineLearning,"This is probably very simple, but I just can't make sense of it.

I want to calculate the expected prediction error for a zero-one loss function. I am given the real and predicted outcomes for an imaginary dataset, but no information about the X values:

    y_pred=[a,a,a,b,b] y_read=[a,b,a,a,a] 

I am also given the conditional probability of P(a|X)=P(b|X)=0.5 .

Looking at the formula for this calculation in the book Elements of Statistical Learning,  I have the zero one loss for X, and the conditional probability , but I am lacking the expected value for X.

Question 1: How can I get the EPE value without knowing Ex?

Question 2: If I vary the conditional probability for each class given X, the EPE value does not change. Is this normal? Does that mean EPE cannot be minimized in this case?

Thank you for your time"
MachineLearning,Not in this day and age
MachineLearning,"Notion.

Except the automatically finding citable information, it is Mendeley on steroids"
MachineLearning,We will - it is going to be so exciting! \*barely holding myself not to spill the beans earlier\*
MachineLearning,Please also share your update with us reddit hermits :)
MachineLearning,I love you too
MachineLearning,"Hey u/bpooqd!   


Fair point! Great to know you find our project cool! Thank you also for hte feedback - we are actually working right now to make it clear in [app.activeloop.ai](https://app.activeloop.ai) to make the difference between Hub (package) and Activeloop (our company).   


Hub, i.e. the entire Python library, the core of what we are focused on and the centre of our know-how is absolutely and completely open source. Having said that, Activeloop holds a few of its projects, like the dataset browser and visualization application ([app.activeloop.ai](https://app.activeloop.ai)) backend proprietary. As for the data, it is hosted where you want it to be 📷 You can use your own Google Cloud Storage/AWS/Azure buckets. You can also use Hub locally.   


Finally, if you want to you may store it with at our servers for free. Re: trust - you are completely on point there and that is why we are proud to say you'd have to stay tuned for a couple of weeks for a MAJOR announcement there (feel free to join the community slack or just stay tuned on twitter (activeloopai) or elsewhere)."
MachineLearning,I love you both
MachineLearning,Love your focus on deep work. Here are some additional [notes](https://trywinston.com/deep-work) for anyone looking for more info about it.
MachineLearning,"hmm they make a 70mg now, let's just solve quantum computing tonight, add it to my jira."
MachineLearning,"theoretically data can be hosted on any backend -- internally we use everything on s3. gcp is not yet supported to my knowledge, but will be soon."
MachineLearning,we use interpolation in such cases
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'll watch the video, but my statement is correct. Your view of AGI is controversial. If it isn't, then everyone would agree with you. Do you know what the word ""controversial"" means?"
MachineLearning,TF serve is magnitudes faster and it's actually built for production.
MachineLearning,"Papers are not focusing on ""production"" or software development. I think it's ok for academics to not use them."
MachineLearning,"In both cases, the name refers to standard [""lazy"" evalutation](https://en.wikipedia.org/wiki/Lazy_evaluation) in computer science: we only perform computations ""at the last minute"", which unlocks the use of efficient low-level schemes and re-factorings of the computations.

Given that, the focus of both libraries are indeed fairly different. KeOps keeps a narrow focus on ""attention-like"" interactions and triggers executions whenever a ""reduction"" operation is called on a ""KeOps LazyTensor"". On the other hand, the PyTorch JIT engine covers a much wider range of computations - but cannot provide top performance in all cases.

In the broader context of scientific computing, we thus describe PyTorch as a ""generalist tensor computing framework"" and KeOps as a ""domain-specific compiler"". Fortunately, the Python ecosystem is now mature and modular enough to let us package KeOps-like engines as **extensions** to PyTorch-like frameworks. Everything now works seamlessly for both users and developers, which is fantastic :-)"
MachineLearning,"I am so happy that this course is emphasizing Unit tests! So many DL papers don't have Unit Tests. 

I feel that anyone creating an ""Approximated Function"" should design unit tests around those functions!. Really great stuff OP!."
MachineLearning,[removed]
MachineLearning,"You have some good articels here, but I just want to point out that the used approach for model deployment is not scaling very well. I mean in the end you can scale everything with hardware, but it's not a smart way to scale.

* Optimize your model before you deploy it   ([Tensorflow](https://www.tensorflow.org/model_optimization/guide))
* Use model servers, use GRPC if possible (e.g. [TFX](https://www.tensorflow.org/tfx/guide/serving))
* Don't send raw numpy images from your client to the server (send jpegs and decode on the server)
* Use GPUs or better TPUs for inference

[Here](https://towardsdatascience.com/how-to-not-deploy-keras-tensorflow-models-4fa60b487682) is why"
MachineLearning,thanks.. fixed it
MachineLearning,"Nice, I've been so lackadaisical with data versions in the past, this is awesome."
MachineLearning,The AMA is over on /r/datasets not /r/machinelearning. If you have questions about making big museum sized datasets please go over there for a chat with the team.
MachineLearning,"Well, the final 6 months to a year of a PhD can be pretty stressful, no doubt about that. I was basically already working a fulltime job 2 months before the final defense and wrapping up the final work at nights ...fueled on a constant diet of candy bars, energy drinks and sleep deprivation (doesn't really count as 'drugs', but enough to have some pretty noticeable health effects)...Luckily my employer allowed me to take my entire year's worth of vacation in one go right after the defense even though I had only been there for such a short while - so everything turned out fine in the end.

Most other PhD students around me (this is wrt germany) fared similarly, but I wasn't aware of anyone doing any illegal drugs - or even copious amounts of alcohol (coffe machine was running 24/7, tho...yes...saturdays and sunday nights, too).

With regard to productivity: It may help with the busy work - but drugs certainly kill any kind of creative output. For that you need time to let your brain ramble on."
MachineLearning,"These opinions are not well supported by evidence. For example you can find quite a few Kaggle competitions where NNs won by a large margin. And for those competitions where NNs (alone) did not win, they were usually an important part of the ensemble.

NNs are best when you have homogeneous data, like pixel color channels, sound amplitudes, word vectors and ideally there is a notion of closeness between features (e.g. one pixel is close to another). Then NNs can find complex hierarchical relations among the features.

So, if your table is 100 columns of some kind of chemical activation, the NNs are the favorite. If your table is a mix of largely unrelated columns of very different data types, and you do not have so many rows, then you usually do not expect complex hierarchies or great benefit from putting a linear neuron on top of that. In that case a GBDT is a favorite.

...but you never know. You have to try, contrary to some opinions who claim to know beforehand."
MachineLearning,"Watch the video and learn what it means to have a theory of AGI vs what it means to be ""Not even wrong!""

That is if you aren't simply a troll."
MachineLearning,Check your model pruning parameters
MachineLearning,I’m only a quarter of the way through but this is actually incredible and hits on a few areas that are rarely hit on. If you ever turned this into a video you’d certainly get some easy views + subscribers.
MachineLearning,"&gt; I usually wake up at 5:00-5:30 and have a vegetable juice and whey protein for breakfast. 

okay so you're a little different than me."
MachineLearning,"Do most people really structure your day? I personally never structure my day, I just do what I feel like/need to do. Sometimes it’s coding or paper writing from 9am to 5am and sometimes it’s watching football all day long."
MachineLearning,"And this is why people don't provide code. Because as soon as code is provided, people start feeling entitled to it working the way they want to, support on issues, etc. so why bother releasing your code and opening yourself up to being publically shamed for its quality?

People should be encouraged to release their code regardless of the quality. Refactoring is orders of magnitude simpler than replicating from a paper. All those hidden little details that don't make it into the paper are going to be *somewhere* in the codebase.

Once releasing code is the norm, then we can focus on the quality. Good Code &gt; Bad Code &gt; No Code"
MachineLearning,Does anyone knows AD framework for irregular time series?
MachineLearning,I see. This explanation makes it clearer whats going on . Thanks for explaining the issue clearer
MachineLearning,You can try [fraidycat](https://fraidyc.at/).
MachineLearning,I am not here to wage battles. I wasn't doing it for you. I tried to tell this user to go easy. I am doing my bit.
MachineLearning,"You just didn't see it. It was happening.

&amp;#x200B;

And they were doing lines of adderall not non-prescription drugs. They had a prescription."
MachineLearning,I mean one of those companies was microsoft so..
MachineLearning,It is totally worth it.
MachineLearning,What fairy land version of academia do you work in?
MachineLearning,"But he still required caffeine beforehand, which is also a stimulant that has withdrawal effects, but for some reason we overlook that.

Coffee was also once illegal too."
MachineLearning,not gonna lie  I'm using modafinil sometimes when I really tried but it usually one or two times per month. 50 mg before breakfast other times just a coffee will do the trick
MachineLearning,I share your feelings and form of expression regarding machine learning research many times. 😁
MachineLearning,Why?
MachineLearning,Yes and also there explainability is more than that of deep learning which can sometimes be a black box as its gets more deeper.
MachineLearning,[https://academictorrents.com/details/535113b8395832f09121bc53ac85d7bc8ef6fa5b](https://academictorrents.com/details/535113b8395832f09121bc53ac85d7bc8ef6fa5b)
MachineLearning,This is a great way to explain it
MachineLearning,"Well grad students are devices for turning coffee into papers. But beyond caffeine and the occasional day drinking, I haven't seen anything."
MachineLearning,When I was in undergrad I used to take a lot of caffeine pills to stay up late and work. I don't do it anymore.
MachineLearning,theres literally psychedelics meetups at NeurIPS lol
MachineLearning,"That guy is completely delusional and is literally, explicitly calling for all of the cancelling and censorship you claim the SJW's want to inflict on you.  If you're agreeing him that should trigger some serious self reflection."
MachineLearning,"Something similar is also observed in RL. In particular, wherever we use value functions, estimating quantiles or distributions (e.g. C51, IQR-dqn) is much better than estimating raw values which can be in the order of tens and hundreds. We also found that bounding the reward significantly improves performance (iirc the paper was ""Implementation Matters in DRL"")."
MachineLearning,"Yes, you are misunderstanding, it's not about using dropout during inference. The issue is that you are maximizing performance on the test set with respect to a hyperparameter (dropout). Because of this, you are overfitting on the test set. I don't think literally checking two dropout values is anything to worry over, but over larger search spaces it is absolutely akin to p-hacking. You should use a test and a val set if you are doing any type of hyperparameter search. Trouble is, reviewers are not calling out this dubious practice."
MachineLearning,"How about you take the average of the difference between any move and the best move?  Meaning you look at each move and determine the difference and then sum them up amd divide by number of moves.

And you could get the mean squared difference  - square the difference for each move , sum then take square root."
MachineLearning,"I'm not a lawyer and this is not legal advice. You should covering with a lawyer.

1.a. There is a Supreme Court precedent where using copyright data for training models is not illegal: https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf

1.b. Not all images have the non-commercial CC license. In the worst case, you could select only the ones that allow commercial use.

2. The only thing being passed from training to deployment are the weights, right? It seems to me that weights would be a derivative work, therefore only allowing non-commercial use. However, if you implement your own code to train a very similar model based on the same paper, none of these should apply. The license is on the code and not on the idea.

3. If you write your own code in item 2, this item is also resolved."
MachineLearning,"Thats actually how many inventions arrived at, Watson and Crick where microdosing LSD if not actually dropping when they arrived at DNA helix structures"
MachineLearning,Agree 100 percent with this. Running early morning with the dog before the world wakes up is fantastic.
MachineLearning,"&gt; It is supposed to be

It isn't ""supposed"" to be taken as a performance-enhancing substance at all (OP's question)."
MachineLearning,Using drugs for enhanced productivity????? I would rather buy better GPUs in my deep learning machine.
MachineLearning,Also everyone drinks coffee. Caffeine has big effects on the brain too!!
MachineLearning,"I know several people in the Bay Area, USA, who take Aderol."
MachineLearning,"It is supposed to be taken right after waking up, so that you have the focus to work all day. I'm seeing people taking it at night to stay awake, and that's the stupid thing ever. The brain has overworked whole day and let's abuse it for another 8 hours."
MachineLearning,"T.g. for TF, I still don't know where I will find myself in this mess, but I appreciate the ppl who help with open source."
MachineLearning,I see you have spare cash.
MachineLearning,"Hah, this reminds me of that kaggle competition where IDs were embedded as an attempt to leak information into the model, thankfully the team was DQ'd. I will try to find the relevant thread and link."
MachineLearning,"At my company, you'll find the occasional person with ADHD and they're medicating for that reason. They're definitely the exception in the data science department. However, all of my college friends who work in the finance side of my company were super into adderall and ritalin as performance drugs during our undergrad years. I really don't think that would've stopped just because they're in the real world for 10 months so far, although I think most recreational / party drug use has ceased due to the pandemic. Who wants to do coke in a crowded club right now?"
MachineLearning,"I have worked with a couple ""late stage"" fellow code monkey's that did Adderall.   Their social skills were alright but focus on personal grooming and well being could have been better.    In addition to my computer programming background I've now been retraining to become a clinical social worker and in hindsight the people who had been using Adderall for an extended period of time were expressing psychotic symptoms."
MachineLearning,I think it is balance-balance
MachineLearning,"If you build up to that dosage and it’s the right one for you, that’s ok too! 30mg is very small dose, but it works for me; 40mg gave me jitters. You’ll know when you find the right one. Good luck! :)"
MachineLearning,"Thank you. I've asked quite a few people, I'm just trying to gather information for my doctor. I've met people who take everything from 60mg vyvanse to 30mg IR and I hope to never be either"
MachineLearning,"I take Vyvanse; started at 20mg, with the goal to build up to 40-50, but I found that 30mg was enough for me and has basically no side effects, so that’s what I’m on now."
MachineLearning,Yaaas! Ty for this share. I'm so excited to work on projects when I have free time. I'm going to start with creating a cloud for practice and take mah steps.
MachineLearning,"Somewhat related question, may I ask what dosage you're taking? I think we probably have a lot in common, because I graduated from college a year early and was successful in academics, but I procrastinated literally every assignment from 4th grade onward.

I was diagnosed two years ago but only recently started taking medication, starting at 5mg IR Adderall, and it's seemingly ineffective."
MachineLearning,You need to look up papers in the venue you’re sending stuff to. Nowadays there are tons of textbooks so you might be able to just point to them to those if it’s any basic theory that one could find on Wikipedia
MachineLearning,"Mmmmm no! in the US, a therapist/psychiatrist has to perform an ADHD assessment, diagnose you with ADHD, give your doctor a referral with the results and your test, before your doctor can prescribe any ADHD treatments."
MachineLearning,"Nobody I know in the ML/AI industry--either in the US or in Europe--uses drugs regularly. They're actually the types *not* to use drugs in general. 

The only people I know regularly using are doing so recreationally, and that's mostly Dutch people (of other different engineer types) and people in NYC/Miami areas."
MachineLearning,The best performance enhancing trick I've discovered is shifting my cardio workouts to the morning. It always sucks to do it but it's crazy how much better my brain works every day that I do.
MachineLearning,"Off subject what do you think about this 

https://www.natureofhealing.org/the-mrna-operating-system/"
MachineLearning,"I've heard it's finally starting to change, now that the opioid epidemic got so bad and everyone has realized how addictive these pills are. Not an expert myself (and I no longer live in the US) but I'm hoping the reports are true! 

In Europe, it's almost the opposite from my experience... You really need to work (and suffer a bit first) to get antibiotics or pain killers out of most doctors."
MachineLearning,"&gt;Erdös said, ""You've showed me I'm not an addict. But I didn't get any work done. I'd get up in the morning and stare at a blank piece of paper. I'd have no ideas, just like an ordinary person. You've set mathematics back a month."" He promptly resumed taking pills, and mathematics was the better for it. 

People always being Erdős up as a positive example but 

1) He started taking stimulants at 1971, at 58 years old, and was very prolific before them.

2) Notoriously he could almost not survive on his own, feed himself, some of the most basic things.

Just a weird example of a person to extrapolate anything from."
MachineLearning,This is so cool. Is it possible to turn off the captions? (I don't know much about coding :/
MachineLearning,"Yes. Generally positions can vary from very specific to not specific at all. And, generally speaking, the more prestigious the education, the more general jobs you can find, and sometimes more prestigious. However, machine learning is a very hot field as you know, and saying AI makes people throw money at you, and so finding a job for a specific position in ML isn’t difficult, especially if you have projects and experience (I learned the importance of independent projects the hard way, all my friends had a cool portfolio, and I just had a decent gpa). Talk to your coworkers at your job, but often times a company might pay for your grad school if you mention that in an interview, and it brings up a good conversation if they ask where you see yourself in the future"
MachineLearning,"Not the person you asked, but high grades don’t equate high functioning. I’m what’s called twice exceptional (ADHD + Gifted), and much like that person, I had amazing grades, won prizes, skipped a grades, etc. But I cannot function in daily life unless I make myself VERY stressed, which allows me to focus for a few hours. 

Since I got my diagnosis and medication, my life has improved tremendously: I can keep my house clean, I don’t lose everything, I’m not late everywhere, I don’t procrastinate everything at work, I’m present and able to listen when people talk to me, I’ve gotten my finances under control, my anxiety is nonexistent, I can brush my teeth regularly and eat well, etc. It’s been nothing short of life changing."
MachineLearning,Thank you for sharing this video!
MachineLearning,Yes that's true. CS 229 by Andrew Ng is a good start for beginners.
MachineLearning,lmao
MachineLearning,"It’s common in the US in all highly competitive fields, and academia in general."
MachineLearning,I understand your pain.
MachineLearning,Very common in all STEM fields
MachineLearning,yeah i believe the primary reason they open source noon commercial licence is to help academia grow
MachineLearning,[TODS](https://github.com/datamllab/tods)
MachineLearning,"It's controversial in the sense of, ""[That's not even wrong!](https://youtu.be/0ghzG14dT-w?t=625)""."
MachineLearning,"oh yeah if companies at that scale are just taking advantage of this situation, that's rough. while they haven't shared any of their dataset but pretty sure their datasets contain such content. GPT3, DallE CLIP all trained on similar datasets and they planning to or already selling that."
MachineLearning," these questions may be beyond the scope of original question but definitely are very important or at least will become more and more important.

music is produced for humans to listen and if they use what they listened to create something new/similar, how transformative is that?

how much is that different from a gan listening to your music and producing it's own etc."
MachineLearning,Not so different on some levels
MachineLearning,"It is foolish to trust your instinct on these things, better to do a grid search across known drugs and doses."
MachineLearning,Visualizing CNNs are better than an acid trip
MachineLearning,healthcare is on a while different level 🙈
MachineLearning,"this is definitely very good analysis, i have come across similar articles a few times. selling of gan generated content is one of the key area which is most confusing."
MachineLearning,"No, they need to be purged. They not only want to discriminate against me, bully me, harass me, deplatform me, they want the AI systems to be designed in such a way as to consistently discriminate against me and people like me. They deserve zero kindness.  
  
Why even entertain their toxic thoughts? Everyone hates them. They are not nice people. They are not kind. They are obnoxious and full of drama. They will backstab you the moment they get the chance.   
  
You think it is acceptable that people are bullied? Changing jobs because of fear? Because of depression? Because of discrimination? Because of their gender/sex/race/religion? This is acceptable to you?  
We need to show kindness to these kinds of people? Really? How? Why?  
  
This community does not need drama and does not need to tolerate these toxic people. Look at other fields, it's not just ML. Its 5% terrorizing the 95%. Normal people just need to speak the fuck up because we are tired of them already. No tolerance for them in any of our spaces.  
If you tolerate them, they will start drama, they will backstab you, they will steal your work, they will bully, they will tear your workplace apart.  
Normal people should not tolerate them at all."
MachineLearning,in many cases it may not be possible to detect but for example stylegan2 pretrained on FFHQ can easily be detected by generating a few images and then calculating metrics like FID etc.
MachineLearning,"My understanding is it’s quite prevalent in colleges in the USA, as well as amongst finance and banking types.

First I’ve heard of it amongst coders and data analysts but I guess it makes sense."
MachineLearning,"Ugh reading this as a 34 year old male that got diagnosed last year speaks volumes to me. 

ADHD is so over diagnosed in america that no one takes it seriously. I had such a hard time finding a practice that is geared towards adults with ADHD, sitting in tiny plastic chairs when your 5’11 in the waiting room was a funny experience. 

And no one takes it seriously in the sense that when you try to explain the things you are going thru, literally every single problem you mentioned, ADhD is a joke. « Oh everyone has it, it’s a disease for kids! » or « what do you mean, you made it thru university! »

Debating wether to starts meds or not. Have been struggling for 4 years with this decision."
MachineLearning,"Yes, clearly. The village idiot. Who has a graduate degree in math.

This is the kind of bullshit I'm talking about. ADHD is a real problem, asshole."
MachineLearning,"Easy easy. Kindness &amp; Honesty in small servings, from every one of us will go a long way :). Let it go man"
MachineLearning,"Yeah for sure, we've been working mostly to target deployment so some of the training needs fleshing out still.

Won't be too long before that's all sorted!"
MachineLearning,"Sorry, but that's a *stupid* way to handle a simple problem.

Ideally, you would be able to submit an entire training function, and I am glad to see you will be providing that soon. I welcome the simpler API for people who don't need any fancy features, however you should always provide the more powerful API as well. Serverless GPUs implies that you actually have access to the GPU. Serverless simple model training sounds more akin to what you're providing right now."
MachineLearning,"You're a joke and deserve permanent unemployment.  
""people are hurt by discriminatory or abusive workplaces"" LOL  
Yet you are creaing abusive workplaces and advocate discrimination:  
""It's not ""political"" to want equity in tech workplaces""

Oh? I thought you said you cared about people being hurt by discriminatory workplaces? Now you want equity? Huh, I guess you can lick my boots.  
&gt; nor is it ""political"" to demand attention on issues that matter to POC and women.

Yes it is political. You're a sexist and a racist, and we already defeated your type once in WW2 and we will happily do it again."
MachineLearning,"Why are the moderators of this subreddit allowing you to have any voice in this subreddit? Why are they not deplatforming you? If you were in my company, you would be cancled. I have zero tolerance for SJWs, which you clearly are. You are a menace to this world and normal people need to stand up to bullies like yourself.  
  
Your type is not hard to detect at all and the rest of us need to start adapting a 100% zero tolerance attitude to your kind. You destroy companies, you destroy peoples lives, you destroy friend groups etc.  
  
/u/backpropped is 100% right in his assessment and you are 100% wrong. He is not sounding like a child at all and you are just mad that he is speaking up and calling you out.  
  
You want drama? Prepare to hide your opinions, because speaking up will get you fired and blacklisted from many places in the future. We wont tolerate you."
MachineLearning,Given the dipshit levels of ML publications in general I'd say why do they even bother
MachineLearning,"We're working on adding in full custom training pipelines at the moment, but as a temporary work around you can implement custom pipelines in the loss function: [https://docs.neuro-ai.co.uk/custom\_loss.html](https://docs.neuro-ai.co.uk/custom_loss.html).

When I have to implement more complex setups like in GANs I wrap all models inside of one larger model and then output everything to the loss function to do back prop.

 We're going to be adding in similar support to Pytorch lightning, and also the option to pass in a fully separate function that is then just run on the GPUs. Let me know if you have any other ideas as well we're still trying to work it out"
MachineLearning,Is it really an issue in research? In undergrad Adderall was so common but I grad school I don't recall seeing it too often and even then just for quals. I don't think Adderall correlated at all with NSF grants or landing tenure track jobs. It's good for grinding leetcode though.
MachineLearning,How would you use a custom training loop with Neuro? It appears you can only pass model objects to the train function.
MachineLearning,"Definitely haven't heard of any academic research, but I'm increasingly hearing 'personal research' rumblings around THCV. I've dabbled a bit with it trying to find an alternative to the stimulants I'm prescribed - and while it's not as potent as the amphetamines, there's way less of the usual stimulant side-effects. I really appreciated how much less it screwed with my sleep... but it's pretty hard to find and expensive for now and definitely not covered by insurance :(

That said I'm fully expecting it to blow up pretty soon - given how prevalent weed is in all of computers, I'm sure ""weed based Adderall"" will be a hit"
MachineLearning,"Thanks for providing a source on a topic that's usually just a lot of opinions!

Do you know if there's any research on the prevalence of THCV?"
MachineLearning,"Another way of thinking about this is that neural networks allow you to transform unstructured data into a space where it is somewhat structured. E.g. going from image space with infinite possible information to a subset of [cat,dog,taco] labels. ""Tabular data"" is typically already highly structured and, perhaps more importantly, the data can be easily separated into independent variables.  Therefore there are much more efficient ways of building classifiers on it."
MachineLearning,What?! I have never once seen this. You’re working at some weird companies man.
MachineLearning,Out of curiosity - how has your life changed since the diagnosis and medication? Obviously you were very high functioning already!
MachineLearning,"Hi, I am creating a **Product Clustering Recommendation System for supermarkets**. 

The flow is like this. I get the sales records as the input. Extract the bought items list from each sale(shopping bill). Analyze **which items are bought together** most, and **cluster them**. 

Then make suggestions to the supermarket management to place the clustered items next to each other (modify the shelf arrangement) so that it would be easier for the customers to buy the items without having to move a lot among the shelves. 

My question is, what is the **most suitable Clustering method/algorithm** for this?"
MachineLearning,"&gt;However no one gives a shit in ML research and most research is non-reproducible garbage so you will be fine

Savage but so true"
MachineLearning,Don't think this is actually true.
MachineLearning,this is every field in the US.
MachineLearning,"I used to think it was that way until I was having a session with a new psychiatrist who my parents had taken me to for unrelated stuff, and I happened to mention issues studying and focusing and he was basically like “well if you think it’ll help we can get you a Ritalin prescription”. 

He was an older dude, private practice, etc. so maybe he was the outlier but I did not expect it to be that easy lol."
MachineLearning,Thanks for the feedback. Mind if i PM you with a few questions?
MachineLearning,Yes as you said it's mostly from the interface. Neuro is presenting what serverless should look like in the ML specific domain instead of more generic serverless on-top of GPUs.
MachineLearning,"&gt; A significant portion of adults (at least in the tech industry) who are diagnosed with ADHD don't actually have a medical condition and instead manipulated doctors to get a prescription.

Being the village idiot isn't a medical condition."
MachineLearning,"Social media feels similar to drugs? I feel a mild ""meh"" when I browse social media."
MachineLearning,Most people aren’t doing lines in the bathroom. They are taking a pill and not mentioning it to anyone.
MachineLearning,"Ml is kinda new in chemistry, and not well known, so I would put in a short summary of what the technique does with references. I would avoid putting in much (maybe any) maths. Does depend on the journal, of course. If it's a more compuchem type journal, they'd be fine with the maths, if more synthetic then a picture explaining the technique might be a better choice. I've worked in both chemistry and AI."
MachineLearning,"Hmm.... *[Google claims to engage in such agreements with companies of all sizes - but sure seems to make it difficult:](https://www.google.com/patents/licensing/#tab=cl)*:

&gt;&gt;  Google **engages with operating companies of all sizes** from similar industries in which we operate. If you have an interest in discussing a cross license with Google: 
&gt;&gt;
&gt;&gt; ...
&gt;&gt;
&gt;&gt; Google Cross License Opportunity Contact Form      
&gt;&gt; **This form is no longer accepting responses.**"
MachineLearning,"I have ADHD and require these drugs simply to function normally. Seeing them abused like that is really... annoying, for obvious reasons."
MachineLearning,"i received a phd in ML from a top-ranked USA school, and i only know of 2 people who took any pills, both of which were for legit ADHD concerns. weed was more common, as a means of putting on the brakes and relaxing, as opposed to needing more gas in the tank."
MachineLearning,"Funny response and that op got downvoted here. The fact is that you don't have to, but you SHOULD.  In many real world cases, unless you're working in computer vision, deep learning is not necessary. Traditional ML techniques are often both less expensive and more appropriate for many cases"
MachineLearning,This is with the random seed set to be the same?
MachineLearning,[https://arxiv.org/abs/2012.06678](https://arxiv.org/abs/2012.06678)
MachineLearning,[deleted]
MachineLearning,[Github link](https://github.com/Shreyz-max/Video-Captioning)
MachineLearning,"The law lags way, way, way behind on tech. I understand that you want to do the right thing, but keep in mind that if you start asking lawyers what is and isn't permissible, they're going to tell you stuff that's totally stupid because their job is to mitigate risk and they have no idea how engineering works. For example, in my experience, lawyers tend to freak out if people's faces are involved (even if those people voluntarily upload their shit to the internet) which would make working with styleGAN a major pain in the ass. 

When it doubt, just do the best you can and feign/plead ignorance. I'd say 1. it's fine 2. it's fine 3. it's probably not fine. Stay away from OpenAI, their shit is not good enough to make up for how hostile they are to open source."
MachineLearning,"I think our legal system is totally outdated and largely rent-seeking/irrelevant, but this is terrible advice. If your startup is to sell investors on technology pending litigation, maybe you should find something else to work on or do it right from the start unless you want a scarlet letter following you around forever."
MachineLearning,Why would one ever use user id or other ids in ML though? Unless you are like following the same individuals (repeated measures) it seems like this would result in overfitting or being outside the X feature space in testing.
MachineLearning,The self-normalizing networks have also closed the gap between NNs and other methods for tabular data
MachineLearning,"One possible case where nn may beat tree-based models is when your goal is extrapolation. Tree based methods are awful, awful, awful, at extrapolation."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"In your post, you say that pre-trained OpenAI CLIP is not ""very open source"", but I double checked and the git repo has an MIT license on it, which is pretty permissive. Did you mean the training data?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"IANAL, but a lot of companies have an open patent pledge(so does Google [1]). The way I understand that pledge is it's a gentleman's agreement that they won't sue you for using their patent(hence the ""open"") but were you to sue them they'll unleash the full force of their portfolio in counter-suing you.

[1] https://www.google.com/patents/opnpledge/pledge/"
MachineLearning,"Thanks, I mixed up the terminology, for some reason my company flips these."
MachineLearning,"&gt;I would synthesize a few samples like the ones you showed in your post  and feed it to the discriminator and see how good that discriminator is.

I've done that earlier, discriminator is almost always sure that generated data is real.

&amp;#x200B;

&gt;If you share your code I can take a look.

Sure, thanks.

[https://pastebin.com/hhgGPXHt](https://pastebin.com/hhgGPXHt)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks for the suggestion. Will go through Siamese first and see how it goes. FAISS looks pretty good resource to look into.
MachineLearning,"I do ML and take a bunch of stimulants

and then browse memes at 500 apm"
MachineLearning,"Unfortunately for the software community, it’s extremely rare for courts to hold up anything claims involving open source."
MachineLearning,"&gt; Like all of Erdös's friends, [fellow mathematician Ronald Graham] was concerned about his drug-taking. In 1979, Graham bet Erdös $500 that he couldn't stop taking amphetamines for a month. Erdös accepted the challenge, and went cold turkey for thirty days. After Graham paid up — and wrote the $500 off as a business expense — Erdös said, ""You've showed me I'm not an addict. But I didn't get any work done. I'd get up in the morning and stare at a blank piece of paper. I'd have no ideas, just like an ordinary person. You've set mathematics back a month."" He promptly resumed taking pills, and mathematics was the better for it."
MachineLearning,"For regular KNN, yes. If this isn't the behavior you want, try weighted KNN by distance. In the sklearn implementation, for example, you can just set weights='distance' as a parameter to do this"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"For 1 and 2, I think this mirrors the situation with GPL in that it's the original work that is licensed - which you can apply but not distribute - while derivative work might not be.

However, the problem with the dataset is that the original data may in part of fully be reproduced or extracted from the trained models. I think this is a genuine legal concern and not just a matter of navigating licenses. What if a close semblance of one of the training images - a photo of a real person - was produced by your software and then used for marketing purposes? This probably would not be nor should be an authorized use of the training data.

So I think the sensible thing here is to talk to lawyers and implement best efforts to prevent those possibilities; or prove that something close to the original dataset is not extractable."
MachineLearning,"Yeah, I think this is all yet to be hashed out by courts.  You just seemed to be very confident that models trained on NC datasets are protected under fair use.  I am under the impression that they _could_ be protected under fair use, but I wouldn’t risk it if the stakes are high."
MachineLearning,"No. On the other hand, last I checked Microsoft was going to make a commercial product out of GPT-3, which, if I understand correctly, is trained on basically the whole public internet, which sure as hell isn't licensed for anything whatsoever."
MachineLearning,Almost surely not -- especially because decision trees are more of an approximate greedy heuristic. But also because decision trees are very well established at this point.  A pointer to the implementation is all that I'd want as a reader.
MachineLearning,i guess general focus of academia is mostly focused on subfield of ethics which focuses on biases but nobody cares about other ethics in general
MachineLearning,Do you know what is the difference between reinforcement learning mario and a classical car in a track example  that learns completing the track without crashing sides?
MachineLearning,"Tbh if all you want is to deploy an API with your model to the web you don't have to code an entire Flask app from scratch.

I would check out these places (I'm not affiliated with either): [https://valohai.com/](https://valohai.com/) [https://gradient.paperspace.com/](https://gradient.paperspace.com/) [http://cnvrg.io/](http://cnvrg.io/) [floydhub.com](https://floydhub.com)

Otherwise, if you want more than just an API you will have to build a full app. And make no mistake, full-stack dev is hard! There's a reason these guys get paid so much:)"
MachineLearning,"I think it also depends on the focus of the article as a whole. If it's a new technique for the field you need more detail. If the focus of the paper is the results rather than the method, then less might be ok. The minimum is probably a paragraph. 

Unless the technique is the focus of the paper, I would leave out equations that can be found in the references, or put them in a supplementary."
MachineLearning,"As someone else said it's not ethics, it's lawsuits. Specifically a small startup is a worthless target for lawsuits because it has no money. A near-IPO company or massive acquirer is a much juicier target for lawsuits. As such an acquiring company cares a lot about the legal liability it is putting itself under by buying another company. Suing a company right before IPO is also a time honored tradition as you have a lot of leverage since they want the lawsuit to go away ASAP so it doesn't tank their IPO. An investor wants either an IPO or an acquisition so they care very much about ensuring that legal liability doesn't prevent those from happening."
MachineLearning,"Be transparent about your methodology without self-flagellating, you'll be fine. It's not a big deal"
MachineLearning,I bet a good NN practitioner who's familiar with advanced NN tools such as dropout and data augmentation will do extremely well with tabular data.
MachineLearning,"I can offer some simple advice here. If you are trying to visualise this then extract the vectors and compare the cosine similarities. If you have good cosine similarities between your vectors and metadata then your representation is good enough otherwise try again and refine. This is intrinsic evaluation. Extrinsiacally, Validation scoring is okay too."
MachineLearning,I'm not sure if this is exactly legal or not but I've run into licensing issues and have generally opted to create my own model in those instances. I've used several 3rd party models to train my own model but I'm not sure what legal implications doing that may have. I've actually been able to use this method to reduce model sizes in a few cases while have the same/similar results as the third party model.
MachineLearning,[removed]
MachineLearning,I've worked in a few labs in Europe and Asia and never heard about it.
MachineLearning,"I think that academic AI ethics push will head more towards ""is this model racist/biased"" and ""is training a model on Bobs location data a violation of bobs privacy"".

I don't think it will much impact the use of GAN's..."
MachineLearning,"No that's not true, dall-e itself already generates images matching the input text.

Openai draws 512 samples from Dall-E in this way and then reranks them based on the clip prediction."
MachineLearning,I think this is actually dependent on how good your lawyer is. Having a guideline for this is flawed as this must be handled on a license by license scenario.
MachineLearning,Just do hackerrank cracking the coding interview and you're set
MachineLearning,I am an ML researcher / have made publications and I do take sporadic a dose of vyvanse to assist with concentration and retention when reading scientific literature. Although I was diagnosed with ADD around 12-13.
MachineLearning,"Thanks! I checked out a few different journals that generally accept “chemistry ML” papers and they seem so inconsistent.

Do you think drafting this with some background is the best way to go?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hey - if you're using GCP, considering giving [https://gpu.land/](https://gpu.land/) a try. Our Tesla V100 instances are dirt-cheap at $0.99/hr. That's 1/3 of what you'd pay at GCP!

Bonus: instances boot in 2 mins and can be pre-configured for Deep Learning, including a 1-click Jupyter server. Basically designed to make your life as easy as possible:)

Full disclosure: I built [gpu.land](https://gpu.land). If you get any questions, just let me know!"
MachineLearning,"&gt;	The use of the training dataset is clearly transformative and protected under fair use.

Do you know of any precedent where a non-commercially (NC) licensed dataset was used commercially and it was protected under fair-use?

I can’t reconcile this notion with the fact that many published datasets have NC licenses."
MachineLearning,I would add that even using LSTMs might be too complex if you don’t have that much data to train on. If you don’t have long dependencies and a lot of data simple RNNs can serve you too.   They have waaay less number of trainable parameters.
MachineLearning,"I think it will depend on the journal and reviewers unfortunately. Really they should be content with a short explanation and references. However, if it is a technique not commonly used in the immediate field, the reviewers might ask for more detail. 
If you're following a procedure that is exactly described somewhere else, you can refer to that.

I would want to include code as supplementary, but reviewers sometimes don't look at it."
MachineLearning,"I would synthesize a few samples like the ones you showed in your post and feed it to the discriminator and see how good that discriminator is. 

It’s hard to tell what’s wrong without looking at the code implementation but I would say there is something wrong with you loss function implementation."
MachineLearning,Yea drugs are everywhere. We’re the number 1 drug consuming nation. Mommy’s little helper
MachineLearning,I don’t know you but great job 👏🏽
MachineLearning,what interests me is the push by AI ethics people in academia but i think industry is concerned about these issues in general.
MachineLearning,"I think this basically is a way to take a bunch of images in your scene that are automatically annotated with the items in the scene based on what you call them in blender. 

i.e. you bring the labeled 3D scene and this will create labeled images using the photorealism of blender."
MachineLearning,"I'm guessing they never use flask with debug mode off and confused ""don't leave this feature on in prod"" with ""this tool isn't appropriate for prod."""
MachineLearning,"Hi, I am the guy that wrote the ""orabona's notes"" ;)

My first suggestion would be to find a supervisor working in this field: online learning is a theory topic and you need to acquire the proper ""taste"" for theory problems. It is difficult (but not impossible) without a supervisor.  


Regarding research problems: online learning is a relatively small field and most of the online learning people are actually moving towards stochastic optimization, because it is very close to online learning and it has a wider visibility. Bandit problems are still hot, especially the contextual ones, as well as the problem of designing algorithms adaptive to unknown characteristic of the problem.  
I would take a look at COLT papers: find the recent ones and see which open problems they list.  


If you are really ambitious, take a look at COLT ""open problems"": these are short papers published in the proceedings and only explaining an open problem. You can search on google scholar to see from the citations which ones are still open. For example, a very difficult one (and still open) is the one by Yoav Freund: [https://cseweb.ucsd.edu/\~yfreund/Colt2016\_open\_problem.pdf](https://cseweb.ucsd.edu/~yfreund/Colt2016_open_problem.pdf)  
Note that a single open problem can give rise to a multitude of papers, like for the Banditron's one ([https://www.cs.mcgill.ca/\~colt2009/papers/041.pdf](https://www.cs.mcgill.ca/~colt2009/papers/041.pdf))  


Regarding books: the book of Nicolo' and Gabor is very comprehensive but I don't think the potential point of view is the right one. Indeed, I just wrote a survey paper with Nicolo' and we used the FTRL/OMD view of my notes and not the potential view."
MachineLearning,Flask is used in production all the time
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Oh, another cool user of FX is [FlexFlow](https://github.com/flexflow/FlexFlow#pytorch-support), which uses FX to take your PyTorch model and automatically parallelize it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I just received an email inviting me to participate in a telephone coding interview, on a shared screen. 

I’m of course ecstatic but I don’t know how big of a deal making it to the first step is. Is this common?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Decouple everything as much as possible. Apache Superset is a great tool for user auth, browser-based interface, database connection, user-created charts and you can render your own HTML inside your own chart. 

As for the backend there are plenty of those available. If you're new to web-dev then Django can be a good starting point. Just don't rely on its templating UI system as its a bit dated and rigid."
MachineLearning,"I'm curious what motivated the new `torch.fx` functionality. For context -- this allows for doing source-to-source transformation of Python code, e.g. to transform already-written functions.

It's a neat idea but it feels a bit magic to me. What's the use case?

I have seen this done before as a way of autodifferentiating functions. But that is of course not necessary here."
MachineLearning,Expose the model as REST service using flask or may be fast-api and let ui folks deal with however sleek they want to build the ui controls
MachineLearning,"Flask and javascript are different things, you'll still probably have to learn javascript to work with frontend. Flask is framework for serving HTML on the backend and javascript is the language that allows you to interact with the browser via DOM. You can serve the model on the backend in python and use Flask to handle serving webpages and HTML. In the frontend, you can use javascript/css/HTML to render whatever your model is serving and handle user inputs."
MachineLearning,"Trying to figure out if you mean a mid level data scientist who’s trying to take it to the next level, or a “Medium”, as in the website, level data scientist which consists of copying and pasting someone else’s project on GitHub and writing a completely useless blog of regurgitation.

I hope it’s the former:)"
MachineLearning,Seems more suited as a r/ai or r/futurology post to me. This is all speculative.
MachineLearning,"Good question. I don't want to necessarily focus in a specific.
I'd like to either learn more complex techniques to handle my data for regression/classification tasks. Instead of throwing standard models at the problem and see what works best. 

I'd like to learn more theory on how each model/technique is better for a certain type of data and how can that help me in practice. And I'm not saying the kind of rule thumbs such as 'CNN for images' and 'RNN for sequences'."
MachineLearning,"I don't have a concrete answer, but it heard it has no impact on ML. Can you actually get a 3060? I tried to get on the EVGA wait list as soon as it launched. Couldn't get the page to load for the first 35 minutes, and a week later, the queue has only gotten to 11 minutes past launch."
MachineLearning,May be you should specify what exactly are you looking to get out of these courses? What is your motivation behind taking such course? Do you feel like you have some skill gap in some particular area? Does something excite you which you haven't had chance to study?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"So you're saying the connection is prediction? I can see that there's a bit of a connection but that seems like kind of a shallow point. 

Also, I just want to say that I don't think I would choose to use the word ""prediction"" for data compression / AGI. I think the term ""Pattern recognition"" fits bet. It's kind of like predicting, I just like it better since it emphasizes that predictions are being made based on some pattern observed in the data."
MachineLearning,There should be some graveyard or something for these kinds of things. I produced 3 to write one paper.
MachineLearning,AGI requires prediction.  What does compression require?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"They dropped it to 12GB. While not the cheapest option, for the money I think the best ML workhorse right now is the Titan RTX. At least where I am their 2nd hand price is slightly higher than an used 2080ti and they preform pretty well for ML. Availability seems better than anything current gen and with 24GB of vram they are on par with the 3090. They won't out compute the 3090 but it isn't slow."
MachineLearning,[deleted]
MachineLearning,"Thank you so much for completing the survey! You're right– the device is primarily used for diagnostic purposes, so you likely wouldn't encounter it in a clinical setting."
MachineLearning,"I did the survey, however I don't think I would ever need to use the device as I already have pretty bad diabetic retinopathy and I'm not sure if the device would tell me anything new."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Lots of options in this space now it seems. Someone else in this thread linked bpycv. I was personally using blenderproc just a little while ago
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks man, You are Awesome !!!"
MachineLearning,Thanks for sharing!
MachineLearning,"You could do this in Excel with a formula, but you said any tool, and an easier solution is to use a text editor like [Sublime Text](https://www.sublimetext.com/).

1. Paste the data into Sublime Text
2. Go to Find &gt; Replace
3. Click the button on the left that looks like "".\*"", which when hovered on says ""Regular Expression""
4. In the Find field put: \^.\*&gt;(.\*)&lt;.\*$
5. In the Replace field put: $1
6. Click ""Replace All"".  Boom.  Only what's in between &gt;&lt; remains.

Here's a little video of the setup in action.

[https://imgur.com/a/iH3Lcu7](https://imgur.com/a/iH3Lcu7)"
MachineLearning,"Paste into Column A in Excel, put this formula in B1 and autofill down.

    =MID(A1,FIND(""&gt;"",A1)+1,FIND(""&lt;/"",A1)-FIND(""&gt;"",A1)-1)"
MachineLearning,"Interesting, synthetic data for tests in a CI pipeline is definitely possible. Just to give you something visual, there are some [example synthetic images in this case study](https://towardsdatascience.com/training-ai-with-cgi-b2fb3ca43929) I did a while back (using an early version of zpy)."
MachineLearning,[Regex](https://en.m.wikipedia.org/wiki/Regular_expression)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Why not?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I love how I knew what it would be based on the context and it being an early comic...
MachineLearning,"Ah, yes, that looks to have done the trick. Serves me right for copy/paste without thinking about what's actually going on.


Thank you!!"
MachineLearning,"Yeah, I was hoping to see those.  Unless I see what it can produce, I won't care.

I developed a package called [kwcoco](https://pypi.org/project/kwcoco/) (kitware-coco, which is mainly designed to improve upon pycocotools). However, because I need CI testing, I need to be able to generate toydata. There is a ""toydata"" module to render toy detection / object tracking problems.

For an example of a generated image: 

[https://warehouse-camo.ingress.cmh1.psfhosted.org/daaaa9b52cd246e091582594f23e4bfc7dd499a3/68747470733a2f2f692e696d6775722e636f6d2f566b307a5548312e706e67](https://warehouse-camo.ingress.cmh1.psfhosted.org/daaaa9b52cd246e091582594f23e4bfc7dd499a3/68747470733a2f2f692e696d6775722e636f6d2f566b307a5548312e706e67)

If zpy can do better than this (which shouldn't be hard) I'm interested. But you've got to show it on your main page!"
MachineLearning,"Title:SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval  

Authors:[Yang Bai](https://arxiv.org/search/cs?searchtype=author&amp;query=Bai%2C+Y), [Xiaoguang Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+X), [Gang Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+G), [Chaoliang Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+C), [Lifeng Shang](https://arxiv.org/search/cs?searchtype=author&amp;query=Shang%2C+L), [Jun Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+J), [Zhaowei Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Z), [Fangshan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+F), [Qun Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+Q)  

&gt; Abstract: Term-based sparse representations dominate the first-stage text retrieval in industrial applications, due to its advantage in efficiency, interpretability, and exact term matching. In this paper, we study the problem of transferring the deep knowledge of the pre-trained language model (PLM) to Term-based Sparse representations, aiming to improve the representation capacity of bag-of-words(BoW) method for semantic-level matching, while still keeping its advantages. Specifically, we propose a novel framework SparTerm to directly learn sparse text representations in the full vocabulary space. The proposed SparTerm comprises an importance predictor to predict the importance for each term in the vocabulary, and a gating controller to control the term activation. These two modules cooperatively ensure the sparsity and flexibility of the final text representation, which unifies the term-weighting and expansion in the same framework. Evaluated on MSMARCO dataset, SparTerm significantly outperforms traditional sparse methods and achieves state of the art ranking performance among all the PLM-based sparse models.  

[PDF Link](https://arxiv.org/pdf/2010.00768) | [Landing Page](https://arxiv.org/abs/2010.00768) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2010.00768/)"
MachineLearning,"I just realized that the second model trains on the unnormalized data since you don‘t pass the normalized_ds in the fit method. So you can try this and also watch out that you normalize the validation set as well. Currently, both normalized sets have the same name."
MachineLearning,I hope so but cannot confirm yet.
MachineLearning,"Title:Autoregressive Entity Retrieval  

Authors:[Nicola De Cao](https://arxiv.org/search/cs?searchtype=author&amp;query=De+Cao%2C+N), [Gautier Izacard](https://arxiv.org/search/cs?searchtype=author&amp;query=Izacard%2C+G), [Sebastian Riedel](https://arxiv.org/search/cs?searchtype=author&amp;query=Riedel%2C+S), [Fabio Petroni](https://arxiv.org/search/cs?searchtype=author&amp;query=Petroni%2C+F)  

&gt; Abstract: Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. Current approaches can be understood as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach has several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion. This mitigates the aforementioned technical issues since: (i) the autoregressive formulation directly captures relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the softmax loss is computed without subsampling negative data. We experiment with more than 20 datasets on entity disambiguation, end- to-end entity linking and document retrieval tasks, achieving new state-of- the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their names. Code and pre-trained models at [this https URL](https://github.com/facebookresearch/GENRE).  

[PDF Link](https://arxiv.org/pdf/2010.00904) | [Landing Page](https://arxiv.org/abs/2010.00904) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2010.00904/)"
MachineLearning,"Fortunately, considered documents were rather short (mostly receipts or one-pagers), so the max sequence length of 1024 was enough in most cases. When sequences exceed 4-5k tokens it is hard to fit them even on top-tier GPUs due to the quadratic complexity of the Vanilla transformer. There are at least three ways to overcome this problem:

1. Use sparse Transformer architecture as a base model. I would prefer ones with a global receptive field, e.g., [Routing Transformer](https://arxiv.org/abs/2003.05997).
2. Employ word-vector elimination, i.e., removal of non-useful document passages in early layers. We have recently proposed a [method that can be used here](https://arxiv.org/abs/2009.05169) and is trainable in an end-to-end manner.
3. Process documents in chunks as TILT returns `None` answer if required information was not present in the document (or its fragment).

The problem with 1 and 2 is that although they can move the length limit considerably,  there will still be some upper bound. The problem with 3 is that it is not easily trainable without information in which document chunk the answer is present. Moreover, it requires some form of results aggregation."
MachineLearning,"&gt;ill impute themselves while modelling. (Or can u elaborate your question, coz I'm still in dilemma)

The idea is how to use ml to find data that should be there but isn't. The data would be not missing but I want to see if there are data points that aren't there."
MachineLearning,"I found GCP to be unreliable as well, and switched to AWS for my side project. The experience with AWS was much more reliable, with GPUs and compute resources readily available at many sizes. Comparing using AWS to my local machine, I spent less time debugging my tensorflow/cuda install and more time programming. I highly recommend AWS, as you’ll get hell of a lot more power and time spent on your actual problem than working on a local machine, and more support and reliability than GCP’s current offering. 

PS I didn’t find nearly as many disconnects from unstable internet to notebooks in AWS compared to GCP, but your mileage might vary."
MachineLearning,I was literally hacking together a script for this. Amazing!
MachineLearning,[deleted]
MachineLearning,"Very interesting, i am also conducting my thesis in applying DRL in financial markets.
Well done 👍"
MachineLearning,"Its worth pointing out some FAANGs, for example, will look very favourably on PhDs in ML. It depends what you want to do - if you want to be a data scientist, or even an ML engineer (e.g. helps orgs put ML innto production), an ML PhD will help. If you want to do cutting edge ML at big companies like Facebook, DeepMind, etc, then it definitely really helps"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,It depends. You can go far with sql alone but also face issues when you try to optimise joining.
MachineLearning,"If you're not planning on training on the CPU itself, a 3950X or 3900X will serve you very well (personal experience). If you're willing to wait, the 5000 series has much better clock speeds and doesn't have the weirdness in scaling that was introduced with multiple CCX nodes (NUMA).

So it took that much because you were training on the CPU, but if you're doing GPU training you'd be batching it between the CPU and GPU so the runtime memory requirements will be lower. However, that said, the code must be structured not to load the entire dataset at a time in order for this to be true, and sometimes you'll get  unlucky. If you're worried about that, 128GB is the way to go. If you're writing your own data loaders and know how to only load partial batches in at a time to pass to the GPU (pytorch, tf both support this out of the box), then you'd be okay with 64 or 32 GB.

Storage, not sure what your use case is, but having multiple drives like that can be an organizing nightmare. I'd highly recommend a single, small fast drive and a large slower one. Especially if you're planning on downloading entire datasets, which can range from gigabytes to multiple terabytes. 3 fast drives is technically fine, but you don't really gain anything from doing that. If you want 6 TB of storage, then get a 2TB + 4TB config. PCI lanes and m.2 slots are also something to worry about. Not all motherboards have enough slots to take 3 nvme drives at once. Each one also uses up 4 PCI lanes, so you'd be using up 12 lanes just for storage, which will eat into the bandwidth to your graphics card."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;https://arxiv.org/abs/2007.13916

[https://arxiv.org/abs/2007.13916](https://arxiv.org/abs/2007.13916)"
MachineLearning,"Yep that's entirely a fair point. I haven't run into CPU training in a while though, at least in my research. However I'm sure someone is still doing it, otherwise why would they keep supporting it."
MachineLearning,"Which one?  The ""Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases"" paper?"
MachineLearning,"Training on synthetic data and then fine tuning the model on a relatively small set of real samples has shown great success, in comparison to only training on the real data."
MachineLearning,[https://xkcd.com/927/](https://xkcd.com/927/)
MachineLearning,Absolutely not true. We use this technique extensively and it results in measurable improvements over just real world tagged data.
MachineLearning,"Can someone who knows a bit about this compare to, e.g., [bpycv](https://github.com/DIYer22/bpycv)? Seems like they are doing very similar stuff"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Well color me surprised! I hadn't seen that, my bad. I still think GAT would be more appropriate for this project, but such is life. Thanks for pointing this out!"
MachineLearning,"For now material jittering just messes with specular, roughness, metallic, and a texture image. But this is a good idea and we have put it on our roadmap!"
MachineLearning,"My understanding is that for AI research, native proper MKL support may be more important. I know there are hacks to get AMD working with Intel MKL - but I've heard that some systems are unstable when they use the hack. 

OP - if you plan on doing things related to trying to accelerate CPU based model training - consider Intel anyway, but otherwsie what Onyx says is correct."
MachineLearning,"Yes the main functionality is generating 2D images (color, segmentation, depth, etc) from a 3D shape. You are basically making a simulated 3D world and taking infinite 2D images inside of it."
MachineLearning,"I've worked quite a bit with synthetic data generation for ML in Blender, so this is a nice effort. Can the material jittering handle bump map jittering too?"
MachineLearning,"Is it possible to generate 2D images? I am a bit lost here, since first tutorial is about 3D.
Is it somehow similar to GANs?"
MachineLearning,"Only pursue a PhD if you are truly interested in spending 4+ years doing research.

To answer your questions. Yes, I think a PhD can be useful for industry roles. I don't think it's a requirement. There is definitely a pay bump compared to a fresh graduate.

But I don't think you should pursue a PhD only for a pay bump down the road.

Source: I dropped out of my PhD to work at a CV startup as their first employee. I thought I wanted to do research going into my PhD, but turns out I didn't."
MachineLearning,"Yeah, thansk for commenting. We looked into segmentation, but we need our shapes to be invariant under translation, rotation and scale. And I haven’t found any segmentation methods that ensure this. Unfortunately."
MachineLearning,One more tipp. Put the pc in another room and remote into it. It will generate a fair amount of heat and noise under load.
MachineLearning,"&gt; I'm not trying to crack a pseudo Random Number generator, but rather building a ml model that given a set of numbers could predict the next one with a bit more than 55%"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Have you tried alwaysAI? Check it out alwaysai.co
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Image-GPT didnt have an auto encoder. Because of that it uses an infeasible amount of VRAM. DALL-E has an autoencoder and lots of other implementation differences.
MachineLearning,"Definitely! We love to talk synthetic data, so even if its just a quick zoom call we love hearing about people's challenges and success stories."
MachineLearning,"Greta point on docker. If you get it working locally, then just send them over to a cloud instance en masse and free up local resources."
MachineLearning,"That has been the case until a new metric was published in the paper. Systems that perform well according to classic measures of performance may perform quite poorly on streaming perception. Optimizing such systems using the newly introduced metric can make them far more reactive, and allow detectors with greater latency than the frame rate (which are more accurate as well).

One insight from the research is that the solution isn't necessarily for the perception system to run faster, but to occasionally take a well-timed pause. Skipping the processing of some frames prevents the system from falling farther and farther behind real-time events. 

Another insight is to add forecasting methods to the perception processing. Just as a batter in baseball swings at where they think the ball is going to be — not where it is — a vehicle can anticipate some movements by other vehicles and pedestrians."
MachineLearning,"One more tip, maybe go for AMD CPU and try 1st 64gb (2x32gb), if you need more you can add it later.

Between air and water colling, water coling cpu and gpu can drop temps very much, but it can leaks and you have to be careful. 

I have air cooling because its safe and i need to use my pc on diferent spots smetimes. Pick a case with good air flow (mesh for example) and put 1800+ rpm fans (like arctic or noctua). My case come with 3 fans 1350rpm they are great but more rpms fresher will be the gpu."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Cuda v11.1 + cuDNN 8.0.5 +  tensorflow 2.4.1 (at windows this works really good, i wanted to try on linux but don't have time to fully setup it)

TF 2.4.1 have gpu inside it you dont need the gpu version or nightly.

Try here: [https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html](https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html)"
MachineLearning,"It was a very standard windows gaming laptop. I tried to install CUDA by following instructions, and after a day of trial and error I read that this specific machine, for some reason, can't run CUDA. I don't remember the exact reason. It was pretty obscure. I just want to make sure it won't happen again."
MachineLearning,"Thanks for the thoughts, some answers to your questions below:  
**On Sim2Real Gap**: Training on synthetic + real data is proven to result in better model performance than training on just real data alone \[1\]\[2\]\[3\]. Training with synthetic data only is also possible in some applications, but it does take time and effort to iteratively move the distribution of synthetic data such that it overlaps the distribution of the real data (this is known as the sim2real gap).

**On Unlabeled Data**: Lots of unlabeled data is useful to create an embedding space, and for things like classification and clustering. However most real world applications use supervised learning and labeled data because the output of the model (for a given input) needs to be something very specific.

**On Blender**: We actually do render our data in a headless docker container running on a GPU in the cloud! We are working on packaging that functionality into our repo to make it seamless for users.

\[1\] [https://arxiv.org/abs/1804.06516](https://arxiv.org/abs/1804.06516)  
\[2\] [https://arxiv.org/pdf/1909.11512.pdf](https://arxiv.org/pdf/1909.11512.pdf)  
\[3\] [https://aireverie.com/rareplanes](https://aireverie.com/rareplanes)"
MachineLearning,It would be really good to put example images at the top of your README!
MachineLearning,What version of cuda and cudnn are you using?
MachineLearning,"Very interesting, my team uses Blender scene generation for CV extensively. I forwarded to my team, and we may reach out to you."
MachineLearning,"If you use a physics-based renderer with realistic material and lighting, training on synthetic images can actually transfer pretty well. AFAIK that's what's NVIDIA is doing to train their self-driving cars (along with real captures too)."
MachineLearning,"Not true, see the threedworld paper, where they show it to transfer similarly well as ImageNet."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You should go to cloud.

1. you can use google Collab
2. you can use AWS

both have hardware better suited to AI/ML than in home consumer parts. I have 10900K + 3090 + 64gb mem, and only test little chunks of my code locally, and then run most of it on google Collab / AWS. Both have free tiers, but you can scale up or down depending on your needs. They can also get very expensive if you dont remember to turn off your instances (paid cases) ."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you for for your advice.
MachineLearning,"Something like ""FAIR paper claims large uncurated data is sufficient to train visual feature extractor"" actually tells us what it's about. Unhelpful parts of current title include ""new"" (we assume it's new) and the link (belongs in the body of the post anyway)."
MachineLearning,"I know, my friend got a 3090, he spent a full week working on the environment, no luck yet. But still, I would like to get something that's top class because it will have to hold for quite a few years before I can afford an upgrade."
MachineLearning,This is a really interesting challenge.  I'd guess that the common balance for self-driving vision systems is to always have the latency be shorter than the streaming frame rate and tradeoff accuracy/size of the detector.
MachineLearning,"unfortunately the transfer of that sort of synthetic data has always shown to be extremely limited when you transfer it into real world data distributions, its probably more beneficial to train unsupervised on lots on unlabled data rather than lots of labeled synthetic data of a completely different distri.. Also I feel like blender for synth data is only ever gonna be really useful when you can headless render on gpu, which afaik is still broken. So why use blender in the first place, why not use some standalone renderer?"
MachineLearning, I have one or several pictures of a specific cup. How can I detect the specific cup I want among the many cups?
MachineLearning,I would not go with a RTX 3090 if you are working with tensorflow. RTX 3090 require cuda 11 and a cudnn version that's not available in conda. You need an unstable nightly build of tensorflow and the compatibility issues can be a pain.
MachineLearning,"Thank you for sharing your experience, I am working on medical related ai applications. Unfortunately, I am not funded by any association, so I have to be really careful about how I build my PC, making sure that it's not too expensive, but also meeting my needs."
MachineLearning,"Glad you like it! We are still just getting started, so any feedback or feature requests will really help us out."
MachineLearning,Why?
MachineLearning,"Thank you! BirdNET is definitely fascinating. And congratulations about your upcoming thesis! 

And yeah, I've worked with MATLAB for a long time, but wanted to branch out to Python!"
MachineLearning,"Great, I was thinking similar project good job."
MachineLearning,Thank you for this.  I have wanted to do this for a long time but never could find the will.
MachineLearning,"I used to train my models on a campus computer, but now that computer is occupied by others, so I have to get one myself."
MachineLearning,"I get your point, thank you for your advice, but my dataset is quite large, and my experience using Google colab wasn't exactly that great, unstable internet connection kinda ruined it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you.
MachineLearning,"Thanks for your advice, the AMD 5000 series seems like a good choice, but they're not available in my country. Do you think I should wait a few more months, get a 3000 series, or wait a few more months?

Also, about the RAM, the dataset that I'm working is large, I used to train my models on a threadripper, it took around 100GB of RAM. I can't afford a threadripper, so I think I will switch over to GPU. How much RAM do you recommend for my build?

And for storage, I think I'll get three 2TB ones, they should be enough to store all my datasets and files."
MachineLearning,"In object detection papers and benchmarks, what is meant by 'single-scale'? 

Does this perhaps mean that during testing the input image is passed in only once at a fixed resolution?(instead of providing the network with multiple resolutions and aggregating the results)

E.g, in the EfficientDet paper, the authors note ""Table 2 Compares EfficientDet with other object detectors, under the single-model single-scale settings with no test-time augmentation"". 
E.g 2, see COCO benchmark entries [here](https://paperswithcode.com/sota/object-detection-on-coco)"
MachineLearning,"I have i9 9900x, 128 GB and a 2080 Ti plus NVME m.2 SSD. I rarely use it for machine learning stuff. I just offload to cloud so I can play cyberpunk or something while I wait."
MachineLearning,Interesting article -- what a terrible title for a reddit post!
MachineLearning,i agree whole heartedly.
MachineLearning,"I built a rig for DL research last September. I  managed to get a 3090 during the bot frenzy directly from NVIDIA. 

I went with an AMD 3700x thinking I would upgrade to a 5-series later down the line but I honestly don't think I will anytime soon. The 3700x is suiting me just fine. Similarly for memory I bought 2x16GBs thinking I could pick up another 2 sticks later, but again I don't think I will. I would save money on the memory side of things and go with 32 or 64GB.

Honestly if I could do it all over again I would probably go Intel and save myself some of the headache of getting PyTorch running efficiently without MKL. It was not particularly difficult to do but there were times when I questioned my decision...

Some context: I do research in computer vision as part of my PhD and I was fortunate to have my rig fully funded by my lab. I typically run most of my training on Compute Canada (which is available to most Canadian universities) on 4xV100s, but queue times can make iteration/debugging a bit cumbersome; hence why I like having a local machine.

There is definitely an argument out there for just running everything in the cloud even if you don't have access to free compute. But this is your decision to make and completely depends on your use case, funding situation, etc."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I actually just started learning pyspark for work. If you understand sql then you can do pyspark.

Its pretty easy"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,A CNN for the image and RNN for the metadata (description) joint together sharing weights
MachineLearning,Ideally You should fix as many things as possible and only vary the one or two things you propose (even down to the random seed).
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"IMO doing a project is the most efficient way to learn. 
I heavily referenced pyspark's api documentation itself and found it really useful.
I used Pyspark for data processing (i.e feature engineering) and building them into pipelines on Google Dataproc. I think such use cases are pretty align with Spark's value proposition of processing/dealing with large amount of data."
MachineLearning,"You sound like a noobie, are you a noobie? If you're, don't waste your money, proceed with your current hardware to the point where your hardware tells you straight, I'm not doing this anymore. 
At that point, you'll have a pretty good idea on what kind of hardware you beed."
MachineLearning,Cry and close the book
MachineLearning,"Good point to clarify, thanks. The script I used to generate the png does load in the lossless binary and extracts the individual images, which then gets sorted by category and saved as png."
MachineLearning,"Hi,
I found this podcast very useful for building a deep learning machine. The type of models highly influences the decision which hardware is suitable. Here's the link to the podcast episode including some more information in the show notes:
https://podcasts.google.com/?feed=aHR0cHM6Ly9jaGFuZ2Vsb2cuY29tL3ByYWN0aWNhbGFpL2ZlZWQ&amp;ep=14&amp;episode=Y2hhbmdlbG9nLmNvbS83LzExMjY"
MachineLearning,"Thanks for the reply! The png images are generated from the python binary data set, visual comparison between the two sets did show that the png looked like it did not have compression artifacts, at least to me :)"
MachineLearning,"This argument ""one thing high school kids call a fallacy is important, therefore all things they call fallacies are also important"" is a famous fallacy as well.

The thing is lots of things in practice are really helpful and at the same time are technically fallacies. Argument from authority is a great example. Sometimes you go really wrong by listening to an expert. But in practice they're often right about the field they're an expert in ."
MachineLearning,"Not a sexy answer, but I agree. Price out what it would be do what you want in the cloud and compare."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Well I would say it's an improvement over using a single metric but as you point out the method has a variety of deficiencies. Mostly that individual data points can shift between strata from one model to the next. If this shifting was minimal then yes you could compare models quantitatively with this technique, otherwise probably not. At least you'd want to be very careful on what conclusions you draw.

A far better approach would be to define your subpopulations in advance (using domain knowledge and/or unsupervised techniques like clustering) and evaluate performance on those.

Really this technique sits within the wider realm of stratified sampling where you impose constraints on purely random sampling to ensure that all portions of your dataset are adequately represented in the sample. This is commonly done in classification problems where samples are drawn randomly per class rather than from the entire dataset to ensure proportional representation. The technique you describe sounds like a shorthand hack to get a similar result which could be handy in the right situation. The core assumption seems to be that the learned model is good enough that its error/uncertainty can be used as a signal for meaningfully partitioning the data. Like a lot of techniques, its usefulness seems pretty situational to the particulars of the dataset/problem."
MachineLearning,[deleted]
MachineLearning,"I would advice you to choose cloud.

The seemingly hottest hardwares today will be redundant in a year, as ever year or so something better comes up.
 
Also, the number of parameters in an ML models apparently keeps increasing[Trillion parameter model](https://www.infoq.com/news/2021/02/google-trillion-parameter-ai/). So, your hardware might be a constraint if you plan to tinker with them.

With cloud you need not worry about any of these issues.
And cloud is not costly. It will be infact be cheaper if you get into the vicious cycle of upgrading your hardware every other year or so.

Even if you argue that your use-case is small, and you're only getting started and any such stuffs, I would still reason with you to choose cloud."
MachineLearning,"&gt; but still it killed all ""deep-learning"" research

It really didn't though; [a lot of progress was made between the 70s and the 90s](https://people.idsia.ch//~juergen/deep-learning-conspiracy.html). Just because Hinton et al didn't cite any of it when they started publishing DL stuff does not mean nothing happened in that time."
MachineLearning,"you tokenize into chars,
each car is either 1-hot encoded or (probably better) mapped to an embedding. Depending on this choice your model's input is a sequence of 1-hot encoded vectors (with length := #chars) or a sequence of integers (length := #chars) that are mapped by the first embedding layer.

Either way, the input to the LSTM would be a tensor (batchsize x embedding/1-hot vector length x num_chars)
the outout of the layer would be a tensor (batchsize x out_dim x num_chars) where out_dim is basically your choice.
This is also the input to the CRF layer (or just a linear layer to keep things simple).

You then train the whole thing on sequences where each position has a label that is begin/inside/outside and thus you can calculate cross-entropy loss. So all in all it is basically:  https://github.com/flairNLP/flair, https://huggingface.co/transformers/model_doc/distilbert.html#tfdistilbertforsequenceclassification or any huggingface model ""for sequence classificaiton"" or but just char based instead of word based. The CRF layer (as included in flair) is optional but may be useful."
MachineLearning,"I agree with this approach. However, someone remarked once that if you use the same benchmark dataset with pre-defined splits and same data pre-processing as an existing work, you could get away with copy-pasting numbers from that paper.

However, the above-mentioned approach from zyl1024 is of course better."
MachineLearning,He's a real friend if he doesn't stop bragging :D
MachineLearning,"yep, my friend managed to order one online just before it went out of stock, he brags about it all the time."
MachineLearning,Look up this: Symmetry-Based Disentangled Representation Learning requires Interaction with Environments
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yes, it is the case (there is some entertainment recorder but they tend to be very affected by noise).

I think that the biggest point to focus on is the amount of available and well-labelled data. For the physical examination that you mentioned, there are abundant because in some countries they were mandatory for specific examination.

For the second part, I think that it is very necessary to solve two majors issues: 

\- The fact that it is necessary to adapt the ML aspect to health, e.g. with some mental disorder using a synthesised voice or a chatbot (instead of a face to face discussion) may cause bias in the assessment part.  The legal aspect must also be very complicated to manage. 

\- The amount of data that are sometimes not available sufficiently or badly annotated. For instance, it is not a mental care problem, but at the beginning of the pandemic, we saw a lot of ML-based approaches to detect covid from CT-scans. After deeper analysis, some of the models were actually biased by the text displayed on the scans...

To sum up, I think that it will be used in the next years (and it will be great), but to help doctors not to replace them."
MachineLearning,thank you so much . i should take a look for that model.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You might be better served by an AMD 5950x rather than an i9 if you're going for top-of-the-line, the extra threads won't help you in training per-say, but builds/compilations will execute much faster with the extra threads. 

I currently own a 3900x and a 3090 and use them for this type of work. More threads definitely helps if you end up having to compile something from source.

128G of RAM is also likely overkill, but it depends on your application of course. It can help with large datasets, but if you're working with GPU accelerated models anyway you'll be limited by the 24G VRAM. I've found 32G to be a good amount, but sometimes I've wished for 64. If you're looking to save anywhere, the RAM will likely be the spot.

I'd also highly recommend an M.2 NVME SSD. You can get 1-2 TB ones these days for pretty cheap and it's hard to go wrong as long as you get one with good reviews from a reputable company. Sabrent and Samsung are my go-to recommendations these days. 

If you need more storage than that then you can go for an additional SATA HDD. 8-12 TB is currently a decent middle between ridiculous pricing and enough storage to last for a long time."
MachineLearning,"I understand that. And that is the usual scenario in which I have seen stratified sampling work. To measure the performance of under-represented groups. But my question is really regarding this new type of sampling.

This is confusing me and I want to learn more about it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You can ask the guys on r/pcmr about the 3090 availability right now, they're not really happy"
MachineLearning,"Thanks for linking this, it looks great!"
MachineLearning,More instructive than the actual scores would be model performance on different types of data points. How model performance differs for different subpopulations of your dataset can be very informative. Just because one model has a higher aggregate score doesn't mean it'll perform better in a production environment on the subtasks that actually matter.
MachineLearning,"if you are processing text, you can downgrade the gpu, but the general rule is to max out the memory first,  then jack the cpu count then get as fast a ssd as you can get . you also want to make sure the os (usually linux) has swap turned off or tuned so you dont need it"
MachineLearning,What model would you use to identify the type of the product from an e-shop given a picture of it and the text from its advertisement?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Wow, this is awesome! I have a mother who's a bird enthusiast and has been trying to teach me different birds for years now, but nothing ever stuck. Then I stumbled upon the BirdNET application, which classifies birds based on sound recordings, which made it a bit more accessable and comprehensible to me (it shows the recording instantanously in a spectogram, which I really like). So since then I've become really interested in learning different birds, and I'm also writing my bachelor's thesis about bird classification using sound now.   


You say, this was a project for you to learn python. I take it you've been programming for some time in other languages, already? :-)"
MachineLearning,"There’s a few key fundamentals to know which will influence the workflow you use when writing code. Such as writing a data frame to a spark table to check sql logic, then using UDFs etc. if you already use Python and sql then you’re pretty much there. 
I think as long as you understand how the lazy evaluation works you can figure out what works for you"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"What would be the input, output, and labels of the transformer/lstm layer? Is it on one architecture with CRF layer?"
MachineLearning,"I think one of the major issues is that we as a field lost track of why we are chasing ""SotA metrics"" on benchmark datasets. We have to ask ourselves: ""Do I want to make a system that's more generally able to solve problems"" or ""Do I want to build a system that can solve a specific ~~dataset~~ problem extremely well?"". Many papers claim the first, but do the latter.Whats even worse is that the latter problem is usually what you want in industry, but because the authors of papers are so confused about what they are doing their solutions wont even be used for that."
MachineLearning,Perhaps that's why so many potential physicists pivot to neuroscience/ML? More to be discovered. (Including myself)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,The pursuit of profit is compelling in both free markets and heavily regulated markets… That’s why ML needs to be used in the public sector to effectively analyze cost/risk management in a visible and patient-centered way.
MachineLearning,"It does not technically require that the data is Gaussian, but only if the data is Gaussian the decorrelated data will follow a Gaussian normal distribution. If you have some weird non-Gaussian distribution and apply a linear transformation, you will get uncorrelated but not independent features. Usually, the density of the latent distribution is almost zero almost everywhere, and there are some modes at different ""places"". That is why if you feed random noise to the decoder, the output will not look like a realistic image. This does not change if you do linear PCA on the latent space because these areas with 0 density that correspond to unrealistic images still exist.

It's worth a try with kernel-PCA maybe, I haven't tried that and I must say my intuition for kernel-PCA is not as good. But it adds capacity to the model, so it doesn't necessarily have the issues I mentioned for linear PCA."
MachineLearning,The question is obviously about pseudo random number generators.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Why would PCA require a gaussian distribution? From what I understand, the main requirement is that the datapoints are distributed more or less along perpendicular lines, but the distribution can be arbitrary. A corner case where it would definitely fail is for example a donut shaped distribution on a plane (however it won't fail with kernel PCA!)"
MachineLearning,Thanks for the update
MachineLearning,From one of the facebook recruiters.
MachineLearning,"For #2 and #3:

I think you could tokenize on chars and then use some typical model (LSTM/Transformer) where each char has an embedding, the LSTM/Transformer layer produces context-sensitive representations for each position and tehn there is a CRF layer on top. You could label sequences with BIO tags (begin-hs, inside-hs, outside) and train just like NER systems.

As usual it'd probably be a good thing to first build a language modelling task without the CRF layer to get good embeddings and meaningful weights for your LSTM/Transformer layers and all data (without labels). Only afterwards add the CRF layer and train on labeled data.

**That said, I don't know if that's actually a good idea.**  I'd imagine a sequence tagging approach can work despite the messy data, but I'd also expect that you will not reach 100% accuracy.

Depending on the application, a different system where you use a simple method, like regex, to very reliably identify HS codes in x% of the cases and just give the remaining (100-x)% to humans could be much more useful than a complex model that gives an answer every time but makes mistakes every now and then."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"My experience has been that a lot of people - especially men - are very good at masking symptoms of mental health issues from other people, so they don't get support until it's become quite serious. (And tragically, as demonstrated by the suicide statistics, many people don't get help at all) If a text analyser or other methods can alert the patient that they need support sooner, the results could be great. It's very difficult for a patient to make an accurate self assessment in the early stages, they may feel like they're exaggerating the problem and/or feel like a burden of they speak up."
MachineLearning,Where did you hear that? I'm curious
MachineLearning,"EMTALA protects patients from discrimination, eg being turned away for emergency medical services due to lack of insurance or being under-insured. On its own it sounds great. ERs receive the bulk of people needing inpatient psych services. Even when your doctor refers you to inpatient, you need to go to the ER for medical clearance and then presented to the psych hospital. This is where the gate keeping happens; when an uninsured or underinsured patient is presented to a psych hospital, often times a Cosper all will continue to sit on their presentation notifying the emergency department that the patient is still in queue to be reviewed. There is no law that compels a psych hospital to divulge what number the patient is within the queue and thus the presentation gets aged out without a decline; this is why emergency psychiatric service departments are key to managing mental health, substance use, corrections and individuals with developmental delay."
MachineLearning,"Fully agree with you there—it's somewhat of an overloaded term. But the strategy for many authors indeed appears to be 'Take well-known stuff from field X and use it for ML problem Y'. Coupled with spurious maths, it is indeed sometimes fully unclear what is going on...

(by the way: awesome username; it's my favourite LaTeX package)"
MachineLearning,"Sorry, by novelty what I really meant was how it ""compares to the state of the art of another field"", not how it compares to the state of machine learning."
MachineLearning,They will begin interviewing starting next week.  So probably will send mails during this week and next week. If not heard by mid March you can expect that you are not selected for the interviewing process.
MachineLearning,[deleted]
MachineLearning,So you're saying the emperor actually *isn't* wearing clothes? Or? What? This is quite shocking
MachineLearning,This is the most correct way imo. You don't know that your evaluation method works the same was as the paper that you're comparing to unless you can get the same results as the paper that you're comparing to.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Except that most of the times, people will see the paper, and be like ""oh this thing does not work, so it's not worth trying"". 
This is what happened with perceptrons in the late 60s, when Minksy and Papert published a book, ""Perceptrons"", mentionning the limits of perceptrons, and their inabilities to learn complex functions like XOR. Of course it didn't apply to multi-layer networks, but still it killed all ""deep-learning"" research, and it tool a very long time to come back. So wrong or misleading negative results can be very harmful to science, and I think this is one of the reasons conferences are careful about publishing those."
MachineLearning,"BTW, Elon has left OpenAI. He also repeatedly criticized it for not being open, and for the general mismanagement. 

&gt; a group of somewhat-rational people who think we're going to be enslaved by AIs

If you mean LessWrong &amp; Co, then your characterization of the group is not entirely correct (I assume, for humorous purposes). It's not about AI enslavement, but about existential risks in general, including the risk of an AGI going rogue (which is a real and often underestimated risk)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,/r/mlquestions
MachineLearning,Standardizing the language of assessment might also clean up the dataset. Careful assessment of questions maybe by HL7 could clean up ambiguity and interpretation issues during assessment to build a cleaner dataset
MachineLearning,probably golang is trying to cover things up
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Link to SEER: https://arxiv.org/abs/2103.01988
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Once you've saved data to jpg, the data that is ""lost"" in the JPEG compression is already gone. Saving it to another format can't bring it back.

As someone else suggested, load an original copy of the dataset that's provided in a lossless format, such as that from the source."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"IMHO there are two ways:

* Empirics: a positive result must be reproducible under many similar but different circumstances to count as applicable. Here you need to be extremely careful in how you design the different circumstances, see the limited transfer discussion in https://arxiv.org/abs/1801.00631 for example.
* theory: properties like statistical consistency are immensely underrated in ML literature, and universal approximation is overrated. We need theoretical guarantees on algorithms. The UAT is an existence result that tells us nothing of how good an actual trained neural network will be."
MachineLearning,"Bertsekas, Dynamic Programming and Optimal Control, Chapter 6.7.

Hard to google because of ambiguities."
MachineLearning,"If some DL on mobiles will take a hold (something simple like convolutional filters, stile GAN etc) there will be need in low computer power DL research. On the other hand if 5G will work as advertised all the work will be done in the cloud and there will not be any demand for DL on mobile. There still remain robots and drones which should be able to work without net connection."
MachineLearning,"If I understand correctly you still load the jpg images.  Therefore it does not make any difference if you save the lossy jpg as a png. The lossy compression stays in the png.

To circumvent this problem you could try to load the data from the original source [https://www.cs.toronto.edu/\~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html) that is mentioned in the TensorFlow documentation [https://www.tensorflow.org/datasets/catalog/cifar10](https://www.tensorflow.org/datasets/catalog/cifar10) of the data set. Then jpg shouldn't be an issue."
MachineLearning,"I found this example ""process\_mnist\_model.py"", but it doesn't work due to the version of tensorflow and python. Maybe your code can be adjusted to support the latest version."
MachineLearning,"Could you elaborate more on EMTALA and how it defends the patients? This part of the conversion is very interesting. The pursuit of profit might be harmful, but I guess I was too focused on innovations here, so I missed that point before. :)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Right now I have an example script for a simple dense neural network on MNIST data under examples/process_mnist_model.py . I don't have a automated function yet. To create such a file for another neural network you have to code a bit and provide a small subset (maybe &lt;  10%) of the training/test data and unfortunately only dense layers are supported, but I might add support for different layers (convolutional) with examples."
MachineLearning,"I don't, man, what do you think it takes to qualify an algorithm as good?"
MachineLearning,"Define good. Run a million identical networks on the same dataset, but each with a different random seed, and you probably get a couple that perform way better than average. But that is not 'a good algorithm', it is nothing but chance. The same network will perform only average on the next task. That is basically what happens now, only we have a thousand researchers each doing a thousand networks, such that one in 1000 get to write a paper about it.

It is quite damaging to the field that this cannot be said without getting down voted, because it means that we are just chasing ghosts for a large part and we cannot talk about it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"That is the main focus of why I am interested in this topic. As you said there are tons of problems with mental health and there are far from enough resources to tackle them all. Therefore, I want to find those solvable cases and reduce the distance between the number of problems and solutions. 

I see you are not supportive of ML use in mental health, but I would really appreciate your opinion or ideas of what we could do regarding this growing problem? Thanks!"
MachineLearning,"I am currently reading the other paper, and it's quite intuitive. Thx"
MachineLearning,Thanks no worries at all!
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks! I will check it out.
MachineLearning,"Yes, I can agree with that. However, it seems to me that there would be many use-cases in similar depth level as for suicide risk. For e.g. simply detecting other signs from the text, like depression or anxiety. Also, maybe data from smartphones could assist even more. What do you think? Thanks!"
MachineLearning,"Interesting thoughts. I was thinking about how many people are now following the steps they are making, but I am not sure if that would actually lead to any proper mental health diagnosis or insights. 

these might be some helpful things to stay in a proper mental state (when you are already ok). 

Or my lack of knowledge in mental health is not revealing something to me? :)"
MachineLearning,"I've seen a LOT of papers trying to predict suicide and other rare mental health outcomes, and the one thing I've wondered about is—if you predict someone's at high risk of suicide, how can you intervene? Could you use that prediction as a basis for a 5150 hold? Or would you try to prioritize them for more intensive treatment? 

Just wondering. There are a lot of clinical ML systems that try to prioritize interventions based on risk, but in many cases you end up not targeting the right people due to HTE, i.e., the high-risk folks don't respond to treatment, or at least perhaps not as well as those at moderate risk but who aren't prioritized. And in this mental health context, it doesn't feel like we have a whole lot of tools to actually act on these predictions, but I could be wrong."
MachineLearning,You'll find better models than I made on the competition's notebook page. The only issue is making it work real-time.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Please reconsider your position on fallacies in science. Using fallacious reasoning only results in bad science, with the most common example being the unjustified attribution of causality to correlated variables. So even if you get the results you expected in an experiment, faulty logic and experimental design will produce wrong interpretations of the results, which I would say is a pretty big problem in science."
MachineLearning,"I have a bunch of medical billing details. The fields I have are drug, dosage, drug administration format, quantity and unit price for every transaction ID.

The objective is to identify fraud and abuse within this data. I don't have any labels so I'm forced to take an unsupervised approach.

I've found a research paper that used association rule learning to generate ~200 rules (positive and negative association rules).

Traditional implementations generate positive association rules using a frequent item set with minimum support. The paper suggests negative rules have more impact.

(1) I wanted to know how I could implement negative association rule generation. 
(2) And currently I've only used drugs per transaction as the item set. Is there a way I can combine different fields that I have to generate rules?"
MachineLearning,"There are definitely some lossy differences in the jpgs, good call.   


I used a modified version of this script to extract to PNG (which as I understand is lossless ?) then I retrained. [https://gist.github.com/juliensimon/273bef4c5b4490c687b2f92ee721b546](https://gist.github.com/juliensimon/273bef4c5b4490c687b2f92ee721b546)  


  
I'm still seeing the same results, which I'm a little surprise about. Thanks for the tip!"
MachineLearning,"Hi, what kind of program are you in? I would love to learn more about this space and application."
MachineLearning,Side question... what would be an ideal program to tackle these types of question?
MachineLearning,This is pretty much on point. I see the same thing in adversarial attack literature.
MachineLearning,I've never heard of two phase control. Can you provide a reference?
MachineLearning,[deleted]
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"This is getting to an arguably even more fundamental problem at work: what do you do when there are just too many papers for even professionals specializing in the (sub)field to keep up with? 

In theory, more papers is *better*, even if they are just ""I tried X and it doesn't seem to help"", because it means when *you* come up with X, you can look it up in the existing literature, see it has been tried, and either discard it, or if you still want to give it a go, go into it armed with more knowledge (""*this* setup didn't work, but it seems to me like it might be because of Y, so I'll try this alternative approach instead"")

Of course, in practice, ""just search the literature for X"" is likely to take levels of effort comparable to implementing the idea and doing some tests yourself, given how hard searching for a nameless concept in a massive sea of poorly indexed papers is.

So I guess it comes down to, is *that* basically an unsolvable problem, at least for the time being, or could we actually do something about it? Somehow distill and classify the findings of *all* papers into a form that makes discovery trivial? Seems like a tough challenge, but surely if anyone can figure it out, it's the combined might of the ML field. And *if* it does get solved, then I think ""publish literally everything"" immediately becomes an extremely attractive idea that would certainly help at least reduce the sort of biases that lead to reproducibility issues etc."
MachineLearning,"Title:Statistically Significant Stopping of Neural Network Training  

Authors:[Justin K. Terry](https://arxiv.org/search/cs?searchtype=author&amp;query=Terry%2C+J+K), [Mario Jayakumar](https://arxiv.org/search/cs?searchtype=author&amp;query=Jayakumar%2C+M), [Kusal De Alwis](https://arxiv.org/search/cs?searchtype=author&amp;query=De+Alwis%2C+K)  

&gt; Abstract: The general approach taken when training deep learning classifiers is to save the parameters after every few iterations, train until either a human observer or a simple metric-based heuristic decides the network isn't learning anymore, and then backtrack and pick the saved parameters with the best validation accuracy. Simple methods are used to determine if a neural network isn't learning anymore because, as long as it's well after the optimal values are found, the condition doesn't impact the final accuracy of the model. However from a runtime perspective, this is of great significance to the many cases where numerous neural networks are trained simultaneously (e.g. hyper-parameter tuning). Motivated by this, we introduce a statistical significance test to determine if a neural network has stopped learning. This stopping criterion appears to represent a happy medium compared to other popular stopping criterions, achieving comparable accuracy to the criterions that achieve the highest final accuracies in 77% or fewer epochs, while the criterions which stop sooner do so with an appreciable loss to final accuracy. Additionally, we use this as the basis of a new learning rate scheduler, removing the need to manually choose learning rate schedules and acting as a quasi-line search, achieving superior or comparable empirical performance to existing methods.  

[PDF Link](https://arxiv.org/pdf/2103.01205) | [Landing Page](https://arxiv.org/abs/2103.01205) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2103.01205/)"
MachineLearning,"Title:Unadversarial Examples: Designing Objects for Robust Vision  

Authors:[Hadi Salman](https://arxiv.org/search/cs?searchtype=author&amp;query=Salman%2C+H), [Andrew Ilyas](https://arxiv.org/search/cs?searchtype=author&amp;query=Ilyas%2C+A), [Logan Engstrom](https://arxiv.org/search/cs?searchtype=author&amp;query=Engstrom%2C+L), [Sai Vemprala](https://arxiv.org/search/cs?searchtype=author&amp;query=Vemprala%2C+S), [Aleksander Madry](https://arxiv.org/search/cs?searchtype=author&amp;query=Madry%2C+A), [Ashish Kapoor](https://arxiv.org/search/cs?searchtype=author&amp;query=Kapoor%2C+A)  

&gt; Abstract: We study a class of realistic computer vision settings wherein one can influence the design of the objects being recognized. We develop a framework that leverages this capability to significantly improve vision models' performance and robustness. This framework exploits the sensitivity of modern machine learning algorithms to input perturbations in order to design ""robust objects,"" i.e., objects that are explicitly optimized to be confidently detected or classified. We demonstrate the efficacy of the framework on a wide variety of vision-based tasks ranging from standard benchmarks, to (in-simulation) robotics, to real-world experiments. Our code can be found at [this https URL](https://git.io/unadversarial) .  

[PDF Link](https://arxiv.org/pdf/2012.12235) | [Landing Page](https://arxiv.org/abs/2012.12235) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2012.12235/)"
MachineLearning,"Title:Why does deep and cheap learning work so well?  

Authors:[Henry W. Lin](https://arxiv.org/search/cond- mat?searchtype=author&amp;query=Lin%2C+H+W) (Harvard), [Max Tegmark](https://arxiv.org/search/cond- mat?searchtype=author&amp;query=Tegmark%2C+M) (MIT), [David Rolnick](https://arxiv.org/search/cond- mat?searchtype=author&amp;query=Rolnick%2C+D) (MIT)  

&gt; Abstract: We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through ""cheap learning"" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various ""no-flattening theorems"" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that $n$ variables cannot be multiplied using fewer than 2^n neurons in a single hidden layer.  

[PDF Link](https://arxiv.org/pdf/1608.08225) | [Landing Page](https://arxiv.org/abs/1608.08225) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/1608.08225/)"
MachineLearning,"Thanks, I'll check them out. 1"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"As another example (not OP), it is well known that GAN failed to cite prior work such as predictive minimization. And look we can look pass that. The work was 20 years before GAN and did not gain that much traction (I think). It was plausible that it was missed (by all 8 researchers, many of whom were connected to Yoshua Bengio, Geoff Hinton and Yann LeCun team). \*doubts\*

But what's really not acceptable is the follow up work that tries to do something theoretical about GAN. Just look at any paper in this field at the moment. I hate to name names but look at the review for one of these paper published from people working at DeepMind: [https://openreview.net/forum?id=SyGjjsC5tQ](https://openreview.net/forum?id=SyGjjsC5tQ)

Look at the comment:  *I was disappointed to see that the authors make no reference to the rich literature on continuous games where the positive-definiteness of the Hessian has been explored quite extensively as a stability criterion.  The role of this condition dates back (at least) to the work of Rosen in the 60's (Econometrica, 1965), wherein it was introduced precisely as a stability criterion for the convergence of first-order learning methods in N-player games with continuous action sets.*

So the authors just causually missed works dating back to the 1960s, including several foundational textbooks. And then despite this massive oversight, the work was still published! I could *easily* pull up much more papers like this (because I had to do a project on it and basically these people were saying game theory started in 2017)."
MachineLearning,"If you’re hoping to apply metalearning to a real world problem, that’s gonna be trial and error and luck."
MachineLearning,"“Finally, the team offered suggestions for improving the robustness of future architectural modifications. They suggest researchers test proposed modifications on multiple completely disparate codebases; apply the modifications to a wide variety of downstream applications; keep the hyperparameters fixed as much as possible when evaluating performance; and ensure best-practice reporting of results to include mean and standard deviation across multiple trials.”

FAANG wannabe researchers will never do these"
MachineLearning,"Segmentation result fed into CNN? Naive approach but you guaranteed it’s shape based, which sometimes is necessary for interpretable models"
MachineLearning,"You always can't judge your model predictions on the dataset at the first attempt. Moreover GENERALISATION, is important parameter to assess during training. Because when you learn something for 1st time, you may not remember 100% hence would like to give 2 or 3 more readings to improve your understanding. (This is what happens in human life). The same application can be applied to machines in terms of loss values. When they learn/train about something the error wouldn't be minimal and tries to train again and again to decrease the loss and improve generalisation quality. But you should also worry about overfitting when you generalize very well."
MachineLearning,"That's a good point, thanks for the suggestion, I'll check it out!"
MachineLearning,jpeg compression is lossy. i'm not sure if tf's dataset is loaded from lossy jpeg or lossless formats such as png internally. perhaps you can check a couple pairs of images to see if they are exactly the same.
MachineLearning,"According to my suggestions to my friends, I would say as 
1) Numpy n pandas
2) Visualization libraries like matplotlib, seaborn, etc.
If possible try to learn about tableau or Power Bi tools.
3) Databases (SQL), which will help u in long run.
4) A little bit of knowledge in math related like calculus, linear algebra, stats(data mining)
5) then you can jump into ML models and try to understand the math behind them which will make your life easier to understand now(if followed all points)
6) Try to implement the models and participate in competitions.
7) A good knowledge on ML will help you to move forward in DL.

Sources:
1) Greatlearning website (free courses)
2) coursera coursers (Andrew ng)
3) Nptel courses (theory knowledge)
4) kaggle notebooks
5) Dphi competitions
6) and lots of YouTube channels.

These are few points n sources which came to my mind now..but I'm sure there are many more to add

[free online courses for data science](https://www.mltut.com/free-online-courses-for-data-science/)."
MachineLearning,"GANs are tricky. I bet a more straightforward transformer-based autoregressive (sequence prediction) model would be a lot more stable and easier to train and get decent results. I don't know how you frame a 3D voxel world as an autoregression problem, but people have had success using sequence prediction in 2D pixel completion tasks."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; Maybe, but please have a look at e.g. syntactic parse trees for natural language. This is one concept of ""spacial"" cohesion that can be found in language, others might be even more semantically driven. Even though I studied it a little bit, I'm also not a linguistic expert, others might help out here.

I am not a linguist either. Though parse trees would be a processed form of language representation, unlike tokenized strings as data. From raw data images appear much more self-contained semantically. I guess the training approach that's suggested by Hinton was a kind of contrastive learning were Spacial crops of that data are hidden."
MachineLearning,"Usually ML models don't accept missing data. If you don't impute then the model will throw an error as missing data. So obviously every ML model will identify the missing data and few models will impute themselves while modelling. (Or can u elaborate your question, coz I'm still in dilemma)"
MachineLearning,[deleted]
MachineLearning,"""Here we provide the implementation but the code is esoteric and requires 10 GPUs to train, good luck"""
MachineLearning,"Am I the only one who thinks the idea that the Hutter prize has anything to do with artificial general intelligence to be absolutely crazy? It's just compression, isn't it?"
MachineLearning,exactly
MachineLearning,"How about we write an opposing paper claiming “non-modified transformers fail to generalize”after taking modified transformer hyperparams and applying to regular transformer!

Would make us quite unhireable at Google, but a worthy cause."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"every ML researcher is empiricist, in some way"
MachineLearning,Wtf
MachineLearning,Thank you for detailed response. Guess I'll try it out as well. Hoping it doesn't go kaput when used with Azure ML Studio (Horovod never detects multiple nodes in a cluster).
MachineLearning,Yes.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[deleted]
MachineLearning,"Great reference! Considering I’m a PhD student in NLP at UMass Amherst, I’m probably going to suggest our reading group discuss these two papers together."
MachineLearning,"This [one](https://github.com/asappresearch/sru)? Looks like it's easy to use, which is nice!

The paper looks quite good and not hard to read. If I can make it work on a simple problem, I'll be happy."
MachineLearning,"Alright thanks a lot for the clarification, I'm not the author but I recommended them this name actually, I don't know I personally really like it :)"
MachineLearning,"In no way. Calling it Generative Adversarial Transformers was perfect. No other name would have been better. I just don't like GANsformer as a name. 

Really love the research direction though. It's so nice when I can use one architecture for so many things now."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hey, I don't mean to diss on the research. Great work getting your paper out there. The name just struck a nerve around naming schemes in current-day DL research.  Take the comment with a grain of salt."
MachineLearning,thx so much! 😁
MachineLearning,:)
MachineLearning,[deleted]
MachineLearning,Exactly!
MachineLearning,in which way GATran is better than GANsformer..?
MachineLearning,"A lot of people just take the numbers from existing paper. But I feel the better way is to

1. reproduce the paper using their setting, and try to get to as close of a performance as possible.
2. report both what you get and what's in the paper.
3. run your method with the same or close ""essential"" hyper-parameters. ""essential"" hyper-parameters include the dataset size, train/validation/test split, training time, number of FLOPs or number, etc. for all other parameters like the number of layers or channels, you should find those on the validation set.
4. report this ""comparable"" accuracy and (ideally) show improvement.
5. run your method in the unconstrained way. use whatever number of epochs or training time necessary, and maybe even acquiring auxiliary data or using auxiliary supervision, and show the absolute best you can get. 
6. report this ""best"" number, but make sure to note that it is not a ""fair"" comparison."
MachineLearning,Sorry I'm really not sure what's the such big issue with that name...? Guys there's a new research paper here with like 20 pages and all of the discussion about the name only is really weird
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Did you have a GPU with CUDA cores? Re you actually working directly with the CUDA drivers or building tensorflow from scratch or something?
MachineLearning,"I have used exactly your setup as an embedding technique, in order to group similar items next to each other. That worked quite well, however, I don't think you can really decorrelate features in a meaningful sense. PCA is a linear transformation and cannot change many features of the latent space, in particular, it doesn't work optimally if the original distribution is not Gaussian which is not the case by default. An interpolation in the decorrelated space is also not affected by doing PCA as an intermediate step, unless you use k&lt;d components (where d is the dimensionality of your latent space), which shows that the capability of decorrelating features in a meaningful way is not really improved compared with the underlying autoencoder."
MachineLearning,"&gt;My biggest issue with this is that you didn't just call them GATs

GATran would have also been fine. 

GANsformer makes me cringe even when I write it."
MachineLearning,GANsformer is such a shitty name
MachineLearning,"The Git branch model sounds like a really good idea. If it was in a repo that included tools for really strong evaluation and fair comparison to models on the master branch, it would force everyone to use the same implementations and evaluation methodology. Maybe then you could publish a paper and people could have some confidence in the results."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,How do you deploy computer vision applications on the edge?
MachineLearning,"Apologies as, even though I have not read through your tidy working, I'm lazy and would appreciate if you compared with Real Time Recurrent Learning. 

RTRL used to be popular in the early 90s but is these days only occasionally encountered (such as in https://openreview.net/forum?id=q3KSThy2GwB). IIRC, a similar forward mode sensitivity approach was already present in the Hochreiter &amp; Schmidhuber LSTM paper.

The primary disadvantage is how compute intensive it is per time-step, practically infeasible for large networks. There have been quite a few approaches which aim to address this issue, and a quick search turns up this early one by Schmidhuber (1992, https://www.semanticscholar.org/paper/A-Fixed-Size-Storage-O(n3)-Time-Complexity-Learning-Schmidhuber/89b9a181801f32bf62c4237c4265ba036a79f9dc).

 Anyways, it's extremely impressive that you arrived at this largely on your own."
MachineLearning,"Well thats the nature of the industry, specially in non tech domains, you will end up woth bone headed customers that are also simply not interested in „details“ like that. 
Main reason why I do not work anymore in the industry and switched to research.

Trust me if it was so easy that I could solve the problems of data security, processing a.s.o. In a reddit discussion I or one of my colleagues would have come up with a solution after 5-6 years of working in the field.
However I survived there for this time so I suppose I have dome Idea of what I was doing.
Fact of the matter is AI needs to be that field ready in order to be applicable on a broader scale.
Think of computers and how they were a geek expert toy until decent OSs arrived or smartphone UI made them something literally everyone uses as a more placative example.
There is also more to it like intuition of people not from the field aka customers on what we do with the data and how accepting they are regarding common practices, what kind of cases they have. But thats going more off topic"
MachineLearning,"Rejected. Reviewer #1 did not change score post-rebuttal. Eh, shit happens. Life goes on."
MachineLearning,"Glad it made sense!

I am not, but I work a lot with ODEs and have spoken with a couple neural-ODE authors about their work."
MachineLearning,We are already drowning in noise. So... you suggest we add more noise?
MachineLearning,"Even if you look it like that you'd be saying they got lucky in the sense that ""they luckily found a good algorithm"". Even if they had no skill and they just luckily made a good algorithm in the end the algorithm is still good so it'd be worthwhile to publish."
MachineLearning,Why don't people cite Hopfield Networks?
MachineLearning,Docker negates the need to setup cuda btw
MachineLearning,[deleted]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Looking at their code, I can't tell if their ReZero implementation is correct. It doesn't look like it is."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"If someone comes up with a fast/real-time transformer model with applications in astronomy, they better call it *Starscream* or I'll be sorely disappointed."
MachineLearning,"Because there're already GATs [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)  
(Graph Attention Networks and they call it GAT)"
MachineLearning,I used UBIAI : https://ubiai.tools  because they offer team management + auto annotation feautres and that helped us to streamline our annotation process
MachineLearning,Thank you!
MachineLearning,"I also work with graph neural nets in a different small molecule context. You are right in general, graph neural nets picked up in popularity in the last \~2 years. But that's only for small molecule stuff, not proteins. Proteins are still too large for the current state of graoh neural nets. I'd say right now GNNs are great for molecular property prediction. SMILES  + RNNs are still useful for generative modeling although GNNs are on par. And for amino acids RNNs are good baselines if your dataset is limited + not everyone has the capacities to work with BERT models."
MachineLearning,Python is not the issue. CUDA is. It failed to install on my Windows gaming laptop before.
MachineLearning,Checkout SRU++. Not peer reviewed but looks promising.
MachineLearning,"But that's perfect! That's exactly what should happen. 

The alternative is that nothing gets published, and nobody will ever see the new architecture and think wow, what a great idea, simply adjust the weight decay and it'll work. That would be sad."
MachineLearning,"Interesting, read your main comment and it seems to align with my intuition in connection to forward/reverse mode differentiation and neural ODE modeling.  Are you an author of one of these neural ODE papers by any chance :)?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Let's agree to disagree on those ballpark numbers. Comparing your debugging cycle as 'false negatives' to published results as false positives is apples vs oranges. 

But to be clear, ML is a p-hacking leader exactly because we have these standardized tests. A million models are trained on the exact same problem with stochastic optimization routines and one emerges to beat the sota. It is virtually guaranteed that a large portion of that model's success is due to chance. It is hard to think of a better example of (crowd-sourced) p-hacking."
MachineLearning,Shocked Pikachu face
MachineLearning,Thanks for sharing!
MachineLearning,My biggest issue with this is that you didn't just call them GATs
MachineLearning,Thanks for sharing!
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Do try contacting the authors first to see if they're willing to help.

If not, please add the paper and implementation to [https://www.paperswithoutcode.com/](https://www.paperswithoutcode.com/) and we will help you get in touch with the authors. 

Even if they don't respond, you will save a lot of time for future researchers trying to implement the paper. The best case is that you can find more collaborators who are trying to implement the same paper."
MachineLearning,"Yes, i would suggest a class on academic writing to learn how to phrase claims with the proper strength"
MachineLearning,"Strongly depends, sometimes (rarely in my personal experience) this is possible, but more often than not coustomers get reaaaaaaaaly anxious when they hear what plan to do with their ~~daughter~~  data.  
In other scenarios  sufficient quantities of data that is close to the target domain is also not available, specially in non-english scenarios."
MachineLearning,"Tacotron 1 and 2, Wavenet, are all examples of this as well."
MachineLearning,"This is confusing.  You have 1000 timesteps of dimension 7?  This can be fed directly into the RNN.  The first dimension would be the batch size.  so (N, 1000, 7) is the final input, with N = batch size."
MachineLearning,[deleted]
MachineLearning,"&gt; How do we slap some sense into conference reviewers?

We are the conference reviewers..."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"RAM and storage drives degrade faster and aren’t engineered for long and heavy compute cycles. Consumer CPUs aren’t rated for the same workloads as server CPU and may be affected by heat and dirty power to a higher degree than server CPUs. Same with consumer motherboards. Consumer GPUs are meant for gaming first. Workstation glass GPUs are meant for long render or ML workloads, even better are purpose engineered boards for ML applications.

Consumer RAM doesn’t offer ECC and will eventually develop a ton of bad segments if running long term high use workloads like ML. Overnight is trivial, I’m talking multi day/month loads. 

You won’t find an out of box system at your price point that supports machine learning without configuration and installs. If you’re just running overnight processes your workloads aren’t huge. Any consumer gaming or workstation PC/laptop with Nvidia CUDA GPU will suffice, but you will have to configure and install you libraries and tool sets. 

Again, the only vendor I know of selling ready to go workstations is lambda labs https://lambdalabs.com and they’re in the $3300+ range. All others make options in your price range but you aren’t going to get a full suite of ML specific stack installed. Just accept that you’ll have to do the install and configuration yourself. It literally takes 15-30 minutes to install Python and tensor flow and a few decent tools depending on your internet speed. It’s normal Python development work to have to manage environments and packages anyways."
MachineLearning,"That's interesting, because afaik graph networks (Junction tree vae, etc) were more dominant in the chemical sequencing space than rnns/sequential stuff, so curious as to why you're going with the rnn/smiles route"
MachineLearning,"And then I come along and say ""but look, you did not adjust weight decay. Of course it won't work. If you also decrease wd by 0.03, it suddenly would have worked beautifully!""

See how you really can't make a negative result the main thing of an empirical paper?"
MachineLearning,"If you are deadset on not having to build it yourself, and your point of comparison is running on a laptop CPU, you could just buy a pre-built gaming PC. They need top end GPUs to run the most recent games on the best settings. Just make sure it's an nvidia GPU. And since they have deals with companies, you might even be able to get a 30 series GPU build for a reasonable price. You'll need to set up CUDA to use the GPU, but it should suit your needs. Biggest downside in my opinion is that theyll probably put it on a small motherboard that won't leave you room to make improvements in the future."
MachineLearning,I also found this a bit odd. By using vanilla transformer's setting and applying it to all others does bias results unfairly towards vanilla transformer by construction!
MachineLearning,"Yes, this is the key. I've dropped many initially great and promising fancy ideas after tuning the baselines more. In fact, I sometimes tuned the boring baselines so well that they beat the SOTA. For some, I then made this a paper instead..."
MachineLearning,"""CUDA Out of Memory Error"" after 3 hours of hyperparameter tuning without having a way to save the model using servers that have different random number generators so saving seed is pointless outside of a session...I've been there."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Two somewhat interesting topics from the industry:

1. on-line learning when information about outcomes is delayed. How can you use data in the real time to update the model, while knowing that the label might change over the following period of time - and if the label does change, how to efficiently - in on-line way- update your model. 
2. How to enjoy benefits of both on-line single-shot learning and multi-pass (batch) learning using fewest possible resources."
MachineLearning,"Probably because it is really expensive to do.

Only Google can do it, because they are paying employees $$$ to do it. A grad student gets nothing out of such a study, and they have no where near the compute to facilitate it."
MachineLearning,"It is interesting what you say about the decay. My thoughts were at some point, granted maybe not immediately, recurrent neural networks will begin to use incredibly long term dependencies - just as our memories can be from decades ago. Thus, eventually the field will outgrow what truncated-BPTT can provide - therefore this forward-mode AD becomes the viable solution despite it being forward mode. As stated, future testing will tell. :)"
MachineLearning,"Part of the big deal about this generation is that the prices dropped, so this generation costs about the same as the older ones. The tensor cores are also improved to have twice the throughput, so the new ones are better at the same retail price point. Right now, the previous generation is about the same price as the current generation, and also sold out everywhere. I just looked up the 2070 and its retail price is $570, but on Amazon it costs over $1100. To find a GPU that isn't sold out right now, you'd have to go back 2 generations.


Tim Detmers has the best guide I've seen for everything you need to know to pick a GPU for a ML build:

https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/"
MachineLearning,OP begs to differ
MachineLearning,I think most models are constrained by training times than anything else
MachineLearning,"As a result of benchmarking, an increase of one percentage point in accuracy is not considered an improvement.  It's something that could easily be achieved with different initial values."
MachineLearning,"How much performance do I lose in practice if I just go with the previous generation of GPU? I'm doing continual learning, but with a focus on increasing efficiency, not on beating SOTA. I shouldn't need amazing performance. Just something that is ""good enough"". If I notice I need more speed, I can always buy another machine later.

My main problem is really that there is too much choice, and too many things to read up on. I do not want to lose several days just to research my hardware requirements. I want to buy something right now to get started, and then upgrade it later only if that turns out to be necessary. At the same time, I'm afraid of making an amateur's mistake and getting something that is completely non-functional or highly inefficient."
MachineLearning,"Do GPUs degrade faster than CPUs? I have been training my small models over night on my 5 year old macbook so far. It's running purely on the CPU and is terribly slow, but I have it running practically day and night without issue.

I am looking for an intermediate solution. Something in the range of 500-1000$ or so. I expect to replace this with something better later, once my requirements are clearer. I basically want something like: ""click on this link and buy this particular machine. Takes ten minutes to buy, half an hour to configure, and has all the basics you are going to need. It's not going to be great, but it doesn't have anything important missing either."""
MachineLearning,"Ray and Horovod interact at different layers, so a comparison isn't perfect. Ray orchestrates processes while Horovod handles distributed communication. Horovod is for training neural networks. Ray is for general purpose distributed computing, so much broader. You can use Ray to execute Horovod training jobs (and this slowly seems to be becoming the recommended way of doing so).

While I don't know much about Elephas, it seems very much Keras and Spark specific."
MachineLearning,"Hardest part will be getting a GPU (or two) right now. At retail price the Nvidia 3000 series GPUs make ML build much more affordable. You can pretty much copy the cheapest Lambda Labs build for a couple thousand dollars. Depending on your needs, you can go even cheaper. I've been planning to do a ML build for my home PC for a few months. Get a big motherboard that can fit multiple GPUs with decent spacing in between (to avoid heat issues). I'm going to build it with 1 GPU and add a second at a later date.


My build with just 1 GPU is going to be about $1500. Problem is getting the GPU. I've been trying to get any of the 30 series GPUs since November (3090, 3080, 3070, or 3060). I'm on all of the EVGA wait lists, and I'll probably get one by summer at the EARLIEST. You can also buy from a scalper, but they're all selling for close to triple retail price right now, so it's gonna hurt your budget for sure.


And if you do your own build, you'll need to install your OS and software/libraries on your own. It's not too difficult. If you don't want to deal with that, you'll have to buy from someone at a markup. Lambda Labs is probably your best option"
MachineLearning,"I suppose that installing libraries is unavoidable without spending a lot of money, but is there any website where I can just go and buy a ready-made computer with the stuff you mentioned?

I'm not looking for amazing value just for something that is good enough to get me started, and won't feel like a waste of money when I replace it with something better down the line, once my requirements are clearer."
MachineLearning,"&gt;Yeah it's not my area of expertise at all! Do you mean like the ski rental problem/ snoopy caching? I've heard about those in the context of Competitive analysis, but we're requiring me to use brain cells used in 6.046 that haven't fired in years so not even going to even pretend like I know the specifics. Was just trying to provide some general techniques for finding research topics!"
MachineLearning,"That would imply people actually try to reproduce other people’s AI experiments, are you a madman?!"
MachineLearning,"I beg to differ, this is a legit course on Data Science that is currently Free, and is on an important topic around the skills for Data Scientists. Many people have enrolled and gave good feedbacks as they found it helpful. Kindly consider."
MachineLearning,"You're right, but then maybe the paper format is the problem? Maybe it should just be git branches instead, each with just a diagram or two describing the change and the results?

I just don't think it's fair to ever call modifications senseless. 99% of my ideas have not panned out in the past, for reasons I only understood after trying them (or never); same for the ones that did end up working out. Similarly, if you had shown the setup of a GAN or transformer to me on paper, I would have never guessed that they work so well.

In other words, my impression is that ML research has almost nothing to do with talent or skill. We just keep tweaking things, some of us win the lottery with something that works unexpectedly well, and then *later* we come up with explanations for why of course that was a great idea, wow, aren't these authors brilliant and deserving of great fame.

So instead of complaining about spam papers, we should find a way to communicate results such that publishing seemingly insignificant data points doesn't feel like spamming."
MachineLearning,"How cheap is cheap? You need an average cpu, 16gb ram, and a gpu with as much VRAM as you can afford. If you can find one with 12gb that's great even if it's old and slow, it'll get you started.

You will have to install libraries though. You'll install Linux, your code editors, and then install docker. Do everything in docker and your GPU stuff will just work these days.

What stuff will you be working on?"
MachineLearning,"Ok it's starting to make sense, but hang on. Why not use normal (possibly technical) documents for pre-training on the cloud, then use 1 local GPU for fine-tuning it on your sensitive documents? Is that also illegal?"
MachineLearning,"Tuning more would make the situation better, not worse. One key problem is people don't tune the baselines."
MachineLearning,this is spam
MachineLearning,[deleted]
MachineLearning,"Legal situation with data security may prohibit you from using external hardware for training, especially in european countries and when working on fields where the content of the data is highly sensitive.  
So you need an upfront investment in hardware. Besides the higher buy-in there also also ongoing cost in maintenance and upkeep that can pile up if not managed correctly."
MachineLearning,"Yeah, I’m definitely not claiming the paper is amazing, but it seems along the same lines you’re talking about"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I agree, its a multiple of the typical backpropagation time - but it is of the same order as it (linear polynomial time). Previous versions of backprop through time have needed quadratic time to be computed. This is the notion I was trying to convey in a short comment :)"
MachineLearning,"Thanks for your comment! I think you misunderstand my question though. Online Learning is hardly a buzzword, it's an area of Optimization/Learning theory. Instead of learning in offline setting with batch data (like most application of deep learning), Online Optimization tries to minimize the regret over a stream of sequential data (even though you can easily perform Online-to-Batch conversion to use Online Learning algorithm in Batch settings)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I feel like I would agree with you if I can just see precise examples of papers you're referring to since I don't know this area ... (link?)

But also maybe the ""sell"" of the more recent works is different idunno."
MachineLearning,"I don’t know of a machine that is out of the box ready to do GPU accelerated machine learning without having to install and configure libraries that is also affordable. Companies like Lambda Labs offers workstations ready to go, but the lowest priced machine is $3300 for the TensorBook. 

Also, workstation and laptop grade hardware is not built to run 24x7 grinding models. You really should be tasking long heavy processing loads to enterprise grade hardware. Nothing like training an LSTM for 8 days to have your cheap ass consumer grade ram throw one too many faults and take a shit on the whole thing (writing from experience). 

If you’re just trying to play around, buy one of the Nvidia Jetson or Intel Neural Compute Sticks. I have the intel neural stick 2 on a raspberry pi with 8GB ram and it’s pretty neat, and cost less than $200. But, there is installs and configurations to fuddle with. 

Or design your experiments to work within the free tiers of cloud providers. GCP has some decent free offerings I hear."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; ""... can now be computed... This has never been the case before.""

Be careful getting overhyped on this. Those be fightin' words to people very familiar with forward sensitivity analysis of dynamical systems.."
MachineLearning,This statement isn't quite right. You need to consider that the forward-pass now includes one additional recurrence relation for *every parameter*. [See my other comment](https://www.reddit.com/r/MachineLearning/comments/lws9oy/r_exact_and_efficient_iterative_method_for/gpkgseh?utm_source=share&amp;utm_medium=web2x&amp;context=3).
MachineLearning,There is nothing to be sorry about. We are making fun of it because nowdays we see “transformers” attached to any words but it is still better than TransGAN so +1 from my side :) amazing paper btw
MachineLearning,"You know this conversation would go a lot better if you realised that a lot of the people you're talking to have substantial experience in ml and statistics and don't need a YouTube video explainer of the filedraw effect.

Ml doesn't really do p-value hacking. Confidence intervals are almost unused in this field, and datasets have standardised test sets and evaluation criteria that makes it hard to cheat in those specific ways.

The file draw effect is real, but false negatives from incorrect code occur in my personal workflow several times a day. False positives coming from the filedraw effect only comes a few times a month from many thousands of researchers. It's intrinsically rarer."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Has anyone tried alwaysAI?
MachineLearning,"Is the 3060 a decent budget buy for prototyping at home?

I want something with &gt;=11GB of VRAM to train over checkpointed models, and it seemed to be a great deal.

(If I can get my hands on one)"
MachineLearning,"Oh shit, that's great! I just skimmed the repo very quickly and I didn't see finetuning in the readme so I was kinda unsure. Might use this then :)

thanks!"
MachineLearning,"Have my upvote, damn you"
MachineLearning,"&gt; Mind you I am assuming that you are not just a terrible researcher, because those should have been filtered out by the peer review anyway. Remember, if someone gets a negative result their first impulse is not to publish, but to endlessly try and improve.

LOL! What a shockingly naive mindset."
MachineLearning,I see this sort of stuff as an excellent use of Google's resources.
MachineLearning,Very cool. Rooting for you guys.
MachineLearning,"I think what the original comment meant about research in engineering, is that it requires a layer of human implementation on top of theory and therefore it is susceptible to human error. Thus a program may run badly because the theoretical algorithm is bad, or it may be a good algorithm that is correctly translated into code. For any paper with a negative result, readers have to trust that the code is the correct implementation of the algorithm, however if a paper has a positive result, then ""the proof is in the pudding"" since a positive result stands for itself (unless a mistake somehow leads to a better algorithm, but I hope you will agree that is much less likely)."
MachineLearning,"I chair in our field, I see it often - but the argument against splits in Gorman &amp; Bedrick ACL'19 didn't get as much traction with reviewers as it should"
MachineLearning,"Okay, i will consider. 
Thanks"
MachineLearning,Thank you about that and I'm totally happy to receive feedback. Will make sure to make the suggested clarifications in both the paper and the code!
MachineLearning,"The private profit-driven folks are already doing this. There are EMTALA laws in place to protect patients and prevent payor gate keeping but they are paper tigers. I’ve seen a single psychiatrist managing inpatient care on 32 people at one time. Jury is still out on virtual visits though. I think that the sometimes indiscernible lag in virtual loses something in that 93% of non-verbal communication. Understand the costs on a macro level include labor, charting time, admin, logistics, and maybe, categorical diagnostic coding? Looking at differential diagnoses, can be as informative to ML as setting that AXIS I code;  I would love to see how that interacts with QUALITY care planning and different labor metrics like time in chart, staffing, and length of stay, outcomes etc."
MachineLearning,I started cleaning up the code a few days ago but unfortunately haven't finish yet. Some of the code is therefore not clean yet and will get cleaned-up by end of today March 3. see description on the readme of the repository.
MachineLearning,"Honestly, the code is in a pretty impressive state already.   It's just tough to use to it to answer small detail questions.  I hope I didn't give the impression I think the code quality is poor."
MachineLearning,Has anyone tried alwaysAI?
MachineLearning,Thanks for being so open about the release process! Exciting work.
MachineLearning,"[Statistician: Do you ever use statistics? ML researcher: Nope. Never. Statistician: What about when reading a paper? ML: Nope. Never. Statistician: Ok. So if you’re reading an ML paper comparing lots of models, how do you know which one is the best? ML: Bold font.](https://mobile.twitter.com/kdpsinghlab/status/1356806968497864706)"
MachineLearning,[deleted]
MachineLearning,"&gt;It's extremely rare that code works significantly better than it should  by chance. On the other hand, code working worse than it could because I  missed something is a daily event.

You have to be kidding me right now. Look up what p-hacking is, Veritasium did a nice video explainer if that helps. Getting significantly better results by chance account for a large body of the published literature even in fields that try to compensate for it. This is a well known and widely accepted fact. This paper just tries to illustrate that ML-type papers should try harder to account for p-hacking."
MachineLearning,"Sorry about that, is there an issue with the name or some other meaning that I'm missing? Please let me know if so!"
MachineLearning,"Hey u/Best_Mord_Brazil, this looks really cool! Will definitely check it out - thanks for sharing!"
MachineLearning,Code is not clean yet and will get cleaned-up by end of today see description on the readme of the repository..
MachineLearning,[deleted]
MachineLearning,"I think I comunicated the point wrong.  
Cost is not my real problem, it is rather a symptom of an underlying problem, that there is no real effort to push a less trial-and-error-heavy strategy in favour of drowning problems in computational resources, while praying to moores law.

There will be allways more resources to burn, what concerns me is that this is also the most effective way of doing things."
MachineLearning,"Hi
I am for the moment PhD candidate and my research project aims to consider the use of electrophysiological signals to assess attention state of children with ADHD (with the help of ML ;)), i.e. are they focus or not?

However, someting really important that I note is that what I am doing may be helpfull for diagnostics or treatment BUT it will never be consider alone. 

If you want further information about ADHD and ML or the use of ML in the context of electrophysiology, don't hesitate to ask me !"
MachineLearning,"If the outperformance is consistent that cannot be ascribed to chance, that is true. But the same holds for underperformance; if underperformance is consistent, it is not due to poor execution, because by chance most executions will not be poor. 

Mind you I am assuming that you are not just a terrible researcher, because those should have been filtered out by the peer review anyway. Remember, if someone gets a negative result their first impulse is not to publish, but to endlessly try and improve.

The big problem here is what the cut-off should be for consistency. With a hundred thousand people (my guess) working on ML-type problems, getting good results on one dataset does not count as consistent outperformance, due to the p-hacking problem."
MachineLearning,The run\_network.py in the Git repo doesn't even have valid python syntax. So much for running the model out of the box.
MachineLearning,"Hate to shamelessly self-plug, but have you looked at [DebateSum](https://www.aclweb.org/anthology/2020.argmining-1.1/)? It's useful even if you're not doing summarization tasks. The documents are almost all politics / philosophy / social science related. The paper goes into more detail about the specific focuses and context of the data. Might be useful for you!"
MachineLearning,"I guess the question is if this is interesting enough to be a paper on its own.

 It sounds like a good blog post or Twitter thread, or an ablation study that could be part of a larger paper describing a system as a whole.

There's more ways to get things out there than writing stand alone papers."
MachineLearning,"I certainly don't think that. I think in the physical sciences, negative results papers are extremely important as it shows which hypotheses aren't supported by experimental evidence. But in ML, I don't think that allowing negative results papers is a solution for de-noising the field. Then, instead of spamming papers with +0.01% on CIFAR100, they spam even more papers with senseless modifications."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Excess industry funding is a rising tide that lifts all ""research"", such that papers which wouldn't make the cut in less funded fields are still making the cut.

The ML field should be in crisis mode, searching for a new paradigm to push the field forward, but the status quo just makes too much god damn money.

I made a related comment [yesterday](https://www.reddit.com/r/MachineLearning/comments/lvwt3l/d_some_interesting_observations_about_machine/gpf5x5d/)."
MachineLearning,I think you've misunderstood me. I mean you should first decide for yourself which questions you want to answers as this will guide how you should look into your data.
MachineLearning,"Fallacies only matter in highschool debates. Experimental science and engineering aren't about logical certainty, but about evidence that shifts our best guesses of what's going on.

It's extremely rare that code works significantly better than it should by chance. On the other hand, code working worse than it could because I missed something is a daily event.

The related point is it doesn't matter if there's a million different designs that mean that something doesn't work providing there's one good design that makes it with reliably. Intrinsically, a reliable positive is a more useful signal than a bunch of reliable negatives."
MachineLearning,"Or worse than that, it would suck if half of ML papers were just lucky initialization weights (coz papers rarely say how many times they trained a model, so who knows if they cherry picked the best training run)"
MachineLearning,"The point of a negative result paper should be primarily about what you tried and didn't work. Ideally, you release your code and have careful benchmarks of what you tried and exactly how it didn't work.

This way, I can get some intuition about techniques that don't work in specific circumstances and additionally since the ideal paper releases code there is an opportunity to at least try to and figure out if the negative result was due to bugs (human error) or really because the proposed idea doesn't work.

But instead, we are left with almost no papers like this and we find that it's quite difficult to know which trees are not worth barking up."
MachineLearning,"I don't think that is true. If an algorithm/model consistently outperforms others on a domain, there is no way for that to happen via chance (unless it gets ""lucky""  data every single time you run it). However, if an algorithm performs badly it may either because the algorithm is bad or because someone made a mistake in the implementation.

Correct me if I am misunderstanding."
MachineLearning,"That's because you think of papers as a vehicle to show off significant progress and garner prestige and citations. I think of papers as a tool for scientists to communicate. ArXiv uploads are free, so papers shouldn't have to prove anything at all. A 1-pager that says ""I tried X on Y, it didn't do anything"" is a useful data point that will never get cited but will help me save time in my own experiment. Why can't that be the norm?"
MachineLearning,"I understand the goals you are describing.  

I do have a question:  isn't it accurate to say that the method in [https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf](https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf) statistical? 

In my understanding, it is a statistical model that has a prior with potentially favorable attributes.  Is it then fair to say that by your definition, a causal model is ""merely"" a sufficiently good statistical model -- one that happens to search over programs rather than over the parameters of a deep neural network?"
MachineLearning,"This, I think, is a great example of how ML \*could\* help improve mental healthcare: it's a well defined domain, with measurable success metrics that can be optimized on.

My only concern would be wanting to know who is doing the optimizing. A for-profit mental healthcare company (of which there are too many) might be more tempted to drive up profit at the expense of quality of care, which would probably result in overloading therapists with too many patients to see in too short a window of time. 

On the other hand, if improvement and patient outcomes were our focus, then I think this could be a really cool thing."
MachineLearning,You can get help at [dvc.org/chat](https://dvc.org/chat) (:
MachineLearning,"&gt; Not tuning hyperparameters handicapped other
methods. While per-modification tuning might improve results (as verified in section 4.2), we argue that
truly useful improvements to the Transformer should
be reasonably hyperparameter-agnostic. Further, if
hyperparameter sensitivity was the issue, it would
be likely that a least a few of the compared methods
“got lucky” with the hyperparameter settings, but
very few modifications produced a boost.

This is a little rich, given the amount of hparam tuning (explicit and implicit) that goes in in some (but not all) Google papers."
MachineLearning,"In my ""niche"" subfield, no one does it. Maybe it's done in your subfield - but I think my subfield is pretty big."
MachineLearning,"I don't buy that argument. If you're testing a new expression for a transformer's attention, you're just switching out a few lines of code at most. You then run this on a bunch of different kinds of data sets, and you publish a short paper saying ""we tested this new attention on data sets X Y and Z, and it didn't do much"". This should be a 1-page (maybe 2-page) paper. A formal version of a twitter thread, essentially.

If I think there's a detail or hyperparameter that you missed, then I can try that myself, and write a 1-page paper in response. In a matter of two weeks. The only reason people don't like this model is because they're optimizing for prestige and citation count, not for fast scientific progress. And that frustrates me to no end."
MachineLearning,"I also don't think that any of these papers would be interesting as negative results either, since they still make no contribution. SOTA is just the trump card they use to get published anyway. I think the larger moral of the story is that 90% of published papers should've died before the manuscript was written."
MachineLearning,"&gt; (e.g. cross validation, which no one in NLP at least does).

come on, this is incorrect"
MachineLearning,DALEK - Data Augmented Learning for Equity and Karma
MachineLearning,"Ah! Online Learning and Online Optimization just sound like buzz words to me! Ok here's some of the questions I asked myself when I was figuring out my topic: 

1) Are you more interested in doing something applied (IE applying some cool known or slightly tweaked techniques to novel data?) or do you want to do something math-y (IE I made the 3rd convolutional layer of a CNN function with 7 convolutions in triplicate instead of 3 and now it gets 4% more accuracy)? 

2) If applied, what areas interest you? And by online optimization did you just mean learning methods based on some web scraping (IE using google maps to predict things? Using some NASA data to predict things? Trying to find crime via some FBI hot spots Etc.?)? Because then I'd think about what you like, if it's politics, energy, animal science, crime, medicine, etc. And then I'd start to take a look at what's out there. 

3) If you're more interested in theory, I'd try to whittle down what types of modeling you like doing. Do you want to do computer vision? Geographic weighted stuff? Deep learning? GANs-y type of data generation things? Data augmentation things (boosting/ bagging/GANs again/ etc.)? Optimization stuff for current methods (IE reducing the number of parameters using in some random step of a random forest?)? Weird feature extraction techniques? (IE texture analysis for computer vision problems, some weird implementation of Otsu's method, some weird clustering technique, etc.) 

4) From there I'd go digging through the literature. I usually start with some dumb google searches then go down a rabbit hole of journals looking for review articles similar to what I want to do. I'd try to keep the searches somewhat general while you're in exploration phase, then get more and more specific as you get closer to what you want to do. Keep in mind you may find your idea has been tried 100 times and doesn't work, so be prepared to keep digging. 

5) Once you're done with all of that, start compiling info for yourself in an organized fashion (I like to pretend I'm writing the background section of my thesis or a paper and try to categorize accordingly) and then start getting into the actual plan! Consider making some test code, or writing up an outline of your thesis plan. 

Hope that helps - good luck!"
MachineLearning,"I'm fairly skeptical of applications of ML on MH. Part of the issue is, as noted, heterogeneity. Another is fundamental to mh overall: diagnostic criteria for mental illness is not usually observed empirically, but contextually. In healthcare we would like to look at phenotypes or genotypes to determine disease types/trajectories. In reality most of the work, especially in mh, exists in something closer to social determinants of health. This isn't to say there aren't binary values that can be empirically observed to show some mental illness. But our theory just hasn't caught up with the practice yet. 
When you have considerable heterogeneity, vaguely defined classes and a dearth of data (predominantly we want stuff about someone's life and sociodemographics) then I'm hard pressed to see the value of ML. Compared to say computer vision which contains heterogeneity and vaguely defined classes we are saved by abundant data. Mturk and a few thousand dollars will get you a big labelled dataset. That simply doesn't exist in the context and fidelity we would need for meaningful differentiation in a complex environment like mh."
MachineLearning,"&gt; I haven't seen this. Again, maybe you're reading the wrong papers. People usually cite relevant previous work mentioned in the related works section.

Prominent example right now is the ignorance of two phase control in the offline reinforcement learning community. Or zero shot learning, however you want to call it. It's happening right in front of our eyes, a seemingly ""novel"" subfield which is basically just a rebranding and the players chose to just not give a fuck about the work of hundreds of researchers that came before them, preferring to reinvent and rename everything."
MachineLearning,"That's a fallacy, playing off a negative result as bad skill is the inverse of ascribing a positive result to good luck.

 That is, by your argument the positive results should not have been published."
MachineLearning,"Briefly:

1. High sample efficiency. Since so much knowledge is built into the model, fewer samples are required to transfer to new tasks. In the extreme case you don't need any samples at all. Here is an [example](https://science.sciencemag.org/content/358/6368/eaag2612) architecture that reports 50,000x data efficiency improvement over CNNs, without utilizing any meta-learning style pretraining. 

2. Modularity. Factoring data into reusable modules is efficient and enables combinatorial generation. Decompose ""lion"", decompose ""eagle"" -- now you can imagine classes, like ""griffon"", outside your dataset.  Here is another [position paper](https://deepmind.com/research/publications/relational-inductive-biases-deep-learning-and-graph-networks) in this vein.

3. OOD. This falls out from modularity / compositionality.  It also comes from knowing the generative causes of the data.  Consider training a CNN to recognize a video game character from a particular camera viewpoint, say an orthogonal frontal view.  CNN performance will collapse if we render the same character from a different view, in a different environment, or with different lighting conditions.  But a model that knows how the graphics engine works could infer the parametric changes that produced the new render.

Statistical methods can only get you so far.  Aside from the confounding variables mentioned in the OP paper, consider a stark example like the digits of pi, which are statistically indistinguishable from random.  Therefore statistical ML will be unable to extract a meaningful pattern from them, and will be unable to generate the next N digits.  However, a simple deterministic program can generate arbitrarily many digits of pi.  An ideal causal ML model would learn such a program and succeed where statistical ML cannot."
MachineLearning,"Cool. So the reason this is advantageous at all (when forward-mode is usually not used in regular NNs) is that in RNNs, the parameters are shared across time, so we can store just a single summed gradient. However, the downside of forward mode is the requirement to explicitly store jacobians, rather than Jacobian-vector products. This is still an issue for RNNs, right?"
MachineLearning,Or at least start to use real transformers name like optimus prime
MachineLearning,"Pretty much yes.

However, the gradients from previous states of the network are also used and are all represented in the single variable Delta. So its pretty much forward-mode AD but with these extra (stored) gradients too.

To note: the individual gradients are not stored, just this delta value which represents all of them summed."
MachineLearning,Check out my github for more details [https://shreyz-max.github.io/Video-Captioning/](https://shreyz-max.github.io/Video-Captioning/)
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Im pretty sure if computing costs go down people will just demand more of it until there’s another bottleneck (e.g. RAM, data, or power consumption). 

&gt;[The loss scales as a power-law with model size, dataset size, and the amount of compute used for training](https://arxiv.org/abs/2001.08361)"
MachineLearning,"Aka the multiple induction problem (Jensen, 2000) on a large scale"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Am I correct in thinking about this as forward-mode AD for RNNs?
MachineLearning,"Negative results are difficult in engineering though.

If I write a paper saying that I couldn't get X to work, should your conclusion be that X doesn't work, or simply that I'm bad at getting X to work?

 A good negative result paper has to be a tour de force where a huge number of viable design solutions need to tried out and shown to be unworkable"
MachineLearning,"I see the gist of your idea. Still, any kind of implementation would go a long way. Even if it underperforms for small networks. Just release the code and somebody with a bunch of GPUs will run your experiment on a large scale and prove you right. Or wrong.

Consider it my engineering bias, but I don't trust DL math without experiments. I have been bitten pretty bad by contrastive learning papers, where each paper claimed to introduce the *method that generalizes all else.* Each paper claimed to be the new SOTA, they had *theorems* about why their method was the new great thing, with proofs and each sounded totally reasonable. Until someone bothered to experimentally test them and found out [it was all noise.](https://arxiv.org/abs/2003.08505) All of these methods performed just like what we had 20 years old.

I am not saying you are dishonest or anything. I just mean you are making a big claim, and I hope you are right, but without experiments, we don't really know.

Great work either way"
MachineLearning,"well, that's what happens when the main criterion for publication is that you beat some stupid SotA benchmark by 0.01%, and negative results aren't considered interesting. Journal/conference editors made this bed, now we all get to lie in it"
MachineLearning,Probably because it gets outpaced by “novel idea with performance benefits small enough to not be sure if just a better random seed” work
MachineLearning,Thanks for sharing!
MachineLearning,Thanks for sharing!
MachineLearning,"Honestly that sounds like a privacy nightmare. It sounds like this would involve either recording sessions and then sending the recording off to a service that analyzed the recording to make predictions about psychological vulnerabilities OR the patient giving a 3rd party access to all their social media or writings or whatever so it could be analyzed.

Both of these would require the patient to have a *tremendous* amount of faith in whomever was providing the ML service (since I just know that psychologists and social workers won't be learning the ins-and-outs of a state-of-the-art ML algorithm themselves).

Really it just seems like finding a way to squeeze ML into a domain that really doesn't need it. There are tons of problems with American mental healthcare (some of which might even be solvable), but none that can be easily solved with neural networks."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Graph databases will change your freakin' life](https://youtu.be/GekQqFZm7mA) by Ed Finkler.

The speaker was/is associated with https://osmihelp.org/, mental health advocacy for tech workers. I wasn't able to find large datasets on their website, only annual surveys of tech workers. I'm sorry, this resource might not be what you are looking for (mental health care problems framed as ML prediction problems, datasets, etc.)"
MachineLearning,"it seems like the space for papers that are just ""novel thing doesn't work nearly as well if at all"" is still very untapped"
MachineLearning,"IF there's a place for it in mental health, I think it'll be at the point before the patient is aware of their symptoms, kinda like a blood pressure monitor. Maybe your smartwatch will notice you're not moving as much, and the text analyser will pick up the heavy use of self-pronouns (think that's how it worked?) and give you a little ""Bleep! Are you okay?"" so issues can be nipped in the bud."
MachineLearning,"Saw that there's an AI craze in South Korea, can't wait to see your's debut!"
MachineLearning,"Yeah I know what you mean, but it is different community and i did not want to explain superficially"
MachineLearning,[deleted]
MachineLearning,"What this is, is a framework of maths showing that backpropagation can now be computed through recurrent neural networks with the same ease and efficiency as a normal forward-connecting network.  This has *never* been the case before.

The problem is the framework in its current state, might not actually outperform the old approximations on say MNIST accuracies. That doesn't mean its useless - because, for the first time, it has made it computationally feasible to run large recurrent neural networks over many iterations.

Analogous to how normal neural networks were not considered feasible for decades until the original backpropagation framework was created."
MachineLearning,"It’s not whataboutism. It just seems you’re complaining that things need to be cheaper as it would come with a lot of advantages, and I agree, but you can say that about a million things and that’s not really a solution. 

That being said, government funding/subsidizing could bring costs down. However, in the long run what we really need is for the normal market cost of computer power to decrease. People are working on that! 

There’s currently a lot of R&amp;D going into neuromorphic computing and hardware acceleration, with the purpose of doing just that.   Apple’s recent M1 chip is a glimpse of what this may look like in the consumer market. There’s a huge market incentive to bring these costs down, but it may take some time."
MachineLearning,"I think DVC is great 
When I used it, I had a bit of an issue with using notebooks, not just py scripts, since notebooks change, it would trigger DVC to re-run."
MachineLearning,"Think about what questions you want to answer, the goal ""I want to analyze my data"" is too undirected. Python with Pandas will be a good choice when you want to dig in to your data."
MachineLearning,"So I did try looking at the code, but the transformer layer is 300 lines long and has 30 different arguments:

https://github.com/dorarad/gansformer/blob/148f72964219f8ead2621204bc5cfa89200b6879/training/network.py#L461

Suffice to say, it didn't really clear things up for me.  The code does look pretty clean (aside from verboseness) though, so I bet if I spend enough time on it, I'll be able to make sense of it."
MachineLearning,"You should never budget around RSUs and bonuses, ever. That’s like counting your chickens before they hatch. If you aren’t happy with the base comp, you’ll just end up indenting yourself by living above your means expecting a bonus to cover it. That bonus may never come - that’s why it’s a bonus."
MachineLearning,You should be a bit more specific about who everyone is.
MachineLearning,"Hi all,

Just finished my latest project, building an AI art installation at home, generating 100 %  unique artworks on the fly. Tired of an artwork? Just push the button below the screen and another one will be displayed. When the button has been pushed, the old artwork is deleted and can't be retrieved again.

I started out the project to get more familiar with edge computing and sensor integration, but quite fast gained an interest in the art aspect of the installation. By adding a dimension where an artwork you like is just a button-push away from being deleted, it actually makes you enjoy  it more.

I’ve also written an extensive guide if you want to build your own installation: [https://github.com/maxvfischer/DIY-ai-art](https://github.com/maxvfischer/DIY-ai-art)

Setup:

* An Nvidia Jetson Xavier NX was used for all logic, machine learning inference, art kiosk GUI etc.
* A StyleGAN was used to generate the artworks, trained on \~5k images of abstract art.
* A passive infrared sensor (SR602) was integrated with the Jetson to  reduce screen burn-in. When no movement has been detected around the  installation within a pre-defined threshold, the screen shuts off until movement is detected.
* A custom control box was built, encapsulating most of the electronics."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yeah, personally I think the equations are a bit confusing and it would be super helpful if they revised them (although maybe it'll be clearer once we see the code)."
MachineLearning,"I know some companies, such as Crisis Text Line [use it for triage](https://www.vox.com/science-and-health/2018/6/8/17441452/suicide-prevention-anthony-bourdain-crisis-text-line-data-science) over text messages to prioritize the most immediate cases of suicide risk. 

Human intervention will always be required in these situations for the foreseeable future IMO, so triage is about the limit of how far we can push it safely until considerable advances are made in explainability and reliability."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; Few of the architectural modifications produced improvements, a finding that largely contradicted the experiment results presented in the research papers that originally proposed the modifications.

Color me surprised"
MachineLearning,"Interesting! It's a bit difficult to grok what's going on since there's a lot of code, but a couple of pointers to elements I think might be missing from your implementation:

* There should be separate bottom-up and top-down predictions computed for each level, rather than just giving a layer access to the outputs above and below it
* The attention should be local
* Once you've computed the bottom up, top down, and local averages, those should be combined in a weighted mean with the same column state to get its new state
* I think (though the text is kinda ambiguous) that positional encodings should be added to the top-down path only

In general, I think it'll be easier for yourself and those using your code if you build GLOM from scratch, vs. modifying HuggingFace ALBERT, which is a bit complex and confusing."
MachineLearning,"For that I would suggest you to look at conferences like neurips icml... and search for online learning, you will find lot’s of research papers along with relevant literatures they cited, also you will find the current research trends in this topic."
MachineLearning,I don't buy that point. Pick a small dataset and run some benchmarks at least
MachineLearning,"Thanks for explaining all of that!  I think it makes sense, want  to mull it over a bit. Interesting work :)"
MachineLearning,This seems interesting. I look forward to reading this paper
MachineLearning,"Ah yeah, I waa focusing on the ""passing"" of features that the templates match the highest with (max pooling) like a filter. But it might be more intuitive to focus on this translation aspect."
MachineLearning,"True, I tried going down the whole function approximation route as I thought it might detract the average reader from the intuition. Maybe I should find a way to easily incorporate this"
MachineLearning,Hit me up! Would love to talk! Mental health nurse in Silicon Valley!
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I would count causal learning and program learning as the same thing.  This paper has many of the goals mentioned in the OP's paper, like high sample efficiency, out-of-distribution generalization, and modularity."
MachineLearning,"I will, thanks!"
MachineLearning,"Makes sense, but as I read, these solutions should only suggest the therapist if she/he missed something. Things like you said would be obvious, but for not that clear patterns ML might be useful. For e.g. patient giving small signals about suicidal thoughts throughout the session, but they are so small, that therapist miss most of them and gets to the wrong conclusion. ML could give a notification about it, so the therapist could make a final decision."
MachineLearning,Exactly! Chemical sequences are another interesting area where this could apply.
MachineLearning,"It doesn't look like OP built the API specifically with finetuning methods in mind, but it's basically just a convenience wrapper on top of huggingface/transformers, so if you wanted to bring your own model or fine tune one wrapped by a class in OP's library, it wouldn't be hard. You'd just have to get under the hood a tiny bit."
MachineLearning,I feel your pain. Hopefully someone suggests something on your comment.
MachineLearning,"I haven't seen anything simpler beyond the original RNN attention mechanism and attention is all you need; rather the opposite. In any case, I think amino acid sequence (20 letters, a token is 1 letter) data and molecular SMILES strings could be interesting contexts (both something I am working with) for developing methods."
MachineLearning,"Yes, yes, and yes. Transformer models take sequence into account, but there is no recurrence, so they don't suffer from many of the limitations of classic sequence models. RNNs and LSTMs were great ideas back in the day, but times have changed.

Maybe another way to put it is that we need a replacement for RNNs and LSTMs that aren't full blown transformer models."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"How to describe Keslers Construction to an audience with DS experience but no background in this concept?

This weekend I’ll be presenting my teams work in which we use keslers construction for our classification task. Here’s a link which describes Keslers construction well:

https://www.seas.upenn.edu/~cis519/fall2018/assets/lectures/lecture-7/07-LecBoosting-MC.pdf

The problem is that my target audience for this most definitely won’t know what keslers construction is, even if they have a background in machine learning. I had trouble introducing this concept to my team of undergrad students over a long meeting, and I won’t have that much time to talk to a general audience about it, and it’s not the point of our project. 

So my question is: how do I briefly explain the intuition behind this concept without completely losing my audience? Any tips would be appreciated!"
MachineLearning,"aren't sequential much time consuming, and unstable to train or there has been some progress?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Also this theory is showing that recurrent neural networks can now be backpropagated through with the same efficiency as a typical forward-propagating neural network. This has never been the case before. 

So I hope the theoretical idea speaks for itself without results.  Hope this helps :)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thank you.

Ok, so the 'Delta' variables are computed once for each forward pass of the network. There is a Delta-variable for each recurrent loop and every trainable variable. This is much the same as the normal gradient descent, where a gradient w.r.t cost is calculated for every trainable variable. Just now, you need one for each recurrent loop (k) too. So in total  (k+1) times as many gradients as there are for typical backpropagation through a feed forward network. So this would be 3x as many gradients as normal for a single LSTM, 5x for two LSTMs etc.

When backpropagation is performed these (k+1) gradients can just be summed over to recover one final gradient (per trainable variable) as in normal feed-forward backprop. Each of these gradients includes the dependence of each variable for the current forward pass (as normal) and all previously run states of the network too (new). So in effect is the full dependence each variable has ever had on the current cost.

You **do not** need to solve eqn 6. recursively through every previous iteration on every forward pass, you just need to update it once per forward pass (using the previous value of the delta function from the last time you updated it on the last forward pass). This makes it very efficient.

*Summary: eqn 6. needs to be updated once per forward pass. So the additional calculations are only around as much as computing the forward pass itself.*

Eqn. 7 is just the normal, commonly used, backpropagation eqn but now with an added correction term for recurrencies. But yes eqn.7 is computed for each trainable variable - this is no different to the existing backpropagation algorithms.

To conclude you do not need to run P forward runs for every time you calculate delta, so is not computationally costly as you described. It is only calculated for the current forward pass, with the old delta variable as input from the previous pass.

*Hope this explains it ok, feel free to ask any more questions. :)*

(In addition all of these operations are easily calculable using the Jacobian, so is pretty much the same computational cost as just running normal backprop. Just now it works for forward propagating networks AND recurrencies (whereas normal backprop only works for forward propagating) - in effect all possible networks ever! as all architectures can be boiled down to recurrencies and/or forward propagating)"
MachineLearning,Thank you the feedback and for using DVC! The community tremendously helps us in this journey.
MachineLearning,How did it go?
MachineLearning,Thank you for your recommendation! I actually already went through shai shalev-shwartz and orabona's notes so I do have a bit of fundamental knowledge on OL/OCO. What I'm more interested to learn is what kind of research problems that intrigue researchers in the field? Probably like what topics that are underexplored or topics that are well explored but still in desperate need for improvements.
MachineLearning,"I am familiar with this paper.  Does it really count as causal learning?  Seems more like inference in latent variable models with latent variables with particular structure.    More importantly, I bet we could get a simple transformer to meta learn a system that classifies and generates new digits or alphabet as well as this hardcoded systems.

I interpret this paper as a supporting argument against the idea that ""causal learning"" has anything substantive to offer, at least today."
MachineLearning,It is high time the AI research and conference communities became a whole lot more accountable. So this is an excellent intiative.
MachineLearning,"Thanks... also, 90% of everything typed on a phone with fat fungers comes out wrong."
MachineLearning,Looks like the training timesteps and amount of data are the same but GLOM takes half the time.
MachineLearning,"Because if you can model NLP extremely well, you're basically just modeling general intelligence. Natural language is the thought-space for all human concepts, descriptions, understandings, etc. It is harder to teach a computer that."
MachineLearning,"You can start with this book by Elad Hazan 

https://arxiv.org/pdf/1909.05207.pdf"
MachineLearning,"Very cool, thank you for your continued contribution to the community. This is a useful tool for my applied research team."
MachineLearning," I don't know how to take this. It seems to me that Spacial Cohesion between parts and wholes in images is far more apparent than in textual data.
Also why exactly did you train one for 6 minutes and the other for 12?
 Just up voting to see better questions."
MachineLearning,"I'm really having trouble understanding what's going on in section 3.1.2.  Y's keys and values are said to be size P x d, but I don't think P is ever mentioned again.  Is it different from m?  Then it says:

&gt;the keys track the centroids of the attention-based assignments from X to Y

What is an attention-based assignment?  And why do I care about its centroid?  It says the formula is K = ab(Y, X), but K is part of Y.  Does Y on the righthand side just mean V?  The text says it's doing something analogous to k-means, but I don't get it.

I feel like I'm missing something obvious here, does anyone else have a better handle on this?"
MachineLearning,"If they cant understand it, they can and will criticize the lack of clarity."
MachineLearning,"Yes I totally understand that viewpoint. However, this is more of a theoretical idea. In addition, as an individual (actually an undergrad student) it would be impractical for me to do robust testing with my computer setup. Any results I produce would be severe underrepresentation of the idea's potential, simply because I do not have the resources to do any proper testing.

Hope this helps explain my actions."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I am in central, our PhD candidates get 120k+10k+ bonus, the above figure is from cali from some of my friends."
MachineLearning,"This is really interesting! I just took a quick read through, and am wondering a few things:

* My intuition tells me that after expanding the backpropagation gradient expression in this way and reordering terms, what you end up with can be thought of as 'forward-propagation' (more concretely, forward-mode differentiation).   For a concrete example, to solve Eqs. (6) and (7) recursively, one must start with N=0 and work their way to N&gt;0 **forward** in the network.  Am I understanding correctly?
* If this intuition is correct, must one compute Eq (7) for **each** trainable parameter 'i'?  It would seem then that there is certainly a memory advantage to a forward mode approach, but the computational cost would still be high since it would take P forward runs all the way through the network for P tunable parameters, potentially a whole lot.

Sorry if my understanding is not completely correct!"
MachineLearning,"I do research in applied machine learning and data science for mental health informatics. Machine learning models are increasingly being developed and validated for a variety of mental health phenotypes (including suicide, bipolar disorder, and drug overdose). Although many complications like heterogeneity, missing data, and heavy case imbalance limit their adoption in clinical practice, ML can leverage high dimensional electronic health record data for automated risk detection and more informed clinical decision making. If you are curious feel free to read more about our research: https://walshscience.com/"
MachineLearning,"I've seen a lot of these studies, but what is never addressed is the value added. 
If someone comes to a therapist and says: ""Doc, I feel down all the time, nothing brings me joy, and I fantasize about being in a fatal accident,"" what exactly do we get by having an ML model that could look at their Facebook and say: ""beep 85% probability of major depression.""

I don't need an ML algorithm to tell me whether someone is depressed if they can do it themselves. At best it seems redundant, and at worst it could lead to medical gatekeeping (""well, you say your depressed but the machine doesn't reach our 75% probability threshold, so insurance is denying your claim for antidepressants."")

I don't know about you, but that sounds like a terrible future."
MachineLearning,"Hi, thanks for asking. It currently is a laptop cam just pointed through the window at my bird feeder. It would probably work a lot better if it was a separate cam mounted outside!"
MachineLearning,"There are some claims, that with ML help, it is possible to detect depression, anxiety, other more serious cases by analyzing written text (like chat). Also, voice (I think tones) can predict some mental issues. Do you think these are simply pushed by trend or these kinds of predictions could be accurate enough to at least help the therapist not to miss something?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Sorry unrelated but your GitHub dp is so cutee man
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[https://www.reddit.com/r/GPT3/comments/konb0a/openai\_cofounder\_and\_chief\_scientist\_ilya/](https://www.reddit.com/r/GPT3/comments/konb0a/openai_cofounder_and_chief_scientist_ilya/)
MachineLearning,"While you are right that we should be skeptical of ML in mental health care, there is a growing body of research demonstrating the ability of predictive models to leverage the depth and breadth of electronic health record data for clinical risk prediction of suicide, bipolar disorder, and other behavioral health outcomes. A few of these automated risk models are now being implemented in clinical production systems and are being evaluated as clinical decision support tools. Like you mentioned, heterogeneity, sparse features, and particularly the heavy class imbalance involved with rare mental health outcomes (e.g. suicide likely occurs in less than 0.5% of cases in a given patient sample) currently limit the potential of these models. However, ML is an invaluable approach for risk detection that is more accurate and scalable than routine clinician assessment (which is often very poor for mental health).

More fundamentally, any application of ML in this field does necessarily rely on subjectively reported symptoms and questionable diagnostic criteria - but this problem also lies at the center of mental health care in general."
MachineLearning,"Without trying to be overly critical (first papers are a thing! We've all been there), the immediate take away I (and everyone else) will have is ""if this is an improvement over prior methods why don't you have experiments showing this?"""
MachineLearning,"Any ideas for machine learning models that can find if data is missing? I don't want to impute the missing values but find values that should be there but aren't. (It is time series related, for example invoices that are booked monthly, are missing this month)"
MachineLearning,[deleted]
MachineLearning,"I implemented it for textual input:  
[https://github.com/ArneBinder/GlomImpl](https://github.com/ArneBinder/GlomImpl)"
MachineLearning,"The problem with the use-ML-to-determine-the-cause angle (which is currently very hot in clinical neurosciences) is that our tools are so primitive. IMO, despite being flashy and popular, fMRI isn't nearly as useful as powerful or useful as the coverage might lead you think.

The brain is so complex, and so hard to interact with that we're note even in a position to begin to understand what data to look for, and even if we did know what to look for, collecting it in an ethical, nonivasive manner is likely to be impossible with current technology."
MachineLearning,The disappointing truth may be that the variables you have access to are not predictive on conversion. The only way forward may be if you can supplement your data or join it with any other features.
MachineLearning,"&gt; So in these scenarios it would make sense to train and adjust existing models but it is simply not possible, even for a fairly sized company.

Why not? I’m not familiar with this situation, but I’m in NLP. When I google “cost of training BERT” I’m getting $7K, which should be within budget, even with like 20 experimental runs? Did you try training a smaller BERT?

But I agree robustness to varying real-world conditions is a problem. It’s actually my main research interest."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Those are some interesting thoughts. I think too, that we should look more into the cause of the problem, but my guess would be that the whole internet thing is just messing our minds. We are so connected, yet so distant. Especially now with covid-19.

However, to another of your points, depressions that differ in symptoms are subcategorised. Not sure how accurate it is, but the depression affecting sleep, energy is called somatic, and one affecting self-esteem, sadness is cognitive.

These are just for example, I am not experienced in mental health, but what I am trying to say, that maybe we should look deeper into the cause, symptoms, etc. and then maybe we could use ML to detect it or to learn more about it."
MachineLearning,"You can do things during training an autoencoder to get approximate orthogonality in the latent space and an ordering of important dimensions. Here’s one paper, I think you can find some code for orthogonality constraints/regularizers. https://arxiv.org/pdf/1904.01277.pdf"
MachineLearning,That would be awsome
MachineLearning,"That’s a pretty big blanket statement. Is some of that true in some cases? Sure. In my experience ML papers making too many big discoveries in one paper is not a widespread issue. If papers are not concrete and focused its because the authors don’t have a coherent message. It’s just a bad paper and these exist in every field. The fact that some of these get into top venues goes back to the broken conference style peer review system which hopefully won’t hold up much longer.

Sometimes people include too much math for no reason. Most of the time, I would disagree. My problem is, since we don’t have rigorous theory for deep learning yet, some people think theory doesn’t matter anymore. So to hear people say there is too much math is disheartening. Good ML research requires good math. There is no reason to prefer to study well defined mathematical objects empirically, rather than theoretically. Yes some people add fluff to make it look like they did more than they did, but calling for less math is dangerous. Good researchers figure out over time what is necessary, what to put in the paper, and what to put in the supplement.

We can fix all of this if we fix peer review."
MachineLearning,Same...
MachineLearning,"No, everyone just needs to stop coming up with names. 

Please stop marketing your goddamn pre-prints. Let the work speak for itself, like it's supposed to."
MachineLearning,is it possible to finetune these models?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Man they really need to come up with better names
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Also, at the risk of getting overtly political, perhaps we should be trying to figure out what it is about our modern world that is making us all depressed and fixing \*that\* as opposed to trying to leverage new technologies to make a clearly-unhealthy environment bearable."
MachineLearning,"I think we should all be very, very, VERY skeptical of ML approaches to mental healthcare. ML works best when all of the data are standardized  and the solution space (be it a prediction, interpolation, etc) is well formalized. 

Mental health is pretty much the opposite of all of these things. The most recent version of the DSM has moved us away from ""diagnostic categories"" (i.e. ""your symptoms put you in Box A but not Box B"") towards a dimensional model that looks at signs and symptoms. 

There's too much heterogenaity in mental health care for it to be amenable to ML approaches. You can have two people with depression, one of whom never eats, sleeps too much and feels numb, and another of whom has insomnia, overeats, and feels emotionally overwhelmed with suffering. An ML model dealing with that would flip a table."
MachineLearning,"so as you know movie rating only has two columns which is ""review"" that contains the review from viewers about the movie itself. And ""sentiment"" that defines the if the review is ""positive"" or ""negative""."
MachineLearning,"&gt; many other papers eventually cite that work, which if it's a good paper, will happen regardless of how ""viral"" it is.  

Unfortunately far from reality due to the amount of papers and the anonymity of online conferences. The best chance of a good idea being noticed, is a big lab republishing it with more compute under their name. Breakthroughs will be noticed but they are not frequent.

&gt; A garbage paper will go nowhere in academia even if it goes viral on social media.  

Only this is true."
MachineLearning,I recall seeing a video lecture on graph databases. The author was working on data science for mental healthcare. I'll see if I can find it and share.
MachineLearning,"Well, honestly it beats me. I'm not sure how you're classifying movie reviews? As helpful or unhelpful? Or are you classifying the movie based on reviews? I'm not even sure what the problem is. 

But might want to look into naive bayes classification."
MachineLearning,does that mean I can't compare the performance beetween ANN and Bayesian Network?
MachineLearning,"I don't understand your complaint. ""Likes"" and ""retweets"" are irrelevant to the academic process. Having your paper ""go viral"" isn't relevant to the academic process.

When a paper is already accepted, why does it matter how many likes it gets? IMO the important thing is how many other papers eventually cite that work, which if it's a good paper, will happen regardless of how ""viral"" it is. 

A garbage paper will go nowhere in academia even if it goes viral on social media."
MachineLearning,I'm trying to to do a sentiment text classification on movie review dataset with bayesian algorithm.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Really awesome. Thanks for sharing. Will send to a friend who wants to improve their English lol.
MachineLearning,Don't understand what you're trying to do with Bayesian. But it sounds like you're thinking of something else.
MachineLearning,[deleted]
MachineLearning,Why so salty?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,*nods*
MachineLearning,"Thank you, I believe it is just taking arxiv a while to verify my submission as it is my first submission to their database."
MachineLearning,"Sorry, I completely agree. I'll upload the arxiv link as soon as possible and edit it into the original post. I apologize for the inconvenience."
MachineLearning,"If Doccano doesn't support your needs, it's fair. You cannot use it.

But this is an open source tool that is made available to you and the community free of cost. If you found bugs, it would be great if you can document each one and create an issue in GitHub to help the author's to clear them. This is how open source works. We benefit, but also help, even this very little, if possible.

When I have done this, I have found that sometimes it is a bug, yes. In other occasions it is a documentation error. But sometimes it is a misunderstanding. It was not a bug. So clarifying with the author's is the best way. If they don't respond to your issue, that is another important indication to know if an application is being maintained, BTW.

About them being in the cloud, I'm sure that you didn't miss the section in doccano documentation on how to do 1-click install in Amazon AWS, Google GCP, Microsoft's Azure y Heroku. So you got that covered too.

Label Studio has 1-click install with GCP, Azure and Heroku documented here too:
https://github.com/heartexlabs/label-studio

I think Label Studio supports entity-relationships too. Make sure to try their demo page.

But I faced a little more trouble installing Label Studio using Anaconda (locally). The installation was no problem, but the command used to launch it was not the one documented in GitHub. I created and issue and the author replied and helped my on the spot. If you are using Anaconda, I'd recommend you to search closed issues. 

And remember to please report issues to the respective teams in GitHub. We all benefit from this process."
MachineLearning,Cool! Will you make the code/models available at some point?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm probably weaker on number theory than you are, based on your profile. (Pleased to meet you on LinkedIn..)

&gt; integration-based optimization problems

Something like a p-adic proportional-integral-differential controller? Now that's a weird thought. I'm going to have to ponder on that for a long time.

&gt;  if there’s a correspondence between {Qp : p prime} and R

It depends on what correspondences you are talking about. The set of all sequences in Qp that converge is vaguely like R (which isn't surprising if you define R as the set of sequences in Q)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I think this is a binary logistic classification.
All your input will need to be normalized so it is between 0 and one (raining: 1 or 0, time: n/90, possession: 0 or 1 etc...)
And your target Y is a probability of marking a goal between 0 and 1. 
Like this, each training step your model must calculate the probability of a goal to be marked, let's say 0.35, compared to the fact that a goal append or not in reality (0 or 1 so absolute error = 0.35 or 0.65). Your model will have to minimize this difference over training time.
In fact in binary logistic classification, your binary predictions are just the model's output probability : if over 0.5 = 1, else 0"
MachineLearning,Thanks a lot !
MachineLearning,wow
MachineLearning,faico
MachineLearning,Google bayesian
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I don't doubt their egineering prowess, but I haven't seen any papers coming out of their research dept so far (but that may just be me not noticing it)."
MachineLearning,"I've tried doccanno but found many bugs, in addition we are looking for cloud based annotation tool that supports entity relations which doccano doesn't have yet"
MachineLearning,"How do we make a "" processed  neural  network file ""(such as a "".pro"" style file)"
MachineLearning,"This is great - one thing I have been looking for is a way to actually visualise the data in each step. For instance - what exactly does the value vector look like next to the q,k vectors and how do they change with each step? What exactly do the k vectors look like?"
MachineLearning,"Today machine learning community is nothing but alchemist group, they have no clue how it works and reliability of their results, ""they just assume"""
MachineLearning,"Well as I said I have a background in maths including probability and real analysis, so no mate I don't think that was the problem lol."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Schmidhuber is all you need.
MachineLearning,"Tho, i don't know if they use kubernets or not"
MachineLearning,"Yes, complete self-containment is completely unrealistic, you have to make assumptions about what your audience's preliminary knowledge. What I am saying is that these assumptions may or may not hold depending on the set of readers/reviewers that is unknown a priori. So it is easy to accidentally explain too much or not enough."
MachineLearning,"&gt;opensource large model

Eleuther.AI are doing this, i hope they give some insight on their project soon"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Agreed. This also goes on in various application domains, although the recipe is slightly different. At the same time, cross-domain fertilization is actually a very important and leads to major scientific advances. However, I have a hard time seeing how most of the current ML qualifies as research. It's mostly just trial-and-error adaptation of well-known methods."
MachineLearning,"That’s really interesting. One thing that comes to mind is that integration and probability theory extend naturally to the p-adics. Obviously this isn’t particularly interesting for regression, but if there are integration-based optimization problems that you are interested in it should be easy to tackle.

The other thing that this has gotten me wondering is if there’s a correspondence between {**Q**_p : p prime} and **R** the same way that there is between {the algebraic closure of **F**_: p prime} and **C**. If it is, that would give a really solid explanation of how to exploit the patterns as you vary p."
MachineLearning,That it is.  Sorry I was just rereading what you said and realized I misunderstood.  I'll take a look at some Debian kernels and see what's stable and can get me going
MachineLearning,It uses coreboot; your computer probably is UEFI
MachineLearning,I guess I'll give pureos a shot in a vm and see where it takes me lol
MachineLearning,"It honestly doesn’t matter; the only differences between Linux distributions are the package manager, default desktops, and init system. I use PureOS on my laptop, but don’t know if it would work on yours (it has no proprietary software;) Debian would be functionally the same, though, and would definitely work on your computer.  Debian testing would offer the newest kernels in all of the different Debian versions."
MachineLearning,Any recommended kernel?  I figured that Linux would be the way to go but I've been trying to decide which one.  I've been recommended pop and Ubuntu so far
MachineLearning,"Use Linux as the operating system, and download anaconda to manage your python packages related to ML"
MachineLearning,"Ad 1: I have to say that my impression is exactly the opposite. People are _obsessed_ with what they claim to be novelty, much to the detriment of the real science. If you thoroughly test a class of methods of other people, you might be rejected. However, a 'flashy' paper that only improves upon the SOTA because of some 'lucky' hyperparameters has a better chance of getting in.

Novelty depends very much on how you look. Personally, I have no problem if researchers 'rehash' old ideas and dress them up nicely, as long as they are cognisant of this fact and do not try to hide it. I prefer a good, solid execution over a fancy newfangled method with broken parameters any day of the week."
MachineLearning,Sorry my phone glitched. I saw you replied but I cant find it anywhere lol @awakesnake
MachineLearning,"&gt;Someone will eventually apply deep learning to a predictable problem where it should ""just work"", except it won't.

I'd say that we are experiencing that already, but nobody's calling it out.

Isn't it odd that some relatively shallow networks are able to do very well on traditional machine learning problems, a little bit of depth gets you some great results on computer vision but you need a honking enormous monstrous neural network to get some shakey NLP results?

Why is it that a 50,000 words is so much harder to work with than a 100,000 row dataframe, or a million pixel image?"
MachineLearning,"This isn’t even a problem that’s unique to ML. A lot of sciences/engineering require huge R&amp;D budgets. That’s just life - developing cutting edge tech is expensive.

On the bright side, things get cheaper with time!"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Perfect! Keep up with the good work!
MachineLearning,"Funnily enough, I thought I was just going to play around with academic ideas, but I've had very clear feedback from my two supervisors (yup, a change of supervisors and I'm not even 4 weeks in) was that I had to have some practical application to demonstrate on, and a benchmark I needed to beat.

I found this strange, it doesn't sound like ""academia"" if practical applications are the most important concern and trying new research directions is discouraged unless they can be guaranteed of a good outcome."
MachineLearning,"The image seems like Singapore to me, isn't it?"
MachineLearning,https://web.mit.edu/cocosci/Papers/Science-2015-Lake-1332-8.pdf
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,is this based for the regions I was asking about or for california?
MachineLearning,Yeah we ended up finding the space to be more competitive than we thought! In the end we decided we cared more about having many happy users than having a fat bank account:)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,ftfy...... sturgeon's law
MachineLearning,"I'm looking at what happens to machine learning algorithms when you use p-adic distance functions instead of Euclidean ones.

4 weeks into it so far, so not much to show beyond one internal seminar presentation: [https://youtu.be/tbCPOr5FmL0](https://youtu.be/tbCPOr5FmL0)

... but still, on a not-contrived data set, my p-adic linear regressor out-performed everything in sklearn."
MachineLearning,"Tesla has mounds of video. Which can suffice for large amount of data needed for training something this big. 

It would also be interesting to see models which have input from diverse modalities and have a common latent space created from training on really large datasets. The modalities can grow but the latent space keeps getting richer and richer."
MachineLearning,That's funny. I reached your platform last Friday and found your business exactly what I was looking for. By then the price was $1.4/h and I see that you brought it down to $.99 now. Pretty cool! Hope you can maintain the price this low :)
MachineLearning,"For the first question, I suspect a system of transforms might work to accomplish that. 

For the second, you'd need to acquire a dataset large enough to give cadence to such a nassive number of connections. Assuming there was a dataset that large, say for text, I suspect the convergence would top out for error, far before a number in that order of magnitude would be reached. In terms of cost Huang's Law suggests we can hopefully expect similar rates of progress as we saw with traditional computing in the past, except with gpus, so hopefully we will consider models in the billions of connections as small potato chips within just a few years."
MachineLearning,bigger models can attract more customers to their paid API services as no such engineering practices are followed by any companies that are into AI services.
MachineLearning,"WFC collapse is neat, but not really what I’m looking for. It’s obviously a good way to generate believable structures using a set of hand-crafted 3D tiles. But I’m interested in taking in the potential of ML to generate interesting and unexpected results, based on datasets of player-created structures."
MachineLearning,No
MachineLearning,"Have you tried [wave function collapse](https://github.com/mxgmn/WaveFunctionCollapse)? I have not personally used it but it is very applicable to grid structures (compared to continuous spaces).

It may be possible to either implement something similar making use of ML or using deep NN along with it to generate city-like structures."
MachineLearning,"Is GPT-3 awesome cause Bengio et al invented attention, or is it because OpenAI burned 5 million dollars on training it?  Certainly it's both.  But I'd be big loss to ML if big, break through ideas like attention struggle to gain acceptance because they weren't discovered by someone with enough resources to beat SOTA.   

I don't think OpenAI really discovered anything for that 5M that wouldn't have been discovered 2 years later for 2M or 4 years later for 1M (internet scale LMs with huge architectures trained with attention and large context windows learn cool stuff).  

I am not even saying let's get rid of that huge advantage that the few mega research labs/universities have entirely.  Sure, unrestricted, mega budget SOTA chasing is still fine.  But also publish how well your ideas/architecture do when they don't have orders of magnitudes more resources than are feasible for a couple of smart grad students in an random university to gain access to.  Otherwise, good ideas suffer."
MachineLearning,How is it different from Horovod and Elephas?
MachineLearning,Wow good job with the illustrations have you tried the pytorch\`s API for transformers I have tried and felt hard to replicate the from scratch version. Hope you can help 😅
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I don’t know. Nobody thinks “let’s put a cap on airplane research budgets so that poor countries have a chance”. I don’t like it, but sometimes there’s just a price of admission. I suspect the only solution is for Congress to set up a researcher cloud and maybe also buy &amp; maintain datasets."
MachineLearning,I have some work with non linear Fourier transforms that could interest you
MachineLearning,"I work for Google as just a machine learning engineer, not in any research capacity.  Speaking just for myself, I do think the SOTA chasing needs a parallel track where there is some agreed on, relatively meager model training budget, and results are shared contingent on that budget.  EG, this is the best model on this dataset that can be trained for &lt;$1,000.  Otherwise there is just an insurmountable disadvantage to most academics."
MachineLearning,"pix2pix is cool

[https://phillipi.github.io/pix2pix/](https://phillipi.github.io/pix2pix/)"
MachineLearning,"When generating complex structures, a beam search algorithm can help improve the final result significantly. This is most often done for text generation but with a bit of effort I'm sure you can adapt it for your use case."
MachineLearning,"And people wonder, “why aren’t there more women in STEM fields?”

Maybe it is because of posts like this? Could not a woman be an AI dev? Would they not be tired at the end of a day?

#facepalm"
MachineLearning,"There is an interesting term coined as ""Hardware Lottery"". You might want to check this paper out [https://arxiv.org/abs/2009.06489](https://arxiv.org/abs/2009.06489)"
MachineLearning,1
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I am an editor for TKDE and DMKD, and I have published 60 journal papers.

Send me the first page, and I will point you to a good venue."
MachineLearning,"10% is base bonus in every company, the rest is on the performance.

&gt;how to estimate what is reasonable in RSUs or equity?

you will get a better answer from the blind app. My company doesn't pay RSU."
MachineLearning,"Check out NeuralProphet by Facebook, looks promising."
MachineLearning,"“Hey babe what are you thinking about? 😉”

Til death doth us part"
MachineLearning,Got a link? I feel like that ones a risky move that’s gonna get me caught but I’ll do it if no one else answers 😂
MachineLearning,Feel like I saw a romance novel corpus once.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I am not trying to compare vs others more just trying to have a feeling for what I should expect and then what is an acceptable range to haggle in if I want you know?   
I am assuming normal would imply 10-20% bonus and then how to estimate what is reasonable in RSUs or equity?"
MachineLearning,And what should be expected in terms of bonus / RSU?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Wild thought- if you have periodic behavior in your signals try Fourier transforming them first and run your xgboost in the frequency domain 

I don’t know a real answer. Check out what people do on numerai, they have pretty good time series chops"
MachineLearning,"""military style camouflage high heels"" is interesting, Since there is no such thing in the real world."
MachineLearning,"Numpy, scipy, pandas are excellent for data manipulation.
Next you shall look into scikit-learn, an ML toolbox that has most of the things you need. Look for popular tiny datasets (tabular, image, etc), such as Titanic, Iris, Boston Housing dataset, MNIST for starters, experiment and get a feel with exploring them. If you're interested in the visual aspect, add opencv to your arsenal too!"
MachineLearning,"I think it would help me if you could maybe give me a more concrete example? I'm not sure we're talking about the same thing. In particular, could you maybe pull an example that illustrates what you mean by 

&gt;  the only novelty is the problem but not the method proposed by the researchers that purports to solve it.

?"
MachineLearning,"See also Paul Romer's excellent paper on mathyness and economics (and his blog post which links to it [here](https://paulromer.net/mathiness/).

""The style that I am calling mathiness lets academic politics masquerade as science. Like mathematical theory, mathiness uses a mixture of words and symbols, but instead of making tight links, it leaves ample room for slippage between statements in natural versus formal language and between statements with theoretical as opposed to empirical content.""

Things have gotten really bad in ML. Like its hilarious that a really well regarded algorithm claimed to require familiarity with category theory in its paper when they are just constructing graphs from topologies. Thats like saying ""this paper requires familiarity with measure theory as it uses euclidian distance"" or ""this paper requires familiarity with number theory as it uses several numbers and the concept of counting."""
MachineLearning,"  Use phase correlation with the desired shape templates or hough transforms.  Neural networks work by creating their own internal representations.    


[https://scikit-image.org/docs/0.11.x/auto\_examples/plot\_register\_translation.html](https://scikit-image.org/docs/0.11.x/auto_examples/plot_register_translation.html)"
MachineLearning,"I would say this falls under learned optimization in general, which is not just limited to neural networks. Learned black box optimizers fall under this. I haven’t read the literature much but my understanding is that the learned optimization community is quite keen on automating training entirely (I.e., very meta). https://arxiv.org/abs/2009.11243"
MachineLearning,"Glad I could help!

Beware though, the Tiny-ImageNet code is old enough to use Python 2.7, and it doesn't look like I saved the image ids so it might be tough to exactly match the same images as the released version of the dataset."
MachineLearning,Rehashing old ideas is all you need
MachineLearning,"This is what citations are supposed to be used for. Instead, they're used as clout badges, so understandably nobody bothers to read them, but that has the consequence that every idea that matters needs to be presented within the paper itself."
MachineLearning,Not with my code.
MachineLearning,Depending on the subfield of physics I think it’s actually as much or more of the former than the latter (not to say the latter doesn’t have a big effect). Many fundamental physics models work very well and it’s just very difficult to come up with a revolutionary idea that explains the data that isn’t explained. The low-hanging fruit is all gone so you have to either be a genius or dabble in phenomenology.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The ""take well-known results from other fields and rename them"" is my biggest pet-peeve, slightly ahead of ""non-sensical inside-joke paper titles."""
MachineLearning,"in south and midwest, 90k - 120k base would be more realistic"
MachineLearning,Thanks!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,ML paper review by ML agents
MachineLearning,"If I would add, I also found that people in ML tend not to use their own proposed method after proposing it, even though in the paper they claim it beats all the benchmarks. This is kind of revealing to me and makes me feel like a lot (not all) research is just for publishing or going to conferences and not really, genuinely trying to solve a problem."
MachineLearning,Chemist here. This party is going to need a bigger tent. Journal proliferation is also an issue. Surgeon's law is appropriate: 90% of everything is crap.
MachineLearning,"It is very difficult I understand. But in most fields people tend to incrementally build up on their previous work to an important application, sometimes over the span of decades, whereas in machine learning people try do the build up and the application in the same paper. I think this results in what I was seeing.

Imagine if people studying computational neuroscience went from analyzing the action potential of a neuron to neural regeneration in a single paper, math and all. In some way this is what ML people are doing."
MachineLearning,"It is very important, but the problem itself usually already given (in a previous, seminal work). So the authors in my examples are coming up with ways to solve this problem (or even some arbitrarily simplified toy problem of this original problem). And then using whatever they learned about this toy problem, directly solve the original problem.

So the process is kind of like this:

1. Original problem -----&gt;
2. Toyify: solve the problem in an extremely simplified or optimistic setup  -----&gt;
3. method shows that it performs well in toy problem ------&gt;
4. take whatever ""part""/""bit"" that seemed to contribute to the performance (""insert name here"" block/technique/architecture/parameter)  and use that to solve the original problem. Claim that this ""part"" is the cause of the better performance.

What really destroys me and I think is very revealing is that **these researchers will never use their own method after proposing it**."
MachineLearning,"I totally agree and I'm fairly tolerant as this happens in engineering fair a bit as well. But it is when they directly apply the result of these theorems (or even intuition gleaned from these theorem) to their application and then doesn't follow up with a discussion that irks me.

For example, not to name names, but I've just quickly read through a paper which proved some results in low dimension about strongly convex, infinitely differentiable, unconstrained programs for a ""novel"" algorithm, which are the most optimistic setting in optimization. And then the authors directly added momentum to their novel algorithm but did not follow up with another result. Ok... then the authors wrote their algorithm in a pseudo-code, which not only had momentum, but also had mini-batches and stochastic sampling. Ok.... and then they ran an experiment on a deep network where the parameters were bounded in someway to enforce Lipschitzness (which has problem of its own).

So the paper went from strongly convex unconstrained programs to non-convex constrained setting. What destroyed me is that nothing is said. It just said the result was good, which I guess this says something about their proposed method??"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I didn't mean to say that people who do theory which doesn't meet application is not useful or doing something wrong. It is just these mismatch are not usually highlighted in the paper. And this raises several major questions that anyone outside of the field would ask, but surprisingly ML folks do not."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You can always save the post
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,*eats popcorn*
MachineLearning,"I can deal with having math. But I cannot deal with this whole ""pretend there is no mismatch with my application"" or ""actually that math part doesn't inform or provide insight, but I'm not going to mention it in the paper"".

Many paper literally reads like something conjoined together: a graduate student worked on some application, another student worked on a theory, and then they just stitched paper together. The theory doesn't match application, and the application requires operation in the code (like sorting, normalization, concatenation, clipping, noise injection, and random shuffling) that wouldn't work without."
MachineLearning,never keep expectations by seeing others on anything in life. Phd getting around 130k-200k need good software engineering skills as well.
MachineLearning,"Math is ok but you have to use it somehow if you do put some math, especially theorems and the like.

You cannot write an entire paper in the most optimistic setting and then ran a simulation in the least optimistic setting and then do not provide any commentary at this gross mismatch.

It's like they pretend that entire theory part is a nightmare just to get over with. ""It never happened if I don't put any remarks""."
MachineLearning,"yeah, its base."
MachineLearning,"this is not exactly common knowledge I feel. Lots of crazy ranges on reddit and even more disparate when searching glassdoor etc. 

Does compensation vary significantly when considering equity and RSUs? I am assuming your 130-200k is solely base correct?"
MachineLearning,Lol maybe you'll have a market for this: [https://futurism.com/the-byte/solar-farm-bird-massacre-mystery-ai-bird-watcher](https://futurism.com/the-byte/solar-farm-bird-massacre-mystery-ai-bird-watcher)
MachineLearning,Awesome thanks! Very interesting. I’ve studied the effect of water extremes at intensive and extensive margins of agricultural productivity but still a noob
MachineLearning,same as everyone 130-200k
MachineLearning,"&gt; he like will need to find another performance indicator and hopefully a better one.

And people will try to game that new indicator thus rendering it  useless

https://en.wikipedia.org/wiki/Goodhart%27s_law

The solution to this problem is 'very simple' but academy wont do it.

Step 1

Remove the 'prestige' of the elite universities. German universities although not perfect do a better job to be uniform in their reputation, salaries and funding. Not like in the US where people wank over the same 10 top programs.

Step 2

Reduce the number of spots for a PhD, to ensure an academic position for the few people taking those positions, give them nationally uniform salaries and contractual obligations to work as a professor N number of years after graduation. People can work in the industry with BS/MS.


This is not perfect by any means but it would help a lot to decrease the current insanity"
MachineLearning,"Brilliant, exactly what I was looking for, thanks!"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"But if the bar for publication becomes sufficiently high, universities and the like will need to find another performance indicator and hopefully a better one. Nobel prizes do a lot for a university's reputation but because they're exclusive enough, they arent under immense pressure to pump out nobel prize laureates. Plus if the standard of quality is raised for papers published, then the number of papers published would become an actually useful measure."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Dare to name some examples? Because it is very possible that it is your knowledge that is lacking and not the clarity of the papers.
MachineLearning,"This sounds like a hypernetwork:

https://arxiv.org/abs/1609.09106"
MachineLearning,[deleted]
MachineLearning,"What kind of webcam did you use?  Is it mounted outside?  Etc. Basically, wondering what hardware you used in general.  Thanks!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Although I am still collecting weather data to improve my model and dataset, you can inspect the model accuracy evaluation with the preliminary dataset [here.](https://www.hackster.io/kutluhan-aktar/iot-tensorflow-weather-station-predicts-rainfall-intensity-534efe#toc-step-5-3--evaluating-the-model-and-making-predictions-on-the-rainfall-intensity-12)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"That's a naive take because people respond to incentives, you cannot say = ""Guys dont write so many papers and create so many spurious conferences but we will definitely  measure your value based on the number of publications"""
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thank you for this explanation,  I'm still trying to make sense of it but I appreciate the effort"
MachineLearning,"There way to many factors to answer that, location, specific field, impact, connections, luck etc"
MachineLearning,"One of the dirty secrets of academic publishing, if they can't understand it, they can't criticize it. You make your project just complicated enough that it's hard to understand without hours and hours of work. Once reviewer 2 can understand what you did, they believe it's their duty to criticize and reject."
MachineLearning,"no shit. but it's guaranteed to be the CORRECT math to produce the results shown, barring outright fraud."
MachineLearning,"I don’t mean to compare the exclusivity of research positions to that of publications. Rather that a less exclusive research space is not a problem in my view, I would prefer instead that the publication space become more exclusive. I struggle to see a benefit of a field so very exclusive that only 5-10 positions are available in all of Europe! On the other hand, I can see a definite benefit for publication venues to dramatically reduce their acceptance rates so that only 20-30 papers will be released in a year - those papers having been judged to be sufficiently weighty and important. 

What I’m saying is, rather than stemming research at the spout (people) for quality control, filter the water coming out (papers)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Ssssssssshhhhhhhhhhhhhhh!
MachineLearning,"That is literally what is said on my brief and nothing else literally, however after many emails back and forth with my tutor, she said that I was to use 2 algorithms, and compare my results based on those 2 using the same datasets and analyse the results. You have done a fantastic job in explaining it and I appreciate it! You did better than 90% of the other 'explanations' online."
MachineLearning,"[Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832) indeed proposes utilising shorter inputs initially when pre-training transformer models. 

This might be what you're looking for."
MachineLearning,"I'm not sure what you mean by ""math can be wrong"". Do you mean they might report one thing in the paper and do something else in the code? I suppose that's true. But the math still helps as long as it's a reasonable approximation of what they do. 

And I'm not saying math is necessarily useful for getting deeper insight. Just that math helps to communicate the process you are taking."
MachineLearning,"&gt;Someone will eventually apply deep learning to a predictable problem where it should ""just work"", except it won't. If it's a big enough surprise, and it raises enough eyebrows, a crisis emerges.

Will it raise eyebrows, or will it raise shoulders that say ""welp, you probably implemented it wrong"" or ""welp, guess that's just not 'the right method™' for this dataset""?"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hi--the idea of a baseline in ML is to build a simple model first as a baseline for metrics (i.e. accuracy, precision, recall, MAE, etc.). Then, as you try more advanced techniques, you can compare to a known ""baseline"". Also, if you update you data / feature engineering, you can-rerun your baseline to see if you get improvement there vs. other methods.

Some teach to use a ""naive"" baseline, which is what would a model return that was simply guessing.  In a binary classification, that would be flipping a coin for every prediction.  

In my work, I usually use linear regression as the baseline for regression and something like logistic regression (a classifier, regardless of the unfortunate name) for classification as the baseline.  The other advantage of such baselines as they have minimal parameters you need to set. There are various other models that might make sense in particular cases.

For a clustering, I would argue k-means is the baseline! 

Regarding your assignment, the ask ""to apply a model to 2 baselines"" does not make sense to me.  Any chance you could share more of the assignment language?"
MachineLearning,How is the accuracy with a GLM?
MachineLearning,"Karapathy is a bad example because he was doing deep learning before it became a fad and he is a smart person. Plus, people who are earlier than most in any field will always have an advantage. You can't stop that and there is no point being salty about it."
MachineLearning,"But Math can be wrong, or have poor constraints (i.e. OP's 4th point).

Math notation and methods can also vary pretty wildly, making it uncessarily difficult to follow equations.

Code is never ""wrong"". Sure it might be buggy - but if you have some interesting results then the code is almost guarenteed to be useful for deeper insight."
MachineLearning,"A collection of great scraping projects:

https://github.com/ml-tooling/best-of-web-python#web-scraping--crawling

My favorites are scrapy as a lightweight scraping framework, beautifulsoup for working with HTML dom and readability for a heuristic article extractor. Selenium is also great for working with pages that aren't static although I try to avoid that if I can. Chrome is also a good tool for inspecting the HTML structure of pages"
MachineLearning,Commenting so I can review it later. Thanks for your work!
MachineLearning,"The flip side of 4 is the lack of precision. I'm sick of seeing the whole model section as terse as this:

&gt; We stick together a YOLO model with a Transformer in some order and slap batch norm somewhere in there and we might have used some kind of optimizer. We also swapped out vague technique A for vague technique B. Okay now time for results.

Making reproducibility impossible. I'll take formulas any day so long as it helps precisely define the work. (This is of course separate from your complaint about irrelevant theorems.)"
MachineLearning,It's supposed to be a fun project at first...I don't know what else to tell you
MachineLearning,"I just got my feet into ML and AI, just done with Numpy and pandas lib in py. Can someone help further where to go and  provide a little resources too"
MachineLearning,I use [inciteful.xyz](https://inciteful.xyz) it's actually pretty awesome.
MachineLearning,"In statistics, theta is typically used to represent a parameter that needs to be estimated. For a linear regression, these parameters are the intercept and the slope wrt each variable. Theta_0 is the intercept, theta_1 the coefficient of X1, theta_2 the coefficient of X2. Another distinction is that theta is the estimate of the true value (assuming a theoretically linear relationship). The estimated value based on the data is theta_hat.


You might wonder why they use theta for every parameter instead of different characters. It's because once you start dealing with high dimensional inputs (thousands of input features), you wouldn't have enough characters, and the coefficients can be viewed as more of a vector multiplication. You have an input vector of features associated with that data point, and a vector of parameters that weights the input vector to create an output."
MachineLearning,"On the contrary, research positions and faculty positions are harder and more exclusive to get than any conference papers. But the later are a pre-requisite to the former so it creates a huge incentive to publish."
MachineLearning,unpopular opinion: Code is nothing else than equations/math in a computer-friendly form.
MachineLearning,Oh. I've never trained anything with variable batch sizes. So.. I guess that's the issue. Youd have to have your own sampler or something. 'Cause the torch data loader tries to retrieve fixed batch sizes.
MachineLearning,"&gt; Because the batch size is fixed

There is no reason this has to be true, unless you're on hardware that requires this (e.g., TPU)."
MachineLearning,"True. True. But I was reading the transformer evaluation paper from google and most of the models there don't even talk about seq2seq data because unless you have very large input data like paragraph or documents the quadratic thing doesnt affect the performance and training time.  

I'd actually go so far as to say that it's smarter to not use bucketing with what you mentioned! Because the batch size is fixed. And if you try the curriculum learning thing and chose a batch size based on that.. Further down the line where you start encountering bigger sequences it'll most definitely crash your CUDA memory.  

I guess it's smarter to train in a descending way. So bigger length sentences first! But in that case also, it makes no difference down the line as you've already fixed batch size for the training. So, you'll always be restricted to the max. Length buckets you have. And per batch quadratic attention time doesn't have much effect for translation or smaller sentence -&gt; sentence pairing dataset."
MachineLearning,"Well said, I'm also an outsider coming from the field of neuroscience and I noticed there is too much emphasis on accuracy benchmarks which means whoever owns better Hardware and more GPUs, can publish more papers! Also, noticed that the signal processing field is very rich in well-grounded ideas that can go back as far as the 1960s. e.g. auto-regressive methods, recursive least squares, forward linear prediction, Kalman filter...etc. These ideas are either ignored by ML-researchers or simply re-used with some variation (adding non-linearity+SGD) and sold under completely different names without referencing the original concepts (they could be reinventing the wheel in many cases so I'm not accusing)"
MachineLearning,"You still get the quadratic explosion in matrix allocation.  You can fit way more data into each batch if you avoid excess padding, otherwise you are stuck w/ # of examples based on your longest example."
MachineLearning,"Depending on your need for interpretability, you can use a CNN and have it so the feature selection for you. I otherwise have nothing to contribute."
MachineLearning,With RNN'S and torch's pad packed approach it's optimal if the i/p is sorted and there's less padding (reduce recurrent steps) but transformers encode and decode in parallel with no recurrence and have src/target masks which just ignore the padding so I've observed minute overhead with unordered batches and paddings.
MachineLearning,"Yeah, i share your perspective, i think the approach of asking SOTA models , they are useful just in industry level having models that does some tasks like translation..., are important we still miss fundamental concepts toward the  main scientific goals of achieving AGI, and understanding intelligence 👍👍👍"
MachineLearning,"&gt; but I'm case of transformers.. It makes no difference

I'm not sure why you say this?--bucketing is just as helpful with transformers.  You have the same issues with padding."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I think this is one of the reasons why explainability field is growing
MachineLearning,"Justin himself, I'm a big fan! 

Thank you very much for this, I just downloaded the kaggle version of imagenet, I will try to use this code.

(Love the new michigan DL course btw)"
MachineLearning,"I don’t think the problem is with the exclusivity of the positions, but rather with the exclusivity of the venues to which publications are submitted. If 100k people are involved in research then great, we have a better chance of breakthroughs than if 10 people are involved. HOWEVER, alongside this, the filters to publication - conferences, journals - need to uphold strict standards as to what constitutes sufficient progress for a paper. I’d prefer science to be democratized as you say - but with corresponding quality controls."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks!
MachineLearning,"Yup that makes sense. 
Yeah.. Bucketing for RNN'S seems helpful implementation wise, but I'm case of transformers.. It makes no difference.. So my main query was based on what you wrote above! Thanks! :)"
MachineLearning,"https://arxiv.org/abs/2010.15581

I haven't gotten around to reading it, but it seemed interesting."
MachineLearning,"&gt; researchers are pushing through a large amount of known results rehashed and somewhat disguised by the novelty of their problem and these papers are all getting accepted because no one can detect the lack of novelty 

maybe a ML model could detect it. sounds like a good topic for a paper..."
MachineLearning,Thomas Kuhn!
MachineLearning,As lucky as you can get...
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hey, I made Tiny-ImageNet. The code to generate the dataset is here:

https://github.com/jcjohnson/tiny-imagenet

You should be able to use it to generate a 256x256 or 224x224 version with the same synsets and subsampling."
MachineLearning,"If I was Schmidhuber, I would be bitter too."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"To clarify, imagine using a known architecture (e.g. VGG Net) and removing some layers and modifying the input and output layer for your problem. That was the paper."
MachineLearning,"https://www.csail.mit.edu/news/computational-limits-deep-learning#:~:text=A%20new%20project%20led%20by,towards%20techniques%20that%20are%20more

Is this what you're looking for?"
MachineLearning,I think math is a better vehicle of communication than code. It's far more succinct.  Code has a bunch of implementation specific details that aren't necessarily really important.
MachineLearning,"It's not about whether or not there is math/equations. It is about intent. You may genuinely care about your problem, and the math is required to establish a lot of assumptions probably because very few people have actually worked on the area you are working, and it is closely related to Math anyway.

The problem comes when Math is used to obfuscate/complicate a simple technique to seem impressive rather than to make things clearer/ formally defined."
MachineLearning,"LOL, I recognize this account from eons ago. Herr Prof Dr Schmidhuber , nice to see you are still going at it with all the vitality and strength!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The other reality is that science has become more democratic which it is good, but that decreases the average quality of scholarship by a lot. In the earliest XX century there were like 5-10 positions for theoretical physics across Europe,so of course all the guys were veritable geniuses. Now any semi-competent person with the right dedication can get a degree and  can start pushing papers like there is no tomorrow because there is competition for academic posts."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It is shitty science, and transform science into marketing."
MachineLearning,"(1 2 3) strike me as very true. I work in automatic differentiation, and lots of times ML ppl have absolutely no idea what had already been done in the field, so they reinvent stuff from decades ago and claim novelty on it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hi KarlaNour96,

Checkout our new tool [https://ubiai.tools](https://ubiai.tools/) , we offer extensive labeling features at very [accessible price](https://ubiai.tools/Package) .  The tool has the following features:

* Easy to use UI for NER and entity relation extraction (perfect for your use case)
* Multi-format document upload: TXT, CSV (each row corresponds to a doc), JSON (allows you to import and modify already annotated JSON files), PDF, DOC, HTML
* Dictionary/Regex auto-annotation: input a list of words or regex patterns along with their associated entities. The tool will automatically scan the documents and auto-annotate
* ML auto-annotation: Train an NER model to auto-annotate your documents
* Collaboration: Share annotation tasks among team members and evaluate team performance
* Annotation format export: JSON, IOB, IOB chatbot, Amazon Comprehend, Stanford CoreNLP

Just send us an email at [admin@ubiai.tools](mailto:admin@ubiai.tools) and we can discuss what plan is most suitable for your use case.

Good luck with your project!"
MachineLearning,"If you don't mind, could you please dm me the exact title of the paper?"
MachineLearning,"Hi, I recently posted about a new tool for exactly this purpose:

 [\[P\] Connected Papers partners with arXiv and Papers with Code : MachineLearning (reddit.com)](https://www.reddit.com/r/MachineLearning/comments/lcj6rg/p_connected_papers_partners_with_arxiv_and_papers/) 

Good luck with literature reviews!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;plagiarising

I would rather call it memorization"
MachineLearning,Thanks for sharing.
MachineLearning,"The worst part for me was that THE RESEARCHER HIMSELF DIDN'T KNOW WHY HIS METHOD WAS GIVING A BETTER ACCURACY, he just tinkered around a bit with different tensorflow functions and published some trash that was a waste of computational power. The state of AI in the lab I worked in was disgraceful, everybody just wanted to get a paper out that's it. Nobody really understood what they were doing"
MachineLearning,"Ive met those too. They *are* brilliant &amp; have a lot of potential, it’s often just wasted. I think the problem with the old tenured profs is lack of creativity. I know I can go into any field (with data) and ‘harmonize’, so I’m not so stuck in one comfortable position. Then those older profs hire single-minded people like them, and put them under a lot of pressure to publish. I guessed jazz improv and less pressure might help, but every individual is different and it would probably require a multi-decade effort to fix &amp; tune."
MachineLearning,i'd rather work out AI savvy and see how well it goes
MachineLearning,"let me make this easier.

math that clarifies - GOOD

math that obfuscates - BAD  


too many ML papers contain too much of the latter."
MachineLearning,"Not hearing back, Mar. 3"
MachineLearning,"I personally would rather read papers with redundant equations than papers with too much text and too few equations. You can understand east concepts with or without equations, but you can only fully understand complicated concepts with decent math expressions"
MachineLearning,i think the only reasonable hope is that we will be able to design AIs with blind spots that are orthogonal or at lease different from our own. Then we can rely on each other...which I really the most optimistic version of a future with both AI and Humans in it.
MachineLearning,"&gt;Someone will eventually apply deep learning to an obscure, but predictable problem where it should ""just work"". Except it won't. That's when the fun begins

I think you’re confusing methods with hypotheses.

Deep Learning is a method not a physical law. It can’t be “disproven” by a problem where it doesn’t work. We already know loads of problems where Deep Learning gives embarrassingly bad results compared to simpler algorithms like boosted forests. Go check Kaggle for a constantly updating list of those.

Cars didn’t come from people finding a type of road where horses didn’t “just work”. They came from thinking outside the paradigm and trying something brand new as opposed to doing tiny iteration on improving horses. ML research right now is heavily focused on doing the latter, which is what this post talks about."
MachineLearning,"it really is, and it's speculated as being an impetus for our large brains - work in social groups and it's important to consider if someone is telling you stories. also really important if you want to make an honest to god AI that isn't hopelessly naive"
MachineLearning,"Applied, haven't received reply."
MachineLearning,"I am a big fan of Chinese poetry, so Chinese poem generation task in this paper drew my eyes. One big problem of poem generation, also evident in OpenAI's GPT series of models, is plagiarism. And this paper is no exception!

Do they realize their chosen sample is plagiarising? Probably not. I mean, yes, 相见无杂言 但道桑麻长 (Despite prolonged separation, we don't have specific words when we finally meet each other, only discussing about everyday life) is a striking poetry. It is also not written by M6, it is written by Tao Yuanming. I immediately recognized it."
MachineLearning,"It has always been and will always be difficult to make real contribution to science. The people working on neural networks a decade or twenty years ago were not popular at all. They also have to fight and be perseverant to eventually show that it was worth working on the subject. 

There is always room for people to think differently, it is simply harder and uncomfortable."
MachineLearning,Yes but yourr hurting my feelings and i think you need a seminar on goodspeak
MachineLearning,Thanks for sharing!
MachineLearning,Amazing
MachineLearning,Thanks for sharing!
MachineLearning,"&gt;Tbh almost everyone I meet in academia is brilliant, they’re just under a lot of pressure

Lucky lol. I met more old tenured professors stuck in the state of the art of the 90s and new hotshots who are ignorant of basics and focus exclusively on getting published with minimal effort than I care to count. And this was at a top 10 worldwide Uni in CS"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Started a toy implementation in Pytorch, in case anyone wants to expand on it :&gt; [https://github.com/742617000027/GLOM-Pytorch](https://github.com/742617000027/GLOM-Pytorch)"
MachineLearning,qppewtyuu
MachineLearning,"There’s equations and equations. Sure, when they elegantly summarise a complex idea an equation is a great addition to a paper.

We’re talking about another use of equations here - when a simplistic and “hazy” idea gets unnecessary mathematicised to make it appear more complex and precise than it is. The point of this usage is precisely the opposite - get you stuck in esoteric math so that you do not realise the banality of the underlying idea."
MachineLearning,jxxjdnxjdjxppjkwfk
MachineLearning,bxjxdnbdfcjxnxhzdnhxdnhxdby
MachineLearning,"Some of his claims are genuine and he was a worthy contender to share the same turing award....but he is known to be a really whiny and resentful person. Whether that's justified or not, depends on the reader."
MachineLearning,Slander
MachineLearning,"Hi KarlaNour, I built a tool (and company) to solve exactly this problem. [www.humanloop.com](https://www.humanloop.com).   


You can find more about our approach here: [https://humanloop.com/blog/why-you-should-be-using-active-learning/](https://humanloop.com/blog/why-you-should-be-using-active-learning/)

&amp;#x200B;

In short we use active learning to help you label the highest value data whilst training your model at the same time."
MachineLearning,"Statistically about ~40% of CS grad students in US (Tier-1 places surveyed in 2016), were Chinese &amp; Indians. They have a lot of nice, well behaved talented folks. The subpar IT infantry working at software sweatshops is another matter though. (Got a first hand look at one such firm 6-7 years ago in Detroit. No wonder so many want visa rules to be tightened)"
MachineLearning,Thanks for sharing!
MachineLearning,"Hold on while i quote all of russels principia on set theory

*2 thousand pages later*

And as we can see, 1+1=2. Now, to build the computer...."
MachineLearning,Thank you !
MachineLearning,That's good. Thanks you !
MachineLearning,"Yes, for patents and NDAs and the like you have to walk a fine line between reproducability and non-disclosure 

I'll admit i still get nervous dealing with writing up research like that. Most of my funding is military too, lots of green lights to jump through"
MachineLearning,"As a physicist - not a ML person - my take is to use language to build up the relevant equations, then describe with lists the routine implemented by the code, then share the main code blocks in the appendix

I very much agree with the above assertion that all papers should have an explicit mathematical description of the most important quantitative concepts. But not so much that it borders on pedantry, just enough to be precise with the idea and cover the limitations of language"
MachineLearning,"Thanks for mentioning (4). I always thought ""Am I too dumb to read papers?"" but I also had a thought ""I mean still it is super unreadable, unlike my math textbooks where it is clear what is happening""."
MachineLearning,Have worked but that don't warrant this comment. And it's not as bad unless you are a big social media star
MachineLearning,"Seems like regex can still solve your problem. Try regex101 to play with your samples.
An NLP framework would be overkill IMHO"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Well said. At least part of the picture is editors and journal publishing practices. I'm in computational biology (not ML). Whenever I review a paper, I always inform the authors and the editor that ""In my opinion, the manuscript does not offer any novel methodologies and is unsuitable for publication under the journal's guidelines."" I am always ignored. Publishing is a content mill -- they need papers to feed their engine."
MachineLearning,"This already exists to some degree. For example, COLT and JMLR don’t really publish papers without any theory. There are also a lot of very good papers with theory in NeurIPS, ICML, ICLR, etc. I think the volume of applied work is just much higher in general for obvious reasons. Industry + marketing gives a lot of attention to applied research, and everyone wants the cushy ML research job and so they try to publish as many papers as they can in the best places they can.

Another issue is the vastness of the literature. You could spend 95% of your time reading and not be entirely sure that your work is original. With time pressure, people read a lot less than this and so are obviously less sure. This is probably true for a lot of fields, not just ML.

I guess the real problem is that research is hard. Not every paper is going to be groundbreaking. This has been true of every field at every time, even when publication rates were much lower. The incremental work is important, but growing expectations and competition lead people to break the work into smaller and smaller chunks. At some point we are just vomiting every idea we have and doing the minimal amount of work to turn it into a viable product. This combined with conference style review and inexperienced reviewers leads to a lot of noise. 

This isn’t the story for everyone obviously, but it’s a big phenomenon."
MachineLearning,"I have a similar background. I've found these papers often go far in depth proving some esoteric theorems, then gloss over the meat and potatoes of how they actually did a thing.

For example, a paper I recently read applied ""standard encoding of categorical data via a design vector"" or something. What did you actually do to the categorical data? 

Meanwhile they went into great depth on proving a copula-trick regression works for their problem. Like, I think we can assume this has a good chance of working with proper choice of copula but I need to know what they did."
MachineLearning,[deleted]
MachineLearning,"&gt; However, upon close examination, the only novelty is the problem but not the method proposed by the researchers that purports to solve it.

I agree with most of your points, but I think this criticism might be misplaced. Formalizing a task into the language of a cost function/optimization problems is really where most of the art in this field is expressed. The definition of a novel task (not just publishing a dataset, I mean stuff like adversarial learning, style transfer, coreference resolution, graph embedding...) usually has a much bigger impact on moving research forward than describing a new activation function or architectural module."
MachineLearning,nice work!
MachineLearning,His claims are genuine and he has a right to be annoyed at how he wasn't credited for his contributions to the research.
MachineLearning,"Like, what problem are they even trying to solve?   Is it even possible to deduce causality in the traditional sense from observational data?   And does any of this causal stuff have any success stories, or perhaps a SoTA on an interesting task?"
MachineLearning,"Anyone understands the ideas in the paper well enough that they're able to explain them clearly? 

I found that I never really understood what the ""casual ML"" papers are trying to do, and how the solutions they propose are better than very basic things like ""use more data"" or ""use better data augmentation"".   

Would really appreciate if anyone would be willing to clearly summarize the ideas."
MachineLearning,And Boobies!
MachineLearning,Say no to the wall of math!
MachineLearning,"alibaba is a behemoth in this area, fwiw.  They're also a huge player in software eng.  We talk about FAANG here in the west, but alibaba's engineering chops are absolutely on the same level."
MachineLearning,"You should check out [researchhub.com](https://researchhub.com)

It could help if it becomes more widespread"
MachineLearning,Am I the only one thinking that there should be more equations in the paper? Yes most equations can be explained by words but things are just so much clearer to me when I read equations than texts.
MachineLearning,"&gt;At that point, I would opt for paying those involved. 

ML start-up rule #1: you don't find unpaid volunteers to implement your random ideas. Seriously, anyone in ML (and any field) worth their salt would be paid fairly (and quite generously in ML) working a ""normal"" job. 

Hire a consultant."
MachineLearning,"Ok, interesting. Would you be willing to share your code? I live in a quiet area,  so maybe the basic models will work better in my location."
MachineLearning,"A prime factor affecting publication these days is the ability to conduct as many experiments as possible. Something will work, then the paper can worked backwards from that points. It's Edison, not Tesla."
MachineLearning,Schmidhuber's bitterness knows no bounds
MachineLearning,"Schmidhuber did a ridiculous amount of important research that largely went unnoticed until the DL renaissance. A great deal of modern ML is essentially rehashing work from the 80s and 90s, the number of truly novel advancements in architectures/ML design has been minimal."
MachineLearning,"I don’t understand this comment. If there’s a negative outcome, why would there be a positive spin on it, without lying about reality? There is no good coming out of this because it’s a plainly negative side-effect of the culture in ML and theoretical physics research"
MachineLearning,"That is such a ""Meta"" thought! I never thought of it this. Thank you for just putting this out there. It makes so much sense!."
MachineLearning,"The *Wall of Math* prevents rejection of the paper by uninformed reviewers. Because the uninformed reviewer who has not understood the main point of the paper may reject the paper because he doesn't like the idea. But seeing the wall of math, he writes a more cautious ""Weak Accept"" or ""Weak Reject"" decision."
MachineLearning,"I am not pointing to the general ""ethics"" of the paper first approach. It's completely fine and have seen many people do it. I am not very comfortable with this approach as I have seen that it sometimes creates a ""fallacy of sunk cost""."
MachineLearning,"The code can not always be shared though (it happens more often than you can imagine). 
I am with you on this one though - I much prefer papers with code."
MachineLearning,"My university has a decent connection but I'm also working on my personal computer, and the connection is not so good.
Also, for some reason the download speed from the imagenet site was really slow (around 100 kb/s), and now I switched to the kaggle mirror and it's faster.

Anyway, the filtering may take some time as well and I hope the id's of the images in the tiny dataset match the id's of the original imagenet."
MachineLearning,Even with a bad connection (doesn't your university have a good one?) 150gb shouldn't take that long. Or did you refer to something else when you say it takes a lot of time?
MachineLearning,"Very true. With the number of papers on ArXiv is growing every month, Imagine how many conference submissions will be there! There were \~ 6K papers in February on ArXiv and this is a large number. 

Plus Every Method has ""Shiny"" metrics but no-one talks about when it would be brittle. Neural Networks are functions and all good SWE's worth their salt write proper test cases for the functions they make. Generally, with research papers, there no solidly written test-cases to make understanding of the learned function more robust. Ideally, they are not incentivized to do this. Pressures of publishing are making people move towards salesmanship instead of science."
MachineLearning,"It's wild how fast China is moving on this stuff, hats off to them."
MachineLearning,"&gt;And are they necessarily in the wrong?

Well, yes. Imagine if every paper in maths or physics tried to re-derive all the work it was based upon. We wouldn't see a paper less than a thousand pages."
MachineLearning,I guess Word2Vec is a good start. Take a look at the Mikolov (2013) paper.
MachineLearning,"There is no problem with the paper-first approach. In fact, some advocate that it's a good practice (see https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper). As far as there aren't any truly unethical practices, it's totally fine."
MachineLearning,"What about making a clear split between theoretical publications and papers aimed at applications/experiments? Do you think that would be 1) possible, 2) desirable?"
MachineLearning,"It’s all about money and status. Things won’t change, other fields are the same or even worse. Have you read any social science/economics/psychology papers? At least the work in ML is mostly reproducible"
MachineLearning,You are spot on! Thanks for this moment of clarity here :)
MachineLearning,"Omg yes, mandatory jazz improv class for all newly appointed researchers at universities! :D"
MachineLearning,"Pretty cool, but I guess the NN is a little overkill in this situation."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I was literally told by my advisor once that we need to make our ""Solution"" look ""complex"" and ""sophisticated"" so we can write a paper on it. I sometimes feel that I could have gotten better results with heuristics instead of ML but in order to ""publish"" papers, we need to show ""Sophisticated math"" to look smart.

This advisor has a ""Paper-first"" research approach. Meaning a research project starts with the advisor starting to write a paper and then conducting experiments based on how the advisor wants to frame the paper.

I am never doing a Ph.D. after my experiences with academia in this field."
MachineLearning,"I found the money quote:

&gt;The brain surfs reality.

Far out, man.

Seriously though, cool paper.  He has been on this quest (visual parsing with NNs) for a long time."
MachineLearning,"Word2Vec, I think it lays the foundations for modern NLP. It brought the idea of large scale weakly supervised pretraining which ultimately led to bert. It also sparked a revolution across ML to just embed everything as a vector."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you so much for sharing this.
MachineLearning,"Might need to read the original paper then.

This was taught in my course and I still don't understand it hahah"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Nice
MachineLearning,&gt; I don't want to get carried away on this specific detail like engineers are ~~want~~ **wont** to do--
MachineLearning,"Sure, please feel free to DM me. Thanks."
MachineLearning,"Sure, please feel free to DM me. Thanks."
MachineLearning,"Yeah, I get that and it squares up with conversations I had while I was negotiating a couple of offers in recent past. Though still begs the question, why would you want to work at a firm whose stocks you expect to remain/are worthless. At a FinTech startup, I was consulting at, one of the employees exited with selling his stocks to the VC firm- could be a one-off case."
MachineLearning,"Thank you for your comment, I just DM-ed you."
MachineLearning,Thank you for the suggestion. I will definitely consider joining the mentor team once I first made sure I can do this well enough in English :D
MachineLearning,"Thank you for your reply, Kaggle competition is not usually of my interest though. Good luck."
MachineLearning,"Well, these sound like anecdotes supporting generalization, instead of specialization.

[https://www.amazon.com/Range-Generalists-Triumph-Specialized-World/dp/0735214484](https://www.amazon.com/Range-Generalists-Triumph-Specialized-World/dp/0735214484)

I'm a generalist SWE with a bachelor's and a few graduate ML courses (no master's). I like applied research and have picked up a few patents over the years (but no publications). Always made a great salary and had interesting co-workers, although the work is not always super exciting. I know a lot of frustrated PhDs/academics, it never seemed worth it to me, and I don't like pretending that I'm brilliant."
MachineLearning,Thank you for your interest in our self-challenging method. I just DM-ed you. :D
MachineLearning,"Noted! I shall try it out then! :)
Thanks a lot!"
MachineLearning,[deleted]
MachineLearning,"I’m an architect and I love Rhino/Grasshopper for how easy it is to model pretty much anything parametrically and with obvious graphical cues. I think you should try it out, you could build an environment in Grasshopper and shuffle around features like trees/etc randomly, then use Rhino to output high quality renders to use as feedback. I think you could even rig the camera to a theoretical flight path"
MachineLearning,"Check out Gym.

Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from [walking](https://gym.openai.com/envs/Humanoid-v1) to playing games like [Pong](https://gym.openai.com/envs/Pong-ram-v0) or [Pinball](https://gym.openai.com/envs/VideoPinball-ram-v0).

They provide easy to use  environments --&gt; [https://gym.openai.com/envs](https://gym.openai.com/envs)

###"
MachineLearning,How does BN limit the amount you can change on weight on the data side? BN only tries to stabilizes input distribution. The weight can still change whatever it can. I can imagine that a bad input would widely change the weight without clipping.
MachineLearning,Upvote for Hinton!
MachineLearning,"Thanks! Remember, it’s all a performance, so razzle dazzle ‘em and they’ll never suspect how nervous you are."
MachineLearning,"Wow, congratulations! That’ll be me in a year or two lol."
MachineLearning,[removed]
MachineLearning,[removed]
MachineLearning,Thanks! I just had to understanding this whole thing as part of my qualifying exam to become a proper doctoral candidate; a large part of my prep was basically writing book report on Attention Is All You Need.
MachineLearning,"Are ideas really cheap? I see this repeated endlessly without any evidence required. I'd like to see the 2012 notes from everyone who says this on how transformer architectures are obvious, just hard to implement/run."
MachineLearning,[deleted]
MachineLearning,"&gt;  But being able to bullshit on the first page just adds to the irony of how work like this is taken almost like gospel while everyone else is out here eating one another alive in peer review.

As far as I can tell, anyone can submit a paper to arxiv, even one that includes speculation on etymology of words in the notes. I don't get your point. If this had been accepted as a conference paper, sure that would be bad. 

But this is just Hinton putting a conceptual paper on arxiv, which basically anyone can do (if there is at least some substance to it). The paper will of course get more attention due to his name, but I'd say that is only natural - it's not like he hasn't earned his reputation over the years."
MachineLearning,"I’m about halfway through your comment but I saw that no one had replied yet. Just wanted to thank you for such a thorough introduction, really, from one student to the next :)"
MachineLearning,"If anyone is interested, Geoff delivered an hour-long talk on this work (virtually) at an ACM conference in India, about a month ago: [https://www.youtube.com/watch?v=eEXnJOHQ\_Xw&amp;feature=emb\_logo&amp;ab\_channel=ACMIKDD](https://www.youtube.com/watch?v=eEXnJOHQ_Xw&amp;feature=emb_logo&amp;ab_channel=ACMIKDD)"
MachineLearning,"You are right, He will. And it's sad that many others are also not writing about their ideas and opinions the way Hinton is. It's not necessary that those ideas resolve into ""papers"" which are proven. Having a decently documented idea can have other people picking it up and working on it."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,What are your nodes and what are your edges?
MachineLearning,"Hm this is really not true. Several of my students with two to five papers and certainly no double digit h index have gone on to researchy engineering jobs with more than 300k total $ (including bonuses). There are many factors that go into what jobs you get, but a PhD certainly is a big one."
MachineLearning,"I didn't get to read all of that, but just a note - - a good chunk of students that I see in our department go through a sort of lull sometime around year 3. 

I think in the first couple of years students tend to do projects they are given in some way and hand held in some way, and that makes it easier to make progress but also more likely to not be your favorite projects. Then there is this time of searching and difficulty and right now the machine learning landscape is somewhat discouraging because of the pase of everything. But by far your most productive time tends to be the last year or two of your PhD definitely your postdoc. I published more in my second year of postdoc then all the other years combined.

I've seen many many students be lost and scared starting year 4, and have super successful PhD defenses 1 -2 years later."
MachineLearning,"Humans are killers though.
Mostly of other species.
In the trillions per year."
MachineLearning,[deleted]
MachineLearning,Idk lots of people come from running SQL queries and analyzing with Excel so the software skills can be rarer than expected.
MachineLearning,"Getting downvoted but commands like: 
* scp to move data
* du / df to see disk space
* ip to view networking info
* lsblk to see devices
* nvidia-smi 
* ps 

won't list them all but it's nice when people know the commands to solve their own problems."
MachineLearning,This is really relevant and super interesting. Thanks for the reference!
MachineLearning,"Thank you! I will do this then because, indeed, i just use it unmodified. Thanks :)"
MachineLearning,"One answer here is definitely older than 5 and the other is true but doesn’t really get at what makes it amazing. 

So attention, as we think of it, is what you pay attention to, obviously. Without context, that’s meaningless. Attention, human or machine, is entirely dependent on the task. So really, attention is a measure of what you need to look at in order to perform a task, or, what information you need to generate output approximating the label. 

I’ll use an example of attention done on word embeddings as that’s what Vaswani et al do in Attention is All You Need.

Word embeddings take words or chunks of words called subwords in the form of tokens and embed each token in a vector describing its position in token space. What is token space? Well, it’s where all the words are. We only know what words mean based on our use of them in context of other words, therefore the meaning of words is dependent on their relation to other words. Two words that have similar meaning should therefore have similar use. If we arrange all words by how close they are to one another, then every word has a list of tokens that is most similar to itself. “Cat” might have “kitty” or “feline” as its closest word, for example. We can represent this list of tokens ordered by nearest meaning as weight vectors, so a list of tokens (words) becomes a matrix, where each row is a token and each column a representation of the nth nearest word to that token. 

So we know what the words are (tokens) and we know what the words mean (embeddings) but that alone will not tell you what a sentence says. A sentence is more than the sum of its parts; the meaning gleaned by reading a sentence is not solely what every individual word in the sentence means out of context; the meaning of the sentence is IN the context of the words. Put another way, the connections between words are what give language meaning greater than memorizing a Oxford simple dictionary.  Attention helps us learn these connections.

Attention generates three matrices from the embedding matrix, which, remember, is just a list of tokens where each token’s meaning is embedded in a vector. These three matrices are the Query, Key, and Value matrices and understanding these is crucial to understanding the underlying mechanism.

In self-attention, as used in the Transformer, all three matrices represent the input. The Query matrix and Key matrix are multiplied by one another - this can be thought of as the model asking the question, “How does what I am looking at (a query) connect to everything else I am looking at (keys)?” So the vector of every word in the input is multiplied by the vector of every other word, establishing the connection between the two. The output matrix - the context - is then then multiplied the Value matrix. After all, learning the connections between words is also insufficient to understand them. We need to understand the Value of each word as well. So we take our output matrix, which holds the connections between Queries and Keys (or, in other words, the significance of each word in the input relative to every other word in the input) and multiplies it by the matrix representing the meaning of every word individually. 

With some scaling to ensure the numbers all line up nicely no matter the input, we now have our Attention Weights: the connections between each word’s meaning and its significance relative to every other word with respect to the output.  A Softmax operation turns these scores into cumulative probabilities so that all values along a given dimension sum to 1. These are our attention scores. Now the model knows which connections it needs to look at it in the input in order to properly predict its label. 

As the OP says, this in essence means that if every word in the input is a node then attention allows the model to intelligently decide which nodes connect to one another - a neural network that learns how its own connections should connect. 

Attention really is all you need! It’s quite powerful and it is causing a major shift in deep learning paradigms. The attention revolution is here to stay."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; **Abstract**

&gt; This paper is something you are probably not interested in."
MachineLearning,"Well, if anyone earned the right to pontificate, it's Geoff Hinton. At his stage of career, he's making much more of a contribution to ML through conceptually deep ideas such as this one rather than sitting down and coding all of it himself."
MachineLearning,"I was writing a long answer and erased it, because frankly this is all you need: https://jalammar.github.io/illustrated-transformer/"
MachineLearning,"I don't want to get carried away on this specific detail like engineers are want to do---I saw the same result as you; it clearly has its own origin beyond being an abbreviation of ""agglomerate"", but that wasn't my point. My point is that admitted speculation about the origin of a word is the cherry on top of publishing your notes on arXiv.

I've said in other comments that I think ""GLOM"" is fine here; perfectly descriptive. Colloquially it certainly serves the place of ""agglomerate"". But being able to bullshit on the first page just adds to the irony of how work like this is taken almost like gospel while everyone else is out here eating one another alive in peer review."
MachineLearning,"Are you sure it's definitely false? I've just looked at the etymology of glom and I see there are two definitions: one is ""to steal"" and the other is ""to attach"". The former use of the word derives from the old-Scottish ""glaum"" (and perhaps the older Gaelic ""glam"").

For the second definition I cannot find any etymology (maybe my Google-fu is bad). Unless you have a source, I'm still convinced that modern use of glom (especially in CS) may have originated from shortening agglomerate. Furthermore, I think the two definitions are different enough that it isn't clear they have the same history."
MachineLearning,Thanks!
MachineLearning,"I literally listened to Bengio hand wring over dealing with flag planting in alternative publication models during a NeurIPS workshop (he's proposing pre-print, fast review journal, then slow review conference w/ more posters).

Good for you for seeing this in the best possible light, sincerely. My time spent in the field has embittered me somewhat to the supposed altruism of ML's present-day godfathers. I've seen enough pettiness from them to at least require convincing that this isn't an example flag planting. Why not make it a blog post rather than posting it on arXiv *like its the preprint of an article to be or currently under review*."
MachineLearning,it's a neural network except the neurons can decide by themselves how to connect for each input datapoint
MachineLearning,One of the most accomplished researchers sharing their ideas is flag planting? Lol.
MachineLearning,One solution is to simply train it using random permutations for each example. The model will not learn order and you have data augmentation :)
MachineLearning,"Can someone ELI5 transformers ? I know it can’t be dumbed down that much but would appreciate a nutshell explanation

I’m somewhat comfortable with most ML concepts but been out of the loop and busy with work so havnt had time to look into them yet"
MachineLearning,The whole thread below is like a search for the minimum viable paper that does not count as flag planting.
MachineLearning,"I’m an ML engineer at a FAANG doing production and research-level work. Top research labs are nearly equivalent to academic positions. You probably won’t land there. But there’s tons of applied ML research positions that you might like. The field is moving so fast, and industry has so many applications, they need people to bridge that gap. You might consider dropping out. A PhD is either to land a research gig or just for fun. Since you won’t get the former and aren’t having fun, you might as well get a job. But try to relax. You’re in a good spot. The worst that could happen is a very comfortable middle class life."
MachineLearning,He mentions the slang term 'glom together'. Search urban dictionary for 'glom' or just google 'glom slang'. Just because you're not familiar with that usage doesn't make it wrong.
MachineLearning,"I love when I see other people do the backwards "" on Latex mistake due to not using ``."
MachineLearning,Employees cannot refer for the residency. Source: Facebook Recruiter
MachineLearning,"It's not easy, it's relatively clear. It is likely that the paper will have a follow-up with an implementation."
MachineLearning,"For the curious, the Larq zoo has [a nice table of SotA results](https://docs.larq.dev/zoo/) with binary networks on ImageNet."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks!
MachineLearning,"This is part of what's driving me to look in to a PhD. I have a masters in statistics and my coworkers with non-quantitative PhDs just don't trust it. They'd rather consult with ""methods experts"" in their fields on modeling decisions and pass off the grunt work to me."
MachineLearning,"Well in detail there is a difference between virtual shares and equity in germany. You usually don't get real equity as long as the company is not public. But these are details,  I just meant, you should get some kind of participation fo the sucess in a startup.

It depends on many things, but not less than 50k."
MachineLearning,[deleted]
MachineLearning,"If you are training a fully connected neural net, it is treating each pixel of an input image as its own feature. The parameters are just the coefficients associated with each pixel. You could view the hidden layers as feature representations of the original image, and then each of them have weights associated with them as well.

&gt; intuitively it seems that the output would be more sensitive to some parameters than others. 

That is measured by the gradient of the loss function with respect to that weight. How much does the loss change if you change that parameter?

&gt; If this is true, can we leverage this information to prioritize improving our estimate of said parameters?

That's already how these parameters are estimated. The backpropagation step in the training process uses gradient descent based optimizers. Weights that are more sensitive/cause a bigger change in the loss function get bigger updates."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[deleted]
MachineLearning,Ah. I see.
MachineLearning,I was more pointing out the double standard and even giving the folks who want to pretend there isn't a double standard the benefit of the doubt by using their characterization of Schmidhuber which even with that assumption there is a double standard.
MachineLearning,Would you say the same about Photoshop?
MachineLearning,"Ah, I thought you were trying to say that Schmidhuber did the same kind of thing."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,We really need access to the logits so we can train models to account for this.
MachineLearning,totally agree about those factors which makes it less apple to apples. It also makes it clearer that there is a double standard when even without aggravating factors there still appears to be a double standard.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Wouldn't that make the double standard clearer.
MachineLearning,Thanks for sharing. Hardware RAID! Who does that any more?? Good luck with your revised workflow :)
MachineLearning,I didn't know about Larq. Thanks for the tip!
MachineLearning,"yeah I was just going by the post title ""How to represent part-whole hierarchies in a neural network"" which the algorithm that I presented does while still being easily comprehensible and simple."
MachineLearning,"I don't agree that ideas are cheap, especially when they encode important principles. In science, ideas are all we have. This stuff won't pan out immediately but it is good to keep an eye on the important principles.

In deep learning research most ""ideas"" are super incremental in the large scheme of things which is why they don't usually work out."
MachineLearning,"Yeah, lumping together."
MachineLearning,"Quants pay as well as HFT, but maybe you're lumping them together?"
MachineLearning,"if it's easy to test and validate the idea, why would Hinton not do it himself?"
MachineLearning,You can check for implementation of some BNNs with Imagenet weights and results on Larq. It’s on github just Google and you can find it.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I wanted to add a broad comment to clarify what I think is a common misconception. When you're in grad school, you get obsessed over very narrow research topics. This is, in fact, by design when pursuing a PhD or a research focused Masters degree. You like to think of yourself as an expert in that particular area and believe that you can only work on that. Now, here's the reality check: it is \*extremely\* unlikely that your full-time work is in the same specific topic as your prior research. The exceptions include the top industry labs you listed.

In most companies like FAANG, you'll be hired as someone who knows ML irrespective of the domain. You'll have to pick up and learn specifics of the problem you are tasked with. If you spent your PhD working on, say, image classification, then it shouldn't surprise you that at work you may be asked to build models for video generation. Your degree and the research papers you produce are only evidence of your ability to solve problems with novel, creative approaches. It does not correlate with the specific work you'd be doing in a company."
MachineLearning,"If the company is not publicly traded (or doing “private IPOs” like SpaceX), who are you gonna sell your shares to?

For stock options, when will you exercise them? Plenty of IPOs were/are delayed for 5+ years: DoorDash, Palantir, RackSpace, to name a few."
MachineLearning,You can try copying the maximum amount of data into the colab vm. Colab vm usually provides around 100gb to 150gb of space i think. you can see it in the top right corner by hovering your mouse. One way to compress data is to train with your laptop first. Then save the output of some intermediate layer and use this output as a new dataset. This essentially freezes weights of all the layers up to the intermediate layer.
MachineLearning,"Python is probably the best language for working with AI/ML due to the number of libraries it supports (pandas, tensorflow, torch, scipy, numpy, etc). In terms of big data storage, I wouldn't say any of them in particular are that much better for AI/ML but structured DB are way easier to work with."
MachineLearning,"I think the experience at Amazon mentioned here is very team dependent. You may end up on a team that's doing more classical ML (non deep learning) or some basic deep learning (which is good enough for many tasks at scale). Keep in mind though that the big companies have hundreds of people doing some sort of ML which naturally mean there's lots of teams working on cutting edge stuff too. Amazon has its own separate internal ML and CV conferences with lots of submissions; same for other related areas. That should give you a sense of the scale within these companies. Not all teams doing cutting edge work publish papers externally, so this may explain the lack of awareness outside the company. Most of the points above hold for other companies too."
MachineLearning,"What has Schmidhuber proposed that wasn't a complete, working thing?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[CLIP-GLaSS](https://github.com/galatolofederico/clip-glass)
MachineLearning,"I can see how that argument could be used against Schmidhuber nowadays, but in the 90s the need for ideas was far greater and the ability to actually implement them was far smaller given the lack of computational power available back then, not even to speak of the overall software ecosystem that we have available with a single git clone these days."
MachineLearning,Life is too short to spend multiple years doing something you're not good at and don't enjoy. Get out with your Masters and find a good job in industry. Be wary of the sunk cost fallacy.
MachineLearning,"Yes, that's correct.  What I'm really hoping to do is get a starting point for researching ML applications where we only care about precision.  That seems like a large share of applied ML problems, but I'm having trouble finding a name for this specific class of problem to guide the research. 

Any advice?"
MachineLearning,The invitation part is true. Thanks for mentioning that. I had completely forgotten that bit. I guess I am rusty now :)
MachineLearning,"That's pretty pedantic, right? It doesn't affect the overall paper."
MachineLearning,"If you want the most robust and long lasting solution, just buy a X1 carbon &amp; install Debian/Ubuntu. If you need GPU, try moving your data to AWS bucket and use Sagemaker. Its a one time heavy lifting - but trust me! its a gift which will never stop giving. You would have freed your rig from requirements to run ML codes, you have a tank for a PC &amp; it will last you for next 6-8 years.

If you are used to Windows that's fine too. X1 Carbons come with windows 10 pro/home"
MachineLearning,Isn't that the argument used against Schmidhuber. I really wonder if there will be a double standard. Although given the press and discussion even just now. It does seem like a double standard
MachineLearning,I love the novelty of this thing. It shows that AI isn’t a single paradigm or dominated by the big tech NN methods. Keep it up!
MachineLearning,Leaving a reply now so that I remember to come back to this and respond later!
MachineLearning,That's interesting but it looks like its just a way to represent and query a graph with a neural network.  That's vaguely related because he is kind of talking about a scene graph.. but its not what the paper is about really and is actually fairly trivial compared to the scope of the paper.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I imagine that this would pair well with ideas from Partial Information Decomposition - this great paper springs to mind

[https://arxiv.org/abs/2008.09535](https://arxiv.org/abs/2008.09535)"
MachineLearning,"This is exciting to me because it reminds me of talks by people like Jeff Hawkins (and maybe Andy Clark a little bit).  It seems like a really serious effort to apply cognitive science to improve computer vision for things like 2d-to-3d reconstruction and scene understanding.  

If anyone starts a github trying to build this, I hope they will post it in this thread."
MachineLearning,Thanks for posting!
MachineLearning,cool i did that too but a few years ago when I was learning tensorflow using the old horrible api. [https://gregmcclement.blogspot.com/2017/02/hierarchies-in-neural-nets.html](https://gregmcclement.blogspot.com/2017/02/hierarchies-in-neural-nets.html)
MachineLearning,"Yes I am shuffling it. Thanks for your help!

Okay I'll keep that in mind next time"
MachineLearning,"I'm pretty sure this is default behavior for every major package, but make sure you are shuffling your training data if you use StratifiedKFold  - note that each fold it provides is sorted by class. Otherwise when training with minibatches your model will be heavily biased towards the last class.

Also, the mods deleted your thread - there's a thread for quick questions pinned at the top of the subreddit."
MachineLearning,"Realistically, unless you end up at a FAANG for your first job, all the salaries will be in the range of \~5k. I agree with you, job hopping is just why you should prefer skills over money at the beginning. I would  much prefer for my first year of working to learn a lot and become better at my job then have 5k more in the bank."
MachineLearning,Something something Schmidhuber
MachineLearning,Cool. Thanks! So my selection of stratifiedkfold was right. I increased the patience of my early stop and now the model seems to do better. Accuracy increased by 10%
MachineLearning,"No. 'Normal' KFold splits the data into `n` consecutive folds, completely agnostic to the data. 

StratifiedKFold splits each class of the data into `n` folds, then concatenates them, ie

    import numpy as np
    from sklearn.model_selection import KFold, StratifiedKFold

    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [1, 2], [3, 4], [5, 6], [7, 8]])
    y = np.array([0, 0, 0, 1, 0, 1, 1, 1])

    kf = KFold(n_splits=2)
    kf.get_n_splits(X, y)
    for train_index, test_index in kf.split(X, y):
        y_train, y_test = y[train_index], y[test_index]
        print(""Train indices:"", train_index, ""Test indices:"", test_index)
        print('Train labels:', y_train, 'Test labels: ', y_test)    

&gt; Train indices: [4 5 6 7] Test indices: [0 1 2 3]

&gt; Train labels: [0 1 1 1] Test labels:  [0 0 0 1]

&gt; Train indices: [0 1 2 3] Test indices: [4 5 6 7]

&gt; Train labels: [0 0 0 1] Test labels:  [0 1 1 1]


    
    skf = StratifiedKFold(n_splits=2)
    skf.get_n_splits(X, y)
    for train_index, test_index in skf.split(X, y):
        y_train, y_test = y[train_index], y[test_index]
        print(""Train indices:"", train_index, ""Test indices:"", test_index)
        print('Train labels:', y_train, 'Test labels: ', y_test)

&gt; Train indices: [2 4 6 7] Test indices: [0 1 3 5]

&gt; Train labels: [0 0 1 1] Test labels:  [0 0 1 1]

&gt; Train indices: [0 1 3 5] Test indices: [2 4 6 7]

&gt; Train labels: [0 0 1 1] Test labels:  [0 0 1 1]"
MachineLearning,"Title:Multi-GPU SNN Simulation with Perfect Static Load Balancing  

Authors:[Dennis Bautembach](https://arxiv.org/search/cs?searchtype=author&amp;query=Bautembach%2C+D), [Iason Oikonomidis](https://arxiv.org/search/cs?searchtype=author&amp;query=Oikonomidis%2C+I), [Antonis Argyros](https://arxiv.org/search/cs?searchtype=author&amp;query=Argyros%2C+A)  

&gt; Abstract: We present a SNN simulator which scales to millions of neurons, billions of synapses, and 8 GPUs. This is made possible by 1) a novel, cache- aware spike transmission algorithm 2) a model parallel multi-GPU distribution scheme and 3) a static, yet very effective load balancing strategy. The simulator further features an easy to use API and the ability to create custom models. We compare the proposed simulator against two state of the art ones on a series of benchmarks using three well-established models. We find that our simulator is faster, consumes less memory, and scales linearly with the number of GPUs.  

[PDF Link](https://arxiv.org/pdf/2102.04681) | [Landing Page](https://arxiv.org/abs/2102.04681) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.04681/)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Regardless of whether or not it's actually a good idea, I do find Geoff Hinton's writing style really enjoyable. He does a good job building up the idea and contextualizing it through all sorts of different lenses. Even if the model he describes doesn't end up very technically feasible, you can walk away from the paper with ideas that are inspired by the line of reasoning he takes"
MachineLearning,"[The Curious Case of Neural Text Degeneration](https://openreview.net/forum?id=rygGQyrFvH)

Not a ground-breaking paper in any way, but well written and explains the ""looping problem"" that all autoregressive generative models have to deal with."
MachineLearning,Currently my plan
MachineLearning,"Title:Towards Confident Machine Reading Comprehension  

Authors:[Rishav Chakravarti](https://arxiv.org/search/cs?searchtype=author&amp;query=Chakravarti%2C+R), [Avirup Sil](https://arxiv.org/search/cs?searchtype=author&amp;query=Sil%2C+A)  

&gt; Abstract: There has been considerable progress on academic benchmarks for the Reading Comprehension (RC) task with State-of-the-Art models closing the gap with human performance on extractive question answering. Datasets such as SQuAD 2.0 &amp; NQ have also introduced an auxiliary task requiring models to predict when a question has no answer in the text. However, in production settings, it is also necessary to provide confidence estimates for the performance of the underlying RC model at both answer extraction and ""answerability"" detection. We propose a novel post-prediction confidence estimation model, which we call Mr.C (short for Mr. Confident), that can be trained to improve a system's ability to refrain from making incorrect predictions with improvements of up to 4 points as measured by Area Under the Curve (AUC) scores. Mr.C can benefit from a novel white-box feature that leverages the underlying RC model's gradients. Performance prediction is particularly important in cases of domain shift (as measured by training RC models on SQUAD 2.0 and evaluating on NQ), where Mr.C not only improves AUC, but also traditional answerability prediction (as measured by a 5 point improvement in F1).  

[PDF Link](https://arxiv.org/pdf/2101.07942) | [Landing Page](https://arxiv.org/abs/2101.07942) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2101.07942/)"
MachineLearning,"Statistically speaking (and this is an ml sub after all), your first salary is very important for your lifetime earnings. You generally raise your salary by job hopping and if you start too low you need to do more of that."
MachineLearning,RBMs were pioneered by Paul Smolensky; Hinton's contribution was a fast learning algorithm for the weights of the RBM.
MachineLearning,"I mean also I think people on reddit just aren't exactly the best barometers for compensation because subreddit populations are diluted in terms of talent (I mean there's 1.7 million people in this sub...).

Both here and cscareerquestions routinely underestimate comp at the ""best"" companies, probably because they don't work at them. A lot of the rest of this thread is saying stuff like 300k is a pipe dream or you'll only get like 125-150k but it really isn't, especially for a PhD coming out of a top program. Chances are if you're at a top program, you're gonna be in demand. A lot of bachelor SWEs make way more than that first year (again using facebook return offers as a reference, TC for the top 5% performing interns is something like 260k first year, 195k recurring)."
MachineLearning,"Your first job salary is not really that important in the long run. The most important thing in your first job is mentorship and learning. If you feel like this is the right place, you should stay there.

If you are concerned about the salary, you can always start to look right now for other job opportunities, which anyways I recommend you to do, just to see what's laying around. This way, when they come up with an offer, you can always negotiate and you have an idea of what you are worth."
MachineLearning,"&gt;Could you convert your efforts in making some positive return on investment from bets?

The only way to have a positive return would be by making better models than the ones they use to create the bets.

The bet contains the probability of victory for one team by the model used to predict it + the margin of error to guarantee a profit

For soccer/football, it's usually hard to know which team is going to win in a big competition. For tennis it's much easier.

But it requires a massive amount of data and it also requires some specific data, like knowing if a player has recently been injured etc. Some people are paid to make the best models so that the company creating the bet never loose money, if you don't put as much effort as what they do it's a bad idea to try to outsmart them"
MachineLearning,"Very informative, great examples (for A/B comparison)."
MachineLearning,"Lol ya know, I was considering coming back and making this change too."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Okay. Does normal KFold balance the data out in each split?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I am currently aiming for a 1.X average grade for the master.

I am also hoping to not get stuck in the comfort zone but respecting that the startup would like to see me in more areas than only ML I think that this would be not the case.

It is not a early stage startup, it hast grown a lot in the past 2 years. (People and Moneywise) 

What is the difference between equity and shares?

How much do you think should I expect as a salary?"
MachineLearning,"In that case it seems like there's a problem with your code, preprocessing, or model choice. Try testing the precision and recall to make sure your model isn't largely/only predicting one class on each fold.

Also try shuffling the dataset if you aren't already."
MachineLearning,"Ideas are cheap, whereas hyperparameter searches on architectures until they boast SOTA results are expensive."
MachineLearning,Yeah it's fun to imagine what the reviews for this paper would look like if it went through double blind peer review.
MachineLearning,"The dataset isn't imbalanced, the imbalance is in the splits generated by kfold. Stratifiedkfold was supposed to help here."
MachineLearning,"Thanks! I'll check all of this again. I was using early stopping before, I increased the patience and it improved the accuracy a lot. But I'll check this out too"
MachineLearning,"Are you happy with that choice now?  


I am a 29 year old that has been doing software engineering for about 7 years professionally now. I've moved to a senior position, and have been involved with machine learning for my product, but at the end of the day I am mostly building data pipelines and model results delivery for customers, and tools to monitor/manage and support that product. It is OK, but the end product isn't something I care about that much.

I love seeing where this industry is headed and spend my weekends working through the tutorials for pytorch and trying to apply the techniques to other problems. I've been doing that for about 5 years, but I feel like the AI revolution is happening and I am close enough to see it happening but not actually part of it.

I want to go back to get my Phd and move into being an active member of it, working on cutting edge techniques to do things with computers we've never done before, rather than the somewhat boring work I've been doing.

But I have no idea how to approach going back to get a Phd after being in the industry for seven years. Where would I get my referrals? I don't even know which schools or programs to apply to? 

&amp;#x200B;

I'm curious how you managed that transition, and if you are happy with the choice now."
MachineLearning,"Thanks, from my experience I can confirm your statement. 
Unfortunately it doesn't make sense why law/sales/management &gt; engineering/R&amp;D 🤦‍♂️
There wouldn't be anything to regulate/sell/manage if there would be no engineering and R&amp;D 🤦‍♂️.
I hope the realisation will come sone, also related to the our behaviour as mankind. 
Thank you for your replay"
MachineLearning,"I sent out some applications (to AI/ML PhD programs whose deadlines hadn’t yet closed) in January. Almost every single moment, the problems u/lostphdstudenthelp came to mind,  but I still submitted them. I know it will be tough but I really need the opportunity"
MachineLearning,"I scanned, and although it's not a working system, I think an experienced developer could design an architecture based on it. It's fairly detailed on the components. Most of the paper clarifies the idea, since it is similar to previous ones."
MachineLearning," HFT &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; big bank &gt; maybe insurance &gt; old school finance firm &gt; medium and small bank &gt; credit union 

fixed"
MachineLearning,"How imbalanced is your dataset overall? For random sampling with a small or highly imbalanced dataset you could have widely different class distributions in each fold, which would give you the kind of accuracy fluctuations you saw.

If your data is highly unbalanced you may need to try an oversampling strategy, and you should probably be considering a different metric as well."
MachineLearning,he did it again
MachineLearning,why is that? what about other stock options?
MachineLearning,"Yeah, I appreciate the disclaimer! That being said, I stopped reading once I saw that."
MachineLearning,"This was posted here yesterday. It only works for classification tasks with predefined classes.

You could probably do something similar for generation tasks if you have a validation/training set with &gt;50k words where you recalibrate the 1-gram frequencies to match your domain. So you can still get away with adding a small number of parameters (50k, maybe 100k), but it's more like 1000-shot than 3-shot."
MachineLearning,"This was 100% my reasoning for going back for a PhD. I made very good money as an engineer, but I did not feel like I had the flexibility to do work that I wanted to focus on."
MachineLearning,"Seems like you're a working student there, if so, unfortunately that doesn't count as work experience, though you should still consider it a plus. 60k sounds ok if you work for a big company, but for a startup it's probably too much. I would be surprised if they pay you that amount. Well, it depends a bit on your background, grades and previous work.

Don't make the mistake of just staying there because it's easy. Get a few other offers and try to compare them. Also, think about where you want to work. Do you like the position, are you interested in startups, do you like the idea of building up something from the ground? If not, than go somewhere else.

If you stay there, you should get shares or equity. I would never join a early-stage startup without shares as a full-time employee. The risk is quite high and you give up a recognized name and a higher salary."
MachineLearning,[deleted]
MachineLearning,"Would you be so kind to leave a hint, related to the question: if the concentration of bad emplyers is higher in the east then in the west? 
I would love to know the opinion of others, who have experience related to it. Thank you very much 👍"
MachineLearning,"Out of curiosity, what do you consider basic bash commands?"
MachineLearning,"I have this theory: all professions eventually end up equally intolerable.

If there exists some ""good"" career path out there, it will draw competition, and the job market will respond by raising barriers to entry, reducing pay, shitting on work-life balance &amp; working conditions, enveloping it in process &amp; regulation &amp; bureaucracy, or some combination.

The only good jobs are either cutting-edge new, hyper-competitive, extremely specialized niches.

DS &amp; ML have been hot for over a decade now. Deep Learning became a buzzword in 2014. In 2015, a flood of bright-eyed CS students started ML PhDs, inspired by the new hype. In 2020, they started to finish their dissertations and enter the job market.

So now I, who entered this field in 2011 with a bachelor's in Linguistics and a referral from a friend who's brother worked in marketing at Google, get to sigh over a smorgasbord of applicants to an L3 ML Engineer position and morosely shake my head at all the PhDs, dolefully inquiring to my coworkers, ""But do they have experience deploying to production?""

Shit's fucked, ya'll."
MachineLearning,"Stratified KFold is great for imbalanced classes. Ie when you have lots of data for some classes and very little for others. Maybe look into your data to see if this is true.

I guess we are in troubleshooting zone, there are various issues to look into on your accuracy: 

• Data : missingness, transformation or any feature engineering 
• Code: any typos, or unintended bugs  
• Model: selection or model type, hyperparameter adjustments 
• Metrics: look into others that may be more suited to your problem? Recall? Sensitivity? AUC 

Failing all above, consider retraining from scratch on another model or choosing another approach. 

2 cents. Hope it gives some ideas on where to check."
MachineLearning,"Pre IPO company RSUs are worth $0. They’re not included in total comp numbers. 

Even employees at Stripe, Waymo, Robinhood, etc consider it $0"
MachineLearning,StyleGANv2 is very nicely presented.
MachineLearning,"float16 4-dim arrays. Wavelet-transformed multi-channel EEG data: `(batches, channels, features, timesteps)`. Though I may merge channels &amp; features.

Things worked fine with dataset half as large, though despite GPUs being vastly superior my laptop was still faster purely due to data load overhead."
MachineLearning,no for this position
MachineLearning,"I think you are more on the right track than I previously was. I essentially added together the individual possibilities per cell and didn’t consider the different combinations. I think the proper way to calculate it is (number of possible values per cell)^(number of cells). For instance, if I had a 3-D vector that each dimension could take one of 100 values, then the number of possible configurations is 100x100x100=100^3. 

So the comparisons should really be 256^(3x256x256)=2^(3x2^19) for the input space versus 8192^(32x32)=2^(13x2^10) for the latent space. From these calculations it appears the difference is very drastic between the two."
MachineLearning,I don't want to be glib but ideas are cheap. Most ideas don't pan out but maybe this the piece of gold he's been panning for.
MachineLearning,"It's a laptop, put the new SSD at old's spot. Thing is the old's general benchmark was 3.4GB/s and new's was 3.5GB/s, yet old read arrays faster. Old was 970 Pro, new is 970 Evo Plus, so it's basically as close as two different SSDs should get (so I'm mistaken on old being rated 3.4). 

Only thing that comes to mind is I benched the old one about a year back, so maybe CPU wore off... still I ask in case this is unusual behavior."
MachineLearning,"I'm a simple man, I see Hinton, I upvote. His RBMs got me interested in ML long ago."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Funny how unpopular this idea is - but I think it's the most insightful in this thread! :)
MachineLearning,"Latent Dirichlet Allocation. It's simple, intuitive, it doesn't need labeled data, it's interpretable, and it just works."
MachineLearning,"I'll chime in about Guild AI (I'm the developer) with some benefits it can offer in your case:

\- Async job scheduling, which lets you stage and run jobs continue working on your project without impacting in-process work

\- Built in grid search,  random search, and Bayesian based hyperparameter optimization - use this with job scheduling to tee up a set of experiments to run over night

\- Tight integration with TensorBoard and other visualization tools like HiPlot from Facebook, Dask dashboard, nbdime for Notebooks diffing, etc.

\- Simple to install - no databases or services, which are costly to maintain operationally, esp for individuals

\- API free integration - with Guild you don't modify your code

\- 100% open source with no platform/commercial biases

Other mature experiment tracking tools to look at include MLflow, Weights and Biases (wandb), and Neptune. Guild takes a different approach as it's designed as an external tool+services rather than an API+services. Guild separates the concerns of running experiments from the code itself. This may sound subtle but it lets you track experiments right away without having to modify your code or install a complex system. It keeps your code independent of any particular tool so others can run it freely.

That said, the other tools' APIs are quite simple and elegant and the modifications to your core are minimal. Guild achieves API-free integration at the cost of added configuration, so there's no free lunch here. The benefit of Guild's approach is that you can run your code independently of the tooling - but if everyone you work with is using the same experiment tracking scheme, this is not a problem."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"As someone who's coming from industry and is now pursuing a Ph.D. - I figured I'd chime in. First off it's TOTALLY normal to be completely lost and feel like research sucks while doing a Ph.D.! If you're concerned about the lack of publications see if you can take on some applied tasks (maybe find some partners in fields you're interested in - do you like healthcare? policy work? environmental work? google AI stuff? computer vision? open to everything as long as someone needs an ML guy?) I've often found that there are tons of random researchers that need help with a stats project or have too much data and don't know what to do with it. Asking around your university and randomly emailing people might net you an interesting project that could turn into an actual thesis project if you like it!

1. You don't. On my team the hiring managers don't even care if you have any papers, they're just impressed you have a paper.
2. All sorts of stuff - sometimes you're building an interesting model, sometimes you're watching the soul drain from your body as you make a powerpoint and clean a database.
3. If you want to be a consultant - just graduate and send out your resume. The bigger consulting firms will devour you and turn you into a highly billable powerpoint/ excel monkey with a six figure salary. 
4. Everything is relevant and nothing is. It's really ""type"" of industry dependent. 
   1. If you go to the federal gov't, some of the tasks might be figuring out how to deal with a database from the 1950s. They might know they want some stats or AI, but you might have to help them figure out what they can do with their data...
   2. If you go to a consulting firm, you might have some clearly defined projects but a lot of work will be helping projects assess what kind of data science they can do and how they can set a project up. You might have image processing, maybe a simple logistic model, maybe some NLP stuff, who knows! 
   3.  If you affiliated with a healthcare or insurance system you'll likely be doing a lot of simple stats work with established code that certain physicians/ insurers/ etc. are super attached to and don't want to change. They'll be amazed that you can change a variable and fix a problem they've been having for years.
   4. For me I found the FAANG work was more of a cog in the wheel kind of thing, here we're making a self driving car, we need you to handle the SQL side of things -&gt; write these queries that integrate with Joe's bs over here. 
   5. If you go into a start-up you might be doing a lot of coding/ business development/ running around trying to figure out who wants to buy your stuff. 
   6. If you go into a non profit you might be doing a lot of very feel-goody type of work, but will find you make very little money. 
   7. If you go into a think tank you'll likely end up doing a lot of interesting applied research - but it might not be a good fit if you don't like coming up with ideas!
   8. If you go into finance you'll find you make a lot of money, but spend a lot of hours at your desk to do so. Like... a lot... My friend at some of the trading firms in Chicago repeatedly had trouble making 5 minutes to go get pizza because she was always watching that ticker...
   9. My random two cents if you're still reading all of this - check out cyber security or AWS certifications. We're really big right now on both of those - and they're not too bad, just familiarize yourself with some vulnerability scanning frameworks or how to work with netflow data and cyber gets a lot easier. For AWS figure out how to set up an AWS server and bam, you can be your company's new ""cloud architect!"" 
5. Every ""low-time commitment"" job I've ever seen advertised has been secretly super time commitment heavy! But maybe that's just my experience. In general I've found federal consulting to be a pretty chill pace of things and as long as you're not on a classified project, it's often 100% remote. 
6. Research is exactly what you've experienced - you sit in a room often by yourself and try to come up with ideas while being frustrated that 99% of them don't work. Industry is a mix of soul-sucking and interesting, where you're either \[1\] wasting an entire month of your life maintaining a database and trying to figure out why the dude on the 3rd floor keeps sending you screenshots you have to hand enter instead of copy/ pasting the text into a dang email, or sitting in an hour long meeting where the guy that never does anything continually talks over you to reiterate he's not sure what he's talking about, and could you CC him on that email to Mary? Or \[2\] Discovering if you implement a fuzzy match algorithm you could merge two address databases and save the operations dept and sales team $200k/ year in what was previously wasted manual labor hours. Or building a predictive model that can help find instances of fraud in some insurance database. Or making pretty network graphs with bro data to help track down some cyber security thing!"
MachineLearning,"You don't have to re-explain everything no. It's not like EfficientNet and convolutions but more like EfficientNet and Linear bottleneck. But even if people would like a complete explanation you just need to refer them to the original source.

If you changed it, you can re-explain a little bit. But if you just used it, you can say ""we use self attention \[5\]"", and \[5\] points to the reference for attention is all you need"
MachineLearning,Thanks - that was my gut feeling too.
MachineLearning,honestly no
MachineLearning,"My company bought one after considering other options like those offered by Microsystems or HPE. The nice part about it is that it just sort of works. It comes with all of nvidia’s software-based bells and whistles without having to go through the headache of configuring them yourself. We also keep ours in a data center so cooling, noise, etc. isn’t a concern. Buying into nvidia’s ecosystem also gets you better direct support from nvidia which can be a plus. The machine is definitely expensive and I see why others say that it’s overpriced. The trade off comes down to time vs. money, not compute vs. money. There are features that you aren’t likely to use if you’re a researcher, like the insane amount of bandwidth that those mellanox cards can handle.

Bottom line: you’ll get more bang for your buck in terms of compute if you go with another vendor. Buy the nvidia box if you want the support and want every ounce of performance that those GPUs can offer (assuming you configure your DNN to do the same)."
MachineLearning,Disk space varies in colab sometimes you get more sometimes you get less. I think it depends on how much resources google has at that time. 200 GB is alot of data to train on colab + colab pro limits usage up to 24 hours only. What kinda of data are you dealing with?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"At that price in a DC, it sounds like a great deal. 

Mostly model size and testing new architectures that are memory intensive. You can solve this with e.g. gradient accumulation and splitting the model over several GPUs, but it can often require a lot of engineering.

The 8x40GB is the sanest option, especially if you are a few people using it. Although the 80GB GPUs will give you a niche edge that most labs don't have access to. However, with larger models, you need more training time and data to make it worth it. If you are running a few bigger experiments it's interesting, but if you are doing several experiments per week, then the 8x40GB is better."
MachineLearning,In Berlin startups I would gauge a realistic salary somewhere between 55-70k. I know of salaries that are higher but those are PhD+ / senior / consulting positions.
MachineLearning,"I prefer [Aim](https://aimstack.io/), a very flexible one.
I use it to group multiple experiments by their hyperparameters and show the min/max/average/median performance of the group. You can [easily notice](https://raw.githubusercontent.com/YerevaNN/parasite/master/graphs.png) which hyperparameter choice affects most."
MachineLearning,"My motherboard only has one full speed NVME slot. The other one runs slower. Maybe that's part of it in your case? You're definitely right up against PCI-E 3.0's bandwidth.

Also advertised peak speed is usually the best-case scenario for the drives, and different makes may use different ways of measuring their peak to make themselves look faster (seeing as your 3.5 GB/s is apparently slightly slower than your 3.4 GB/s). Whichever new controller and hardware inside your new drive may not work as well."
MachineLearning,"Hi, I am not from the same field but if you do not mind, my advice is would be to answer to your self the following questions:
- Do you like it there? (This means: do you find the work interesting and fulfilling? Do you like the colleges?)
- Are the working condition good? 

Related to salary, it depends if it is in the east part or west part. 
If east: start with a salary which is 1500€/week work hour. So if you intend to work 40h/week, the hole (brutto) salary for a year should be 60000€/Year. (this is because you already have proven to them that you can do your stuff) and start high because they most probably will try to bargain with you. That is a thing you want them to do. If the say yes without bargain, then after a year get more. 
If it is the west and you already have the experience put 10000-20000€ more per year. 

From personal experience in a similar complex field (software development for machines) and related to East Germany I can tell you, it is not that easy to find a good employer. Could be a east Germany thing, but I haven't worked in west German jet."
MachineLearning,"Thanks, I follow most of these things. I use high level libraries like Huggingface's transformers and use VS Code extensively (including it's remote support) for programming.

Curious to know which tools are there to track experiments other than tensorboard."
MachineLearning,"Interesting. Would you really for for 4x80GB instead of 8x40GB A100? Is batch size that valuable? I have a quote here for a quad A100 system with space for another four for $50K (all in; Scandinavia).

Noise is fine, it's for our DC"
MachineLearning,"Literally my experience right now. I’m at Amazon bored of doing mostly DE for my role, which means meetings all the time to figure out which team owns which tables...I am planning to move to a ~200 people company to work on more interesting problems even if it means giving up those sweet sweet RSUs."
MachineLearning,"Yeah you're probably right about the architecture. 

But that's still a hard work to do to merge everything. I'm on something else for now but I might want to look more into it if I find the motivation to re-try it.

I would need the motivation to get back on my work, merge it with what they did in TTFnet (their improvements are orthogonal) and merge that with augmentations and regularization modern architectures do. (+ I also saw another paper which managed to improve objects as points but they did quite complex things)

I mean sure, if someone is willing to pay me to do it x) But I'm sure many people are already working on it. And again I stopped my research because I don't personally have the hardware to do much better. If I was working in a lab I would totally do it but for now it would not be easy for me to compete with these big papers with my own hardware"
MachineLearning,Softmatch? I have never seen it called softmatch.
MachineLearning,"Yes thank you, i have to get out there and ask around. I could always go into consulting but not sure just yet. But i definitely dont want to have a 9to5 for the rest of my life."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The A100s alone are 8x$10k, plus 25% for mobo, CPUs etc, so \~$100k. The cost of assembly is around 20%, plus a 20% margin. Hence the cheapest vendors will be around \~$150k. At this level, I'd go for Nvidia's product. 

If you have \~$100k, I'd go for the 4x80GB DGX Station. DGX has water cooling which makes the performance slightly better and reduces a lot of the noise.

Workstations are capped at 4 GPUs, mostly due to PSUs, but all the components are designed for 4 GPU systems, which makes 4+ systems too much of a hustle. 

If you have \~$30k, I'd go for the lambda 4xA6000. But it's both loud and warm, so you need a good place for the machine. BIZON is good at water cooling systems, which is ideal if you have them in the same space where you work, although, they require maintenance and are fragile to move. 

If you have \~$20k, you can build the machine yourself with 4xA6000. That's what I did. You can also connect several 4xGPU machines with Mellanox networking cards, that's how Nvidia builds their larger pods. 

However, you have overhead with software and bandwidth configs, hence, I'd just keep them separate, ideal if you have more than one person using the machines. If you have more than one rig in a socket, it's good to check with an electrician."
MachineLearning,"These are some technical tools/tricks I use that help me to save a lot of time and concentrate on more important tasks:

First of all, use high-level ML frameworks ([AllenNLP](https://allennlp.org/), [PyTorch-Lightning](https://pytorchlightning.ai/)). No need to write boilerplate code and implement standard ML approaches from scratch.
[Here are some suggestions](https://docs.google.com/presentation/d/17NoJY2SnC2UMbVegaRCWA7Oca7UCZ3vHnMqBV4SUayc/edit) (thought more NLP-focused) that I feel improved my research coding experience a lot.

Log everything, literally everything, including hyperparameters, command-line arguments, environment variables, outputs, checkpoints, resource usage, etc. Decent High-level ML frameworks provide this out-of-the-box. Configure a callback to your trainer to send a notification through Slack. To track and compare your experiments use tools other than just a plain `tensorboard`. [Aim](https://aimstack.io/) is a fantastic tool to get insights from hundreds of experiments.

[Try to avoid jupyter notebooks](https://docs.google.com/presentation/d/1n2RlMdmv1p25Xy5thJUhkKGvjtV-dkAIsUXP-AL4ffI/edit#slide=id.g362da58057_0_1), use them **only for very preliminary experiments** to save time... But for the long-run, use decent IDEs (vscode, PyCharm) can easily help you to stay away from stupid bugs. PyCharm has stunning Python language support, while open-source [VSCode, Insiders Channel](https://code.visualstudio.com/insiders/) makes it very easy to code, run and debug **remotely**.
Use [Mosh](https://mosh.org/) or [Eternal Terminal](https://eternalterminal.dev/) to prevent disconnection even if your computer is asleep/disconnected from the internet, use [tmux](https://github.com/tmux/tmux/wiki) to run tasks when you're away. You can use your smartphone to always stay connected to the same `tmux` session and monitor the training."
MachineLearning,"[""Evolving Neural Networks through Augmenting Topologies"" (Stanley and Miikkulainen, 2002)](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)

Do I think it's particularly useful in this day and age? No. But it's inspired a lot of AutoML work, and that counts for something.

But, at the end of the day, while I think that gradient descent-based optimization has a lot more power in a lot of tasks, evolutionary algorithms strike me as more... creative? I'm not quite sure what the word I want there is. I'm in ML - among other reasons - because I enjoy the extraordinary power that gradient descent gives us to solve problems when combined with neural networks. But there's something about NEAT that just makes me go ""Damn, that's really fucking clever."" Not because it's using vector calculus, but because it put together some pretty advanced capabilities without ever needing to take a derivative.

Is it the paper that has influenced my work the most? No. Is it a paper that I would recommend literally everyone read because it will make them more productive? No. But, when I think about papers that remind me that clever fuckery is possible, I think of NEAT."
MachineLearning,"Right, this was my experience too: get a vendor to build something to fit. Also that DGX power consumption - peak 6.5kW - doesn't help!"
MachineLearning,"I mean it's a bit reductionist, right, because raw count isn't the only factor, but let's say ""up to"" 20B at mixed precision"
MachineLearning,"Yeah, really happy with the Supermicro offering. 

Sorry - NVSwitch, not NVCache"
MachineLearning,"Looks like TTFnet achieves about +2 AP for most of their models over the comparable Objects As Points models.  Might be worth looking at their improvements if they are orthogonal to yours.  

The newer Yolo architectures are good, but I don't think the architecture is mostly responsible for their performance.  If you took away the data augmentation and regularization and trained them for 48 epochs I'll bet they'd perform similar to FCOS or Objects As Points."
MachineLearning,"Wasn't aware data can be loaded onto the VM; what's the limit for pro users? My dataset's under 200GB, uncompressed, I might get it under 80GB w/ compression."
MachineLearning,"It's worth remembering that images are actually an extremely compressed representation of the true high-dimensional dynamic system of the physical environment. That compression is achieved by discarding huge amounts of information (for example, information about occluded regions is completely lost). While the image representation does a good job of aligning with the human vision system, it's actually an extremely poor and inefficient representation of the original physical environment (lots of important information is discarded while simultaneously tons of information about fairly trivial details is retained).

An embedded representation learned by something like a VAE is ideally more closely aligned with capturing the underlying dynamics of the original physical environment and therefore learns an overall more meaningful embedding space. Of course that's the ideal. Research and practical experience has shown that these embedding spaces often feature plenty of their own representational deficiencies."
MachineLearning,"Awesome, thanks for sharing"
MachineLearning,"Reinforcement learning is certainly a good thought! Alpha Go in its earlier iterations used monte carlo tree search in addition to neural networks. Current Alpha Go is totally RL with no rollouts. But Go is like the epitome of huge search space. DeepBlue beat humans in chess using only tree search. So yeah I agree maybe RL isn't your first line of attack but certainly could be used when more direct methods fail

Good luck!"
MachineLearning,"Great advice...too many people in here think FAANG is the end all be all and stress themselves out.

My advice to you though, is before starting your own company, get out in the private sector first, preferably with un up and coming company so you can learn some of the lessons, especially on the business side.  It can be brutal but very rewarding."
MachineLearning,"I'll look into it! I thought reinforcement learning might be a natural fit since it's shown success in optimizing problems with huge search spaces like this, but maybe I need to try other more traditional methods first. Thanks!"
MachineLearning,"Thanks for the suggestions! The search space seems very large to me (tens of thousands of players on 11 positions where position matters) but there are ways to reduce it to a certain subset of ""viable"" players for any given puzzle, I think. No point including 60-rated players if the target average rating is 84, or including bronze players if the squad needs to be full gold, etc. I need to read up more on these algorithms to see if I can make them fit the problem in my head. My first thought was reinforcement learning since it has shown great success when facing these sorts of difficult problems with huge search spaces, but maybe I need to start smaller. Thanks again!"
MachineLearning,"Agreed about the grant and smaller country points. Edinburgh is definitely top tier and a great city to live in too. Overall, I'm probably biased by a lot of things, so not a great judge in this debate. I'd argue that the greatest selling point of UK PhDs is their duration: 4 years works way better for someone with a master's who knows what they want to work on like me.  

The non-Schengen visa thing is news to me, thank you for clarifying that. As a matter of fact, I do follow you on Twitter: you're probably the most vocal anti-Brexit person on my feed :)"
MachineLearning,"I use duckdns.org. You just set up a Cron job that updates your IP address and log in by domain instead of by IP.

My IP actually changes very rarely anyway though. I've even lost power and kept the same IP, but with duck DNS, it's never a problem"
MachineLearning,"A recent discussion, possibly related: [Senior researcher here; introducing deep learning to a field which doesn't have it. Suggestions on how to deal with bad reviews](https://www.reddit.com/r/MachineLearning/comments/k3nqn5/d_senior_researcher_here_introducing_deep/)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"PhD is about opening more doors

Its just like those games which announce ""Achievement unlocked"" and you have more special skills. Life imitates gaming art"
MachineLearning,"5.	If by 
 
&gt;	remote, low-commitment 

He/she means a normal 40hr/wk, that doesn’t suck your soul, absolutely possible, I’m in an ML gig like that right now. You do have to find it and have enough corporate experience to push back on longer hours.

However, I’m also doing OMSCS w/ two classes... so pick your poison lol"
MachineLearning,"I was in the same boat, I was looking for jobs, applied to google and fb got rejected from fb (tbh they were kinda disrespectful, garbage interview) and google (way more professional) and i solved their problem but ultimately got rejected.

My advice is to know people, meet people, and take advantage of opportunities they present. I spoke to my advisor for my phd letting her know i needed a job. Suddenly i had two jobs furnished for me. Now i am a data scientist and faculty member at my university. The position pays well and its more relaxed, and I work from home (due to covid).

I think that getting into one of those top tier places requires you to know some people. Reach out to professors, other students, and even authors of papers and build rapport. This is how you land a job.

Also, most of your day will be filled with data preprocessing.

I also have dreams of starting my own company (which i should do soon) and its not all good. There is a lot of work that goes into building a business and one will probably spend 60+ hrs working. Here is the catch: you are working for you, not someone else.

Anyhow thats my two cents

If you dont get your dream job dont let it weigh on you too much, fuck that. Do your best, if they dont take you, then try other places that will and build up from there."
MachineLearning,"For this kind of problem, integer linear programming is what you want. There are many solvers available.

ML is more ""learn patterns from data"", something like this more falls under optimization research."
MachineLearning,"The recent Yolo method's performance partially relies on implementation tricks like extremely long training time, regularization  and data augmentation.  They also have gpl licenses which isn't ideal.  

I wonder what a lightweight FCOS would look like trained with identical tricks for 300 epochs instead of 48.  

Have you seen TTFnet?  They made some great improvements to the objects as points method and provided a clean implementation on mmdetection."
MachineLearning,"PAC learning theory might not be investigating the limits to ML that interest you, but it is very clearly an attempt to formalize a class of ""fundamental limits to ML,"" which is what OP asked about. 

Honestly, I'm not super well versed in the topic myself. My understanding is that the theory attempts to generalize model complexity/capacity. I believe it does take data quality into account via coverings, which is part of how shattering is defined."
MachineLearning,How many total parameters are you trying to train with?
MachineLearning,"Cool, and yes that makes sense.

I think the best approach depends on the size of the player population and whether there's a greedy way to evaluate the benefit of adding a member to the squad. You could try to look at A\* algorithm and see if that works.

If the search space is truly too large for a brute force, dynamic programming, or greedy algorithm (or some other deterministic algortihm) to produce an objectively optimal answer, then maybe ML solutions start to make sense.

&amp;#x200B;

If it comes down to that, I'd look into maybe using value iteration or value iteration in conjunction with a tree search. Can you get away with using some variant of MCTS [https://en.wikipedia.org/wiki/Monte\_Carlo\_tree\_search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) ? 

&amp;#x200B;

Hope these are some helpful ideas to get you started but I'm not an optimization expert and these are just some first thoughts. Seems like this might be a pretty easy or extremely difficult problem depending on the size of the search space."
MachineLearning,"Ideas are cheap.
If you can't even get a proof of concept, maybe don't publish a paper about it yet."
MachineLearning,!RemindMe 1 week
MachineLearning,"From personal experience, I'll echo what many others have said. It depends on what tools and frameworks you use, but be prepared to spend a lot of time making things work and for some things not to work at all.   


I got an M1 shortly after they came out and I love the machine, but I can't carry out my whole ML workflow on it. I still end up having to use an intel machine for Tensorflow or I run stuff on Colab/Kaggle kernels. I expect that to remain the case for some time to come.

The Apple fork of (native) Tensorflow is not anywhere near ready to be used for anything more than playing around with. Just take a look at the issues list on Github. I also don't think this is something that will ever be more than a prototype. It has something like 5 contributors working on it last time I checked and is not really seeing much active development. It's possible Google is secretly working on its own implementation, but honestly I don't see thy have much incentive.

Running Tensorflow under Rosetta also doesn't work because it depends on a library (I think it's called AVC from memory, used for hardware acceleration)  that is not supported by Rosetta. Tensorflow will install but crashes when you try to import the module. Some people claim to have got it working by building Tensorflow from scratch against some other libraries. I haven't gone down that path yet, but this gives you an idea of what your'e in for.   


I don't use Pytorch, but they don't support M1 natively, I don't know if it will run under Rosetta.  


Other libraries such as Scipy and numpy also won't run natively because they need a Fortran compiler which does not yet exist on M1.   


If you must run things locally on a laptop then I'd avoid the M1. However if you can use the cloud for training (at least for the time being) then it's great machine."
MachineLearning,First author here. Happy to discuss and answer questions :)
MachineLearning,"lool same, im literally working on my applications this very moment 

given me the break i needed"
MachineLearning,"I’m an ML engineer at FAANG and while I can’t speak for the more pure research roles, for applied research roles there are many people with non top tier PhDs and with a masters only. These roles can get you $300k out of PhD and I think you’ll definitely be able to land an interview. However, there are coding portions of the interview so I would make sure to prep Leetcode"
MachineLearning,"&gt; Any good task manager, trello will do.

Notion?"
MachineLearning,Same here!
MachineLearning,Thanks for the thorough reply!
MachineLearning,"It's easy to make mistakes in multi-threaded code which don't surface on strongly-ordered memory models (like x86), but trigger on weakly-ordered memory models (like ARM), and hence someone who wrote and tested their code on x86 might not even be aware of problems. See e.g. [this blog post](https://randomascii.wordpress.com/2020/11/29/arm-and-lock-free-programming/) for some examples. Running into a problem like that, a problem that is likely very difficult to debug, might not be a desired outcome in final weeks of a project… especially if a project depends on a pile of academic and third-party code (like it usually is in academia), where each item would have to be audited separately."
MachineLearning,"Do you love research of getting a degree? Because when you are too biased toward thinking about Jobs, degree with good grades, fellow people's success and becoming a ""flawless version"" then you foget the root problem and stucked in sub problems.

May be my answer is subjective, but if I were at your position, I'll stop worrying about Job, Good career,  getting into big five and following the race of ML papers.

Instead I'll enjoy the process, go for an extra mile do provide quality over quantity and *enjoy* the research publication process.

Those task you mentioned are not boring, it's boring because you are doing in that way, you can share your insights online, write blogs and you could publish a short paper on these topics, you don't need to come up with a great idea but make a small idea great. Not all of us can do great things. But we can do small things with great love.

Look at some papers, they have contributed very little but the contribution is very useful and cited by most of papers in that field(if you do care about citations)

When you love the process, good result is byproduct of that.
Go back, think about the root, why did you came into research field, bring some curiousity, forget the success, fame and getting good job, just try it and share your results.

And even if you fail in results, share your fail experiment result in blog or somewhere, those are more useful than successful ones. 

I believe, every researcher should add one section what he tried and failed in paper because usually they share what tried and worked but negative results save other researchers time and resources more than SOTA accuracy and good results.

My thoughts could be subjective, but I would say Enjoy the process, if you are going to try, go all the way, go all the way.."
MachineLearning,"RemindMe! 1 week ""Ph.D. ideas"""
MachineLearning,Sounds spot on to me
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Yes
MachineLearning,"No, it doesn't, I just thought that it was a natural approach since the search space is so large, but I know very little about dynamic programming. The chemistry scores are calculated by the connections between players. Every player is put in a position on the squad (goalkeeper, striker, etc.) and some positions are connected to others. You can have a 0, 1, 2 or 3 chemistry link with another player, depending on whether the two players are from the same country, play in the same league or the same club. So it isn't hard to calculate them when optimizing.

So yes, it's a search/optimization problem, I just don't know the best way to approach it!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Why would it?
MachineLearning,"PhD student here:

1. I'm using the free tier of Jira and confluence, for just myself. I make a list of all of the experiments and training runs to execute, put them in a table in confluence, have one column for in-progress/to-do/done and work my way through. When a run finishes and I have a gpu available then I check the list to see what to start next. I start everything manually I don't have a queue system. My runs take a few days so this is fine for me.
2. When a model is training and all my configs are set up then I'll work on other things. I find it hard to switch though and it takes me time to get into it. This is fine though mostly because my models take days to train. When my models start training i watch the output closely for 10 minutes to check it's going ok. It is what it is. I'd rather watch time here and make sure it's ok than come back 2 days later and find that it's done nothing for the last 1.5 days.
3. I run everything in containers, so I simply load up a container with the relevant environment.
4. Yes"
MachineLearning,I have limited myself to 16 minutes per model. I'm working on interpretable AI. The models are comparatively tiny (64&lt; nodes) and sparse (1 or 2 connections per node)
MachineLearning,"I gave the analogy of CI because all else being equal, time is a good proxy for difficulty level and is non-linear. It is not as if B-aires have 1000X IQ than that of M-aires, just that they are solving problems that are of a different nature altogether in terms of scope (markets)/impact which is why the reference to 'Zero to One'."
MachineLearning," RemindMe! 1 year ""think"""
MachineLearning,"Any advice on leveling up my math and stats skills? I currently work in a more senior analytics role (i.e. have a team reporting to me, report directly to C suite).

My programming is solid and my biz side (aka product skills, presentation skills, data viz/design) etc. is also solid.

My math and stats aren't awful, but I haven't had a formal class since college and while I did well in those classes I don't think I established a strong base in the subjects (lotta cramming then regurgitating knowledge).

The usual problem I've found with a lot of Course era, Udemy courses, etc. is that they'll combine math/stats with learning to program in Python/R. I'll find the programming parts extremely boring and be forced to skip through/extricate the math/stats parts.

Anyone have a good recommendation on something a little more classical that solely focuses on the math?

Thanks!"
MachineLearning,How fast do your models train? Mine are days at a time!
MachineLearning,"&gt; if you can compile your code with llvm then expect &gt;10% improvement over your previous Mac.

Assuming that the weaker memory model won't affect your code."
MachineLearning,That's a fair point; throw in pre-ipo startups and the upper ranges could go up even more
MachineLearning,"Hey, are you still working in India?"
MachineLearning,"Title:How to represent part-whole hierarchies in a neural network  

Authors:[Geoffrey Hinton](https://arxiv.org/search/cs?searchtype=author&amp;query=Hinton%2C+G)  

&gt; Abstract: This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language  

[PDF Link](https://arxiv.org/pdf/2102.12627) | [Landing Page](https://arxiv.org/abs/2102.12627) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.12627/)"
MachineLearning,"That seems like an interesting project! Is the mileage analog or digital ( not sure as I know posche has both ).

But since you mentioned OCR, I am guessing it's digital. I used Tesseract which does fine for me most of the time, however Google Vision API is pretty solid too."
MachineLearning,Total comp offers I know from friends who work in ML or systems even are at in that range or higher. I think people underestimate at large tech firms how much bonus and RSUs contribute to total comp even at 160-180k base salary even senior engineers can be making north of 220k a year.
MachineLearning,+1 on Azure ML. I use a similar platform as well.
MachineLearning,I don't use jupyter notebook. Really like VS Code's python debugging facilities.
MachineLearning,L2........
MachineLearning,Lol no.
MachineLearning,Dm me. I did not publish for 3.5 years and I'm doing well now as a researcher at a good company doing stuff that I love. I can probably sort something out for you if you'd like.
MachineLearning,"As a software developer that spent his younger years installing Linux on everything (and just purchased a macbook air with an m1): Don't do it! I hate apple, but my app (which uses tensor flow js) needs to be cross platform and future proof. I know what I'm getting myself into, I have a compelling need, and i posses the the skills / stubbornness to see the project through.

If I were you I'd get a lightly used dell latitude with great specs and put my favorite Ubuntu flavor on it. Or, if you're feeling adventurous maybe go straight for a nice System76 rig."
MachineLearning,"1. As far as what do you need to work at one of the FAANG's labs, the one colleague and friend I know who works at one is a certified genius.  My work is great but I don't try to compare myself to him. 

2. All kinds...huge spectrum.  For the most part (on the applied side at least)  you're doing what you do right now: wrangling data, applying creative solutions, coding.  I see us as software engineers but with the creativity to come up with solutions. 

3.  You're late to the game. You should have a plan A, B and C already.  Take some time and envision a couple career paths and how to get there.  Go to job listing sites, look at the qualifications, get on linkedin and 'link' with research scientists, ask them questions, build your network, post. 

4.  On the applied side, tons of places are hiring.  Most firms are playing catchup when it comes to AI/ML.  You are the expert, you tell them what they need.  I'm not joking, that's why you're getting the PhD.  Start looking at these listings and thinking about the kind of datasets these companies compile and how you can come up with solutions.  If you're in the US, get off the coast, come to the flyover states.  You can make 100K to start and move up pretty quick to more and live much better in my opinion. 

5.  I think its the opposite with remote jobs...i think you end up working more hours.  There are plenty of 40 hr a week jobs out there.  If you want to build your own business, you better get used to 100 hr/wk...I'm not kidding.  

6.   I like it.  I don't kill myself on hours and I feel pretty valued. 

You are a young person breaking into the field.  Just like any other professional (doctor, lawyer, academic) you need to expect to work hard for the 5-7 years.  If you want to be a trades-person, you can punch the clock, not be passionate and do 40-hrs.  You hint that you want to work for one of the big boys...they work their ass off and earn it.  

Start looking at the wide array of jobs out there and start figuring out the ones you might want.  Hook up with people on the outside, figure out the hook for selling yourself and you'll gain confidence."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yeah you're probably not going to get a 300k job out of school, maybe 130-150k depending on where you want to live. It won't be a breeze to find a job in data science or ml engineering, but you'll find one if you try for 3-6 months given you're relatively picky about what you want to work on. 

If you go for a big company (e.g. Amazon) you'll often work with classical techniques on very specific problems. You'll spend a decent amount of time cleaning data and doing data eng. If you go to a small company, you might have a better shot of doing more cutting edge stuff, but your resources will be limited and you'll be doing even more data eng.

Finally, a phd is very nonlinear. It's not rare for someone to not publish for a few years. I wouldn't give up on yourself yet if you think you want to do research. Keep at it, you never know, you probably have 40% of your phd left. Try to get some steam going by publishing simple stuff, applications papers are still publishable (here's a transformer plugged into this dataset). You won't get in top journals probably, but you will find that doing these projects gives you more ideas about more interesting research directions, and you never know what you might find.

Best of luck"
MachineLearning,These numbers are correct
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; Tell that to the high school teachers in those areas...

Tell the teachers about supply and demand.

Low pay, yet no shortage of teachers."
MachineLearning,"The rule is valid but the exception is kinda personal decision. Using new stuff is good for your professional development but bad for your short-term output. You have to convince your boss that short-term productivity loss will be compensated in the future.

Honestly, mac was always a joke to me. Look into enterprise-level dells or thinkpads, especially older ones."
MachineLearning,"&gt; Who is this for? Students or seasoned researchers? 

Typically ML/AI PhD students. 

Originally started for non ML students but competition became intense and now you need ML publications prior to applying. 

&gt; Would this be a good way to get your foot in industry?

Yes, but extremely difficult to get"
MachineLearning,"&gt; The difference in the number of CDT fundings for international students vs domestic students is pretty stark.

Yes, that's true. It's good to remember that these universities are lot less wealthy. However, many ML profs have grants that can support international students, so worth talking directly to potential supervisors.

&gt; Furthermore, outside of the top 4 (Oxford, Cambridge, Imperial, and UCL), there's really maybe one or two other places doing ML research and publishing at top venues.

We have fewer ""top"" universities, it's true, but it's a much smaller country. Edinburgh, at very least, should be in the top tier for ML/CS.

&gt; For me personally, the reasons to prefer the US or EU for a Ph.D. versus the UK would go beyond funding. For example, the ease of free travel and post-degree job location options would be a big factor for me not considering the UK as a Ph.D. destination as seriously as when you guys were with the EU.

If you follow me on twitter (and I have no reason to assume you would), you'll know I'm no friend of Brexit or the current government / general direction of the UK. However, despite that, if you are not a UK/EU citizen, the situation you coming to the UK for a PhD will not have changed much between pre- and post-brexit times. Even when the UK was in the EU, getting a visa to come study here did not entail having a Shengen area visa, which had to be applied for separately, so freedom of movement would not apply to you simply by virtue of studying in the UK."
MachineLearning,"Starting base for PhDs at Facebook (across the board, not just FAIR--I bet it's a lot higher for FAIR) is 160k salary, 300k/4 yrs RSUs, and 75k signing bonus, and this is without considering bonuses. If you're really good, they've doubled RSUs in negotation stage before and it isn't uncommon to get more on the signing bonus side of things. I bet the top ML PhDs going directly into FAIR are making well over 350k recurring. Google Brain/OpenAI/MSR probably pay just as competitively, maybe MSR a little bit less if SWE compensation is a valid indicator. Not to mention you get access to insane compute at any of these places to research whatever you want.

Just thought I'd add some data points."
MachineLearning,"I have said BASE in my answer. Total package might be more , however all the new phd grads I saw were L2 - L3. Not L4"
MachineLearning,"That we agree. But it was same with web developers in 2000. The fad will wear off and possibly the salaries as well. I guess people are enjoying it as long as it lasts.
Full disclosure: The 7 figure was hit only once. Most people are in upper low to mid 6-figures, including me."
MachineLearning,Working ass off to get a seven figure salary I suppose? A lot of people would probably allow a meat hook to be put in their shoulder for such a salary and not complain but there is some minor distortion (not necessarily in your post) in the tech and ML space about how outsized the rewards for whatever hard work is supposed to be put in.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"A set of almost 10000 generated pages can be found here: [https://drive.google.com/drive/folders/1ySnYXA\_l\_lQ3n8yh0EjpFk7RISYj3nCL?usp=sharing](https://drive.google.com/drive/folders/1ySnYXA_l_lQ3n8yh0EjpFk7RISYj3nCL?usp=sharing)

All elements in the image are created through either some sort of Artificial Intelligence or a custom algorithm.

The specific tools used were:  
Pokemon image: Stylegan-ada  
Pokemon sprite: Stylegan-ada projection, later images are simply resized from the original image  
Types: Custom algorithm   
name: Word2Vec pre-trained model, custom algorithmWeight: Custom algorithm   
Height: Custom algorithmEntry text: GPT2"
MachineLearning,"I can speak from experience that you often know when your work is ""done"" and for some it might be with just 1 paper they are proud of while others might hit it at 10. Also publishing is extremely non-linear and a lot of middle author papers come out without you doing anything after an initial contribution years ago. Not exactly plannable other than just being nosy and helpful in as many projects as possible."
MachineLearning,Nope. Nor from Facebook AI Residency either. :(
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I look forward to reading the paper, awesome!

There is so much more to measure theory than the general student of abstract mathematics thinks. I myself take an interest in order theoretic probability theory (using lattices), how this relates to the category of Hey or CHey (just remember it won't form a complete lattice in R when you consider borel sets!), is actually interesting and not just abstract nonsense. I really hope the machine learning community goes even deeper into real abstract, proof-based methods of research. It sometimes feels like most of the math in ML papers are somehow context free copy and pasted from wikipedia or something and not actually properly defined in terms of strict mathematics."
MachineLearning,[deleted]
MachineLearning,"One thing is to learn how to program, ci/cd, correct use of git, test, and stop using shitty tools like jupyter notebooks. People first write code on jupyter (no debugger, formatter, shitty enviroment) and then they just copy the code to python files, why not learning how to directly code in python?"
MachineLearning,"I like it quite a lot, it's helped me organize my work better (I'm a really messy person)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"RemindMe! 2 years ""think after PhD"""
MachineLearning,"Sure. No, a simple bash script is not enough. In my case, we have several machines shared in the department, some with GPUs, some without. What I have is a python script that gets a list of jobs and then it schedule them in the first available machine (according to memory/CPU/GPU availability). Unfortunately, what I have is really entangled with our computing platform (Docker-based with a shared filesystem) and not really easy to have it as standalone project (that's why I said ""know you infrastructure""). The most similar thing that I could find online is [this project](https://github.com/jigangkim/nvidia-gpu-scheduler). I believe there are then some HPC tools that could be useful (e.g. Slurm), but that's way too much for what we need."
MachineLearning,"Ahah, this is what I'm thinking; my opportunity to get a new fancy laptop. 

Yeah, the new company will provide a new tech setup, I'll try and talk to them today to find out what it is. I'll still probably want a personal laptop as well though right? 

Perhaps the M1 air will be sufficient, although people have made a lot of good points about just sticking to what you know when your in a time crunch... 

I'm gonna ask around my family to see if anyone has an old laptop that I can borrow for now, but I feel like I should just be getting my own tbh."
MachineLearning,"I wanted to hear ""GPT-4 is coming soon"" so bad."
MachineLearning,"Just curious, why would somebody publish 10+ papers and not graduate at like the first 5  ?"
MachineLearning,"As someone from India, I once had a burning desire to get into a top ML PhD program in the US but instead, a family commitment pulled me back, I took an ML job at a top IB and now make close to 70k $, which affords me quite comfy life here. I'm glad I didn't make that jump.

I don't agree with others comments that Banking firms have bad work life balance though, I worked hard for first 2 years, putting in 10 hours a day but now I hardly ever put more than 8 hours a day, including lunch. You should not hope to get comfy right out of college, even if PhD, maybe once you're two years in."
MachineLearning,"Company should provide you hardware to do your job. You can confirm, but unless they explicitly ask you to bring a computer, you should be thinking about personal use only.

If you're using git and latex any old computer will work - you can delay the decision if you have a spare computer (say a uni provided one) that you can use. Given its the end of your PhD, and if you have a hard deadline, I'd say go safe, but this is a computer you're going to use for the next few years, you may want to just buy a crappy second hand Linux laptop and use that so you can get a new fancy laptop with that sweet salary coming your way."
MachineLearning,"I'm only a random master's student, but most companies me and my friends work/interned at provide computers for you. i don't think you need to necessarily worry about what the company stack is.

if you really need a computer asap, i personally think you should just go with what's tried and true (aka the intel platform based stuff). you might accidentally shoot yourself in the foot if you try to bet on new stuff (especially stuff as brand new as the M1 mac)"
MachineLearning,There is a paper [Inquisitive Question Generation for High Level Text Comprehension](https://arxiv.org/abs/2010.01657) that tries to generate questions to clarify things not present in a paragraph.
MachineLearning,"I'm guess it's because people create issues like this:

[https://github.com/openai/gpt-3/issues/2](https://github.com/openai/gpt-3/issues/2)

It takes work to respond to these and close them."
MachineLearning,watch till the end. He mentions he understands why it wasnt released to the public
MachineLearning,OMG. If DS consider bash as DevOps then I can see how it can be bothering. I should probably check my privileges...
MachineLearning,"Because it is a compressed representation with a continuous mapping, which is mostly smooth.

Imagine the latent space were 1 dimensional, then following the latent will get you through all datapoints. Now this is a compressed representation as well, so that path will try to bundle similar images close together in order to compress better and find a smoother path."
MachineLearning,https://amitness.com/2020/05/data-augmentation-for-nlp/
MachineLearning,"The record of volunteer work does not show your interest in the world. There are people who don't do volunteer work but are actually much more interest in the world then the people who 'pretend' to do. Honestly, if schools use these as a part of their metric it is just a way for them to get the rich high class students (I bet most people know why I say this)."
MachineLearning,Many thanks for this!!!
MachineLearning,"I can try to answer your question based on my experience. I have a PhD in ML from a top school. I ve worked in the UK and the US in one of the big tech  


1. What level of research background do I need to land ML research scientist industry positions? Both more and less applied roles. Are these open to PhD graduates with just a handful of lower-impact, less-interesting, less-cited, but still sound papers?  


Not mush or any tbh. Most people I worked with in tech were software engineers with some understanding in ML. There are some places that like top journal publications like Microsoft Research or Google Deep Mind but everyone else doesn't care. 

&amp;#x200B;

1. What sorts of jobs do ML PhD graduates have? What is the day-to-day work like?

The deal here is pretty simple: With a PhD you will join as a junior member of the team and might be able to get a promo in a year or 2. Without a PhD or experience you will probably stay junior a bit longer. Day to Day: You have a manager and a bunch of stakeholders, they tell you what the problem is, you try to find a solution. Normally in tech there is also a tech lead that is responsible for helping you find the best solution while the manager is responsible for the people's stuff

&amp;#x200B;

1. **How should I go about planning for/setting myself up for the type of career I described?** Is this even the right way to think about things?

You need some years of work of experience. Noone will hire someone out of a PhD and pay them a lot of money straight away. That is not even realistic. Also trying to shortcut things is not something that normally works, despite PhD or no PhD. You still need to work and get  some experience. After some years you can set your self up as a consultant and charge by the hour. For this career to be successful you need to convince people you are credible, and that's not about the PhD, but about work experience and how convincing you are when you talk to your clients. You have mentioned your friend: Truth is there isn't enough there to understand what he does and what his experiences are. I can def. tell you noone gets 300K straight out of uni / PhD. I was hiring these people for years (and still do). You are looking at 150K base even in big tech. I was working as a manager for one of those companies and I was on about 100K more. I haven't reached 300K base after all these years of working. If you want this kind of money you can get into hedge funding but you will be working 18 hours a day. As I already mentioned there are no shortcuts here and everything is a trade off. if you want a life were you do stuff at your own pace, be more relaxed and actually having a life big tech and hedge funds are not the place. I was constantly exhausted from late nights and stressed because of very high demand / stretched deliverables. 

&amp;#x200B;

1. How can I find out what sort of applications and topics are most relevant in industry? I'm aware of applications in CV and NLP and a couple other topics I've worked on, but most of the work I hear about comes from academic conferences, reading groups, faculty and students talks, etc. I don't really have a good sense for what happens in industry, and what's important to people there.

It really doesn't matter. During your PhD try to get as much knowledge on as many topics as you can. Noone expects you to be an expert in anything straight out of PhD. The PhD gave me very deep knowledge in something very small. Industry gave me very shallow knowledge on a lot of topics. Unless you want to join a research lab for one of the companies mentioned above you just absorb as much knowledge as you can during school years. After getting a job you will be a junior in a company and you will start learning whatever they are doing. Before you join make sure that subject is interesting to you but if you don't like it you can always leave

&amp;#x200B;

1. Has anyone had success in this field with a remote low-time-commitment sort of job? Or built a business that allowed you to do that?

How do you define success? Is success getting okish money to live in a cheap place in Asia? Or is success enough money to live in NYC / London? As I said you can become a consultant (AFTER YEARS OF INDUSTRY EXPERIENCE) and charge by the hour. I know people like that, i even have my own side business that does that. Money is not enough to live the life I want just by doing consulting. It depends what you feel is a comfortable life. I seriously doubt you can work 12 hour week and be able to afford a flat in London or NYC (which is what I consider comfy)

&amp;#x200B;

1. What are your industry experiences in general working in more/less applied positions, research positions, engineering positions, etc.?

I ve been a manager, an engineering manager, a head of XXXX the past 5 years. I can tell you the stress levels are the same and probably more as you climb the ladder. My goal is to start working 4 day weeks at some point and take a salary cut. In terms of the actual work, I am not doing hands-on work anymore."
MachineLearning,I rather feel that some people just cannot admit they are privileged and got their position due to it with the expense of some genuine hard workers
MachineLearning,"Ahah, yeah sorry, I'm getting wires crossed here. 

1) For finishing my PhD, I basically just need Git and latex. Over my PhD most of my work was done on the department's computing cluster, so I rarely ran anything too heavy on my laptop anyway. The reason I've been training stuff is for technical assessments for job interviews (which I've now been offered) but I'm kind of looking ahead to what I'll need on the job. 

I know the company I've been offered at uses azure heavily and I'm guessing they'll use docker (which may be an issue with M1). I'm not sure what ML frameworks they use, but they wanted to test my sklearn (this may just be as a benchmark test tho..)

2) Aren't we all just running scripts and hoping they work on some level ;). I'm saying that I have never had to worry too much about the machine that my jobs are running on before now, I just submit things to different batch systems. At no point in my work have I had to worry about specs before.

Yeah, as you say, is rather not have to go through work around like this. I'm sure I could but it's not a very efficient use off time figuring out how"
MachineLearning,"Peter Thiel's zero to one had something similar, if you are looking for practical significance. Read it a while back but I think his point was to build businesses that are completely novel (""moonshot"") rather than competing in an existing marketplace...at any rate, if your aim is to build a $20-100 M annual turnover business the problem that you are targeting needs to be chosen accordingly, if that makes any sense"
MachineLearning,What is the most cutting edge open source language and big data storage for AI and ML today?  Is Microsoft ML.NET worth exploring?  Structured or non-structured databases?
MachineLearning,I wonder if they did experiments with more training steps
MachineLearning,"I'd say there is a grain of truth in that statement. Becoming a billionaire without luck is definitely 1000x harder than becoming a millionaire without luck. But how many billionaires are there that didn't have any element of luck? For that reason alone, the large proportion of billionaires probably did as much work any millionaire business owner. With that said though, it still stands that you need more than 1000x more luck to become a billionaire than a millionaire. 

I'm not really sure what the practical significance is eithery way."
MachineLearning,Not getting royalties is the current state of the porn industry - almost every sales channel is owned by the same company under different brands.
MachineLearning,I the exact same filename :-)
MachineLearning,"Any chance you transition into a windows laptop? The new Apple laptops have really bad cooling system with a fan that serve no purpose. If you're going to be running ML from it, your laptop will require sufficient cooling. Check this out:
[Apple Cooling](https://youtu.be/iiCBYAP_Sgg)"
MachineLearning,It was started by Elon as an imitation of MIRI after he hung out with some Bay area rationalists (a group of somewhat-rational people who think we're going to be enslaved by AIs). The current ownership essentially changed the entire business but kept the name.
MachineLearning,"I think you missed my point, so many people aim for ML PhD because they say they ""want"" to do ML research, but here we see someone who already reach there and yet still confused if research is their path. I just comment in a ironic way thinking maybe ""I"" also need to rethink what I really want to do before just charging for something I might not really want."
MachineLearning,"M1 is not really cutting edge technology, it's been shipping in iPads for years. Tensorflow for Metal is pretty new though."
MachineLearning,"This guy deleted himself because he realized how fucking dumb he is lmao.

ExTenDinG ReSearCh? What?

Honey, what do you think research is? Do you really think Hinton came up with backprop in his sleep? No, he extended someone's research on backprop and made it more practical."
MachineLearning,"Yep, it's quite discouraging"
MachineLearning,"Wow, all I can say is thank you for making this post. You described my current situation/state of mind frighteningly well. Best of luck to you. I hope you find peace!"
MachineLearning,"I just received an offer to a top ML PhD programme as well and this post articulated my fears.

Time is of the essence, and since you are a third year student, I would suggest to start the thesis and get it done ASAP with a goal to transition into less competitive field where ML is used. For example, I'm currently working in agriculture where there aren't many ML experts and PhDs are sought after.

If you want to reach higher wages, I'd suggest to put all of your effort in specialising in one niche, just like you said that in the post. At this stage of your career I think it will be hard to start chasing publications especially if you don't enjoy them. Rather, develop a skillset that companies would pay for and attempt to leverage and develop your network. Your network can compensate for a lack of stellar research career."
MachineLearning,"Caution everyone: Dilmer Valecillos is a spammer.

He routinely [spams](https://imgur.com/a/PWNAQl9), [breaks subreddit rules](https://imgur.com/a/01hnt4t) to promote himself, and even spams [anyone who connects with him on LinkedIn](https://imgur.com/a/00CiyYa), which he thinks is [just fine](https://imgur.com/a/dYfnGWl)[ (source)](https://np.reddit.com/r/unity_tutorials/comments/bugza2/how_to_exploit_reddit_for_financial_gain_just/ev8te0l/?context=3).

He has a [patreon](https://imgur.com/a/c8jhbdo), and creates [clickbait videos](https://imgur.com/a/PzIvsVa) [(source)](https://np.reddit.com/r/unity_tutorials/comments/9wpwnb/how_much_money_can_you_make_by_making_great_games/) and hastily made tutorials (mostly knockoffs of proper Unity tutorials), sometimes with [shoddy code](https://np.reddit.com/r/augmentedreality/comments/c429xx/placing_tv_screens_in_augmented_reality_can/eryo45b/?context=3) (which he refuses to correct even several months after the errors were reported)... all in order to [monetize himself](https://imgur.com/a/ZC3ATfF) and his channel.  When folks call him out on spamming, he [threatens them](https://imgur.com/a/H3jet1o) [(source)](https://np.reddit.com/r/creativecoding/comments/buewqv/dont_miss_it_procedural_generation_city_with/epc2aki/) and then deletes his posts.  

Lately, he's been getting banned from several subreddits, and has in response created fake versions of those subs (i.e. r/Unity3D_Tutorials instead of the real r/Unity_Tutorials), where he bans anyone who criticizes him.  Please don't fall for this.

Here are some good subreddits for quality Unity3D content:

r/augmentedReality

r/Unity3D

r/Unity_Tutorials

r/UnityCurated (full disclosure, I'm the mod of that sub, which showcases tutorials that have been vetted by professional game developers for quality and accuracy.  I do it to help the community here on reddit, not for financial gain)."
MachineLearning,"It depends on what kind of software you use. If you run windows stuff in a vm then there will be a (10&lt;= penalty &lt;= 30) percent. If you run stuff that are already compiled to x86 ie binaries you don’t compile locally, then expect a 10% drop, if you can compile your code with llvm then expect &gt;10% improvement over your previous Mac. 

In general there should be notable improvements and the high frequency ram and the huge L2 cache should be very beneficial, especially with python."
MachineLearning,Thank you!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Forgot about trello, it's pretty good I'd recommend it as well"
MachineLearning,"You are on a time crunch.

Will the improvement in performance save you so much time that it outweighs the lost time of having to set everything up on a new processor type?

This is an unknown question but one you need to evaluate realistic probabilities to and make a choice on. 

What happens if your probability estimates are off? M1 is easy and fast = no problem, M1 is a headache and causes more lost time than you anticipate = the quality of your final report is reduced by the hours that are not spent working on the thesis.

Lastly, considering training is asynchronous to your workflow, it is probably not actually the bottleneck you need to solve for. If your intel machine takes 20% longer to get results, you can probably find other ways to use those hours to improve your paper, either by reading other sources or proofreading or just writing templates and then fleshing them out once the results are in.

My opinion: go with what you know, get it done, and revisit after you graduate on what you want to use after"
MachineLearning,Just curious how you setup ssh forwarding on your home network. Did you have to get a static IP from your ISP?
MachineLearning,They support up to six displays. Just YouTube search. But you do you
MachineLearning,"Is PAC learning really a limit though? Isn't it like a worse case scenario/upper bound? 

For instance, with PAC calculations, you can say with N examples, we can say with X% confidence that the true error is Y. This calculation depends on the VC dimension, which is usually an upper bound.

Lastly, I don't think PAC calculations factor in data bias at all. So if you have a very representative datasets, it would perform better? 

All in all, it sounds like a worse case scenario, which doesn't seem like a limit to me. It tells you a bottom line, but not the upper limit. 

Could be wrong, so interested to hear your thoughts."
MachineLearning,"Get a Mac mini for $599. Super cheap and return if it doesn’t work

I did the same and returned it. It doesn’t run py torch, docker, or older versions of tensorflow. (So my gpt2 programs wouldn’t work)

It’s a great box otherwise. Trains as fast as my 10700k for the notebooks that do support it."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Which part strikes you like the glitch in matrix
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Why would you want to improve your productivity ?
MachineLearning,What parallel universe is this?
MachineLearning,DM me let's talk
MachineLearning,"Other commenters are missing the point here. On surface mathematical equations are not more difficult to parse than code. It is the ability to instantly lookup symbols definitions and context that differentiates it from the Greek symbols which often have implicitly assumed meaning. For centuries papers use the same citation mechanism to refer a concept. It is like referring a variable by the 1000 LOC file containing its definition instead of using class scopes. I sometimes toy with the idea of creating a framework for instant lookup of the symbol meaning in research papers. At the moment, the only solution is to be persistent and it will get easier with experience."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Does anyone in here actually use it for fast paced ML research? Isn't it too much overhead?
MachineLearning,"The point is why interpolating hidden representations can give meaningful results.

For example, interpolation in image pixel space is usually bad. However, we can get a better result by interpolating VAE hidden representations.

My question is, how VAE makes it happen."
MachineLearning,"Not really. Of course, it is non-linear, that is why you have compound interest. Those who are building billion dollar businesses are not working 1000 \* number of hours millionaires because total hours in the day are same for everyone. also, number of billionaires to millionaire ratio is &lt; 10k"
MachineLearning,Who is this for? Students or seasoned researchers? Would this be a good way to get your foot in industry?
MachineLearning,There’s a list of software that has been ported to arm for M1 macs. Check it out here https://isapplesiliconready.com
MachineLearning,"Correct, one should not pursue a PhD solely to make more money. 

Having a PhD does open new types/areas of jobs — but not necessarily higher pay."
MachineLearning,Agree with this. The highest salaries I know are for those who are basically the top in their field.
MachineLearning,"It’s more about the work you do than the money.

With a PhD you can more easily choose your projects and work and even company.

With a bachelors it’s really easy to get stuck as a wage slave doing things you don’t care for but the moneys good and you can clock out at 4:30 and roll in at 9:30. 

Each personality is different on what they prefer"
MachineLearning,[deleted]
MachineLearning,"Properly parametrized PyTorch Lightning scripts, running on Azure ML using all the experimentation goodies that come with the platform. A script will kick off 50 runs that will complete over night, come the next day and look at the result and think what I want to try next."
MachineLearning,"I’m a little confused about your math. If we are talking about the size of the input space, shouldn’t it be (3x256x256)^256? The latent space would be (32x32)^8192. I’m not sure which is bigger."
MachineLearning,No. I haven't done a quantitative assessment. Only qualitative assessments were conducted.
MachineLearning,"Hi, could you elaborate on the scheduler you mention? Is it like a bash script that starts running 4-5 different python scripts? Or something else? Also, if it's possible, could you share any resources/GitHub links for the same?"
MachineLearning,[deleted]
MachineLearning,"I probably have the worst workflow but here goes

I have a repo that consists of a library I’ve been writing for the type of models I’m studying, as well as the specific models/algorithms for my thesis project.

Then I have some notebooks for testing the code on my local machine, and if everything works I push the repo to my school’s cluster and get CUDA errors"
MachineLearning,"Coming from the Ops side, it's best if you know what you are doing on the DevOps side if you want to get anything done (unless you are at a large corp). 

Any disconnects in knowledge can waste time for both parties.

I personally don't enjoy helping out data scientists who can't perform basic bash commands."
MachineLearning,Look into GitHub actions. The docs are decent so should be able to follow along.
MachineLearning,"90% of the real work ends up being software eng. So while things run you can work on the other elements like data cleaning, containerization, git automation and CI/CD...

I transitioned from data science to Ops after being so frustrated with the slow progress of most ML projects."
MachineLearning,"In the real vector space interpolation is a given, no? Perhaps I'm missing why you wouldn't expect to be able to."
MachineLearning,"You can have a look at aws ground truth for this, or have a look at this https://github.com/heartexlabs/awesome-data-labeling"
MachineLearning,Training data is the ethical dilemma. Where do you get it? Who would provide it knowing it feeds an algorithm that provides them no royalties? Why would they?
MachineLearning,"Life is funny, so many people including me are struggling to get into a top ML PhD program. However, even after getting in you still face these problems..."
MachineLearning,"yeah, from what I know actually the average salaries after bachelors masters and PhD is not very different in CS (something like $100k vs $120k vs $140k) correct me if I'm wrong. That's why I heard if you want to earn money, PhD is not a wise choice."
MachineLearning,"A new M1 mac will likely run all the software you need in terms of normal software. I just bought one of the new mac minis, and it's great - except many of the development tools I use are not as easily supported. Docker only has a development build available, and the only python 3 version available is 3.9, which is the latest, but then poses a lot of problems with other packages. If you're doing data science I'd check out if the tools you'll be using (eg tensorflow, torch, or whatever) and make sure they all supports M1.

I found that 90% of my daily needs are compatible, but that 10% was still enough to be a deal breaker. As a result, I'm still mostly using my old laptop, and basically just waiting for the software to catch up to my new mac."
MachineLearning,Probably shouldn't buy the first generation of any new tech. You're not going to be disappointed by the Intel Mac.
MachineLearning,This is cool! Almost all NLP research is done on English and not enough in other languages. Do you have metrics to compare against other sentence splitters?
MachineLearning,Got it. Saw the video. Now it's clear.
MachineLearning,Does it have to be ML? Feels like maybe a dynamic programming optimization problem. Do you know all the chemistry scores or is there some feedback on how the squad did like win/lose? If you don’t have to actually build a model then it’s a search/optimization problem. Maybe I misunderstood the problem
MachineLearning,M1 is obviously the better processor/computer when when translating x86 apps. See the benchmarks.
MachineLearning,"Here's your first industry lesson: never use cutting edge technology for mission-critical work. 

One exception: if you enjoy AND have experience with this kind of stuff (e.g. you installed linux as a kid) AND have the time/money to cover mistakes, maybe get it and test it hard, for as long as the return policy allows."
MachineLearning,The Intel processor is a safer move given that your time is limited. You don't need to regret on M1 processor's performance because there will always be better alternatives after one or two years.
MachineLearning,"https://discord.gg/mvreqHrNMq
This one supposedly doesn't expire."
MachineLearning,Is there a permanent invite link?
MachineLearning,"This whole thread had me concerned.

1. I'm unsure how you've gotten to a stage where you're running experiments in your final weeks. At that point experiments didn't matter it was writing and compiling all my results instead. I came to the thread about to say, I think latex works fine, so you don't need to worry, only go find you're still running experiments.
2. I'm unsure how a final year PhD ""just runs scripts"". I was past that in early undergrad - if you're not aware of how the code works, it's probably not important to your research and if it is, I am gobsmacked. 
3. And you've been running ML workloads on your laptop this entire time. Most unis have computers and compute clusters, let alone the ability to rent AWS instances for arbitrary workload. 

Given your situation, absolutely do NOT go for an M1. You don't seem to have the knowledge to debug issues and if you're doing ML, there's no guarantee that it will work on your M1 since it's mostly implemented in  C and I don't have much confidence in your ability to rent a Linux instance and run your code."
MachineLearning,"There *is* a way around but it’s a little weird. I think you need to use a dock/hub and then you can get more displays. I’ve seen it mentioned a decent bit. 

That being said, there are some rumors the newer ones will support more monitors, which seems sensible."
MachineLearning,Do you have a list of the tools you need to finish your PhD? The list of things that work on the M1 is pretty well published now
MachineLearning,Has anyone heard back from Apple teams based in Santa Clara?
MachineLearning,[deleted]
MachineLearning,"The projects I have open sourced were old ones, kind of a mess honesty. I'll probably make few projects public in a few months"
MachineLearning,What do you mean by that?
MachineLearning,[deleted]
MachineLearning,"Here are a couple for you, but there are plenty out there depending on the extent of your project/ambition:

Underwater Raspberry Pi Drone/Sub

https://www.raspberrypi.org/blog/raspberry-pi-underwater-camera-drone-magpi-80/


Autonomous Raspberry Pi Drone:

https://www.hackster.io/korigod/programming-drones-with-raspberry-pi-on-board-easily-b2190e"
MachineLearning,"The m1 macs don't seem to support three separate displays (two external monitors and the built in screen) and it seems like that's not going to change soon. Ive gotten really used to my current setup and and was looking for a drop in replacement. Is there a way around that you are aware of? Also, a smaller deal is that starcraft doesn't run on m1 yet, which is how I break up my dev work, though that probably will change."
MachineLearning,"Had the same thought -- m1 seems like a much better deal (sameish performance for half the price) but I didn't want to have to risk any software not being compatible, so I just got the 16 inch with Intel."
MachineLearning,"Hmm, maybe this is why my old Mac has run into the ground tbh ahah"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[deleted]
MachineLearning,Decent stepping stone to the insanely high-paying NYC/Chicago trading jobs.
MachineLearning,Bad move. M1 delivers more perf for less money. Return it if you can.
MachineLearning,"I avoid running things on a laptop at all costs. Both for home and work, I have desktop rigs with GPUs that I ssh into. If your data is fine in the cloud, a Colab notebook can run faster and for longer than I would want my laptop screaming away for anyway.   


I'm sure there are cases where training models on a laptop makes sense, I've just never encountered any."
MachineLearning,"If only I had that option, but I've got a thesis to finish. Pretty sure they will be providing some sort of tech set up though, although I'm guessing that I'll want my own laptop as well anyway? Idk..


Yeah, I've just been watching alot of benchmarking videos on YouTube but none of the tests they show seem that reliable ... 


So If I'm training and running networks regularly, you'd reccomend avoid the M1, and maybe go for a gaming laptop or smt?"
MachineLearning,why do you say that?
MachineLearning,1.1 seems too low. 1.2 looks like it might work though.
MachineLearning,"I think this depends on what kind of distribution shift you expect to see. Do you expect to see NEW kinds of images or a shift in distribution of the same types of images? Another thing to keep in mind, is can the shift in distribution be attributed to some seasonal trend?"
MachineLearning,"Maybe ask whoever you'll be working for? They may be providing you a laptop or know better what their workflow is.  
If you're running anything reasonably large actually on your laptop, anything with a dedicated Nvidia GPU will run circles around either Apple laptop. I briefly looked into TensorFlow benchmarks on the M1, and it was only tiny models where the M1 was competitive. It might be due to low memory latency or something like that. Anything compute dominated will still run faster on a GPU."
MachineLearning,"Exactly. I've only tried 2, so not sure if 1.1 is enough."
MachineLearning,"You mean like this?

&gt;  normu = torch.nn.functional.gumbel_softmax(self.normu.view(1, 8192, -1), dim=-1, tau = 1.1).view(1, 8192, 64, 64)"
MachineLearning,Anyone know if the Nvidia CMP GPUs will be any good for ML?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I would like describe my approach which I use in a professional setting doing research.

I use a statefull approach separated it in 3 sequential stages:

1. Data stage:
    a. Register Data Class: this class register the raw dataset and meta data.

2. Pipeline Stage:
    b. Register the Data class and metadata of the dataset in the PipeLine Class. It also saves relevant metadata as properties associated with all transformations such as feature engineering and selection, etc.

3. Model Stage:
    c. Register PipeLine class from the previous case into the Model Class and run experiments.

Every state generate its log separately. Data logs, Pipeline logs, Model logs.

This method allows separation of concerns and traceability at every step. You can create your own configuration file in json or dictionary form. Also, when you are done you save serialize the model info as a dictionary or binary json with metadata associated with the type of model deployment for example it would be saved as:

{
  ""name"": "" My model"",
  ""author"": "" John Doe"",
  ""schema"": {
                     ""var 1"": int,
                     ""var 2"":float,
                      ...
                     },
   ""model"": &lt;model object&gt;,
   ""date"": ""2021-02-22 hhmmss"",
   ""risk"": ""low"",
    ... etc.

}

so when you save the model is almost ready for production with metadata for tracebility.

Another good think is that for dev purposes you can save serialized the complete model class with all loaded data or pointers to the data for posterity if you ever wanted to know about where the final saved model came from."
MachineLearning,"From what I've seen (including speaking with profs and students at UCL) the difference in the number of CDT fundings for international students vs domestic students is pretty stark. I think that's okay since the US also has a lot of domestic only funding. I also understand that as a faculty you probably have way better insight into the matter than I do. However, the perception that it's harder to apply for PhDs in the UK vs the US for international students is one that other international students who I've spoken with have shared. I feel the key reason is that in the UK you get admitted for a Ph.D. and get funding in two separate steps versus in the US where it's just one step. I've known friends from India who joined Imperial and Oxford (albeit not in ML: mathematics and mechanical engineering respectively) for a graduate degree without funding and only secured scholarships one-two years into the program. On a positive note, it seems like the UKRI fundings will be opened up to international students from this year, which should help attract more international Ph.D. applicants.     

Furthermore, outside of the top 4 (Oxford, Cambridge, Imperial, and UCL), there's really maybe one or two other places doing ML research and publishing at top venues. In the US, there are over two dozen other great universities outside the top 4 doing amazing ML research (NYU, Georgia Tech, UT Austin, Columbia, Cornell, UPenn, Harvard, Princeton, state universities like UW, UCSD, UIUC) and regularly publishing at top venues. The number of choices available in terms of research areas and universities biases a lot of international students towards the US vs the UK.  

For me personally, the reasons to prefer the US or EU for a Ph.D. versus the 
 UK would go beyond funding. For example, the ease of free travel and post-degree job location options would be a big factor for me not considering the UK as a Ph.D. destination as seriously as when you guys were with the EU."
MachineLearning,"Subjective best, not objective best.

But thanks for the sentiment!"
MachineLearning,"You can get rid of the white spots by increasing tau (temperature) in the gumbel\_softmax function args under the ""Latent Coordinate"" header."
MachineLearning,Mac is selling mbps both with intel and m1. I just bought an Intel model.
MachineLearning,They have a reputation for being a terrible place to work iirc.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You can checkout r/datascience they do a yearly salary thread. Not exactly the same but can give you a good approximation
MachineLearning,"So you'd suggest getting one of the old MBP's? I'm not set of getting the M1, but since I need to get a new laptop anyway, it's deffo something to consider.. 

Also, my PhD is in Physics, so not directly in the field, but yeah, ngl I should pay more interest in the hardware"
MachineLearning,"Implementing a Tacotron like encoder/decoder with attention mechanism (i.e. input character sequence, but output in another domain).  The output actually looks reasonable, but it is not following the encoder character sequence.  When I examine the attention weights, they are all stuck at the beginning and it is not learning to slide along.  Has anybody had similar difficulties implementing a classic attention model?  I've triple checked the input sequence, which is one-hot-encoded, and no issues there.  I'm thinking the decoder  hidden state is getting stuck maybe so it's not learning anything, or it's just learning to ignore the input sequence altogether."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Don't mess with what's working; it will be a huge distraction for no benefit.

That said, if you're working on a PhD in the field, you are expected to know about hardware, not ""just run scripts and hope they work."""
MachineLearning,"by specific types of finance, do you mean trading firms?"
MachineLearning,so would you say that AI for trading is one of the highest paid jobs in AI?
MachineLearning,"clueless me ranting loudly seems to have done the trick to get to the actual answer

thanks so much! that makes things much clearer! &lt;3"
MachineLearning,Tell that to the high school teachers in those areas...
MachineLearning,"https://www.reddit.com/r/ProgrammerHumor/comments/ls5wao/side_projects_be_like/?utm_medium=android_app&amp;utm_source=share

That post was literally above this one lol"
MachineLearning,"HFT &gt; big bank &gt; maybe insurance &gt; old school finance firm &gt; medium and small bank &gt; credit union

All will say they’re in the financial industry."
MachineLearning,"Sure. I have a collection of images that people captured in which they interact with a product they bought, and a textual description of an experience that the person had with the product (for example, an image of a person wearing a red shirt, and then a sentence that this person wrote about that shirt). I'd like to train a model that learns to generate the text from the image. Overall I have a dataset of about 30K images, and a collection of few sentences associated with each image.

I could use some up-to-date models (e.g, this one: [https://github.com/aimagelab/meshed-memory-transformer](https://github.com/aimagelab/meshed-memory-transformer)), but all those I looked into require pre-processing step of features/bounding-boxes generation. The problem is that I can't use an off-the shelf bounding-box extraction model as it would not perform well on the dataset I have (images are not like COCO at all). So I was wondering if there is a relatively up-to-date architecture that I can use that will not require this pre-processing step. That is, an implementation that requires only inputs (images) and outputs (sentences)."
MachineLearning,"Exactly. A financial analyst in a proper finance firm isn’t the same as a financial analyst at a bank, but banks like to say they’re part of the financial industry doing financial analysis and AI. It’s more confusing when some large banks actually do proper finance, and smaller banks have finance departments that just kind of plug and play old 1995 style risk models from a vendor. The former more closely aligned with HFTs and computational predictive analytics and research, the latter just an excel monkey."
MachineLearning,Well they have to pay you that much so that you can live there.
MachineLearning,"4 + 5. yes 9am - 5pm, is possible especially if you purposefully block times in order to avoid potentially counter-productive meetings"
MachineLearning,Links please! I would love to play with one of these.
MachineLearning,"Not true! We have (some) CDT funding for international students at UCL, and also other funding sources (e.g. ORS)."
MachineLearning,"PhD student, Deep Learning research. We have an HPC in my advisor's office we have access to with 4 GTX 1080 Ti which I run my experiments on.  

There has been great advice so far from other people, I will not repeat them. Here are my advice that hasn't been mentioned so far yet:

Start with the what point you are trying to prove in the paper (i.e. the hypothesis). A paper is usually made out of multiple hypotheses related to your research question. Each hypothesis will then have a few summary statistics or tables or figures that helps the author prove their point. Sketch these out first. Then backtrack from there and think about the Dataframe (in tidy form) that you would need to generate that table/figure etc. Then think about each row of that DataFrame and that brings you to the experiment you need to run. From the perspective of functional programming, every experiment is a ""pure function"" (of course if you ignore the data you persist). It takes some parameters and data, then it outputs the exact same thing (for reproducibility reasons. An experiment is a directed acyclic graph of operations. Now here comes the most important part, and highly opinionated part:
- The graph should be as modular as possible, meaning that you need to break down an experiment to its atomic parts. I'd use a process flow diagram here before I write any single piece of code. You figure out so many nuisances as you go through that process end-to-end.
- Each node in the graph should be tested rigorously before running batch experiments.
- If you can, log every intermediary result in each node. Because you might need it at a later stage. Now that's obviously not feasible because of storage constraints. This is why a good idea is to give priority to the loggables of the slow steps. Such as during model training, logging model parameters, optimizers states etc.

For building experiments as a DAG, I suggest [Metaflow](https://metaflow.org) from Netflix. I like the ability to resume if I make a mistake. Make sure you tag your runs so you can always filter runs that had a flaw in them.

About parallel running, I use [GNU parallel](https://www.gnu.org/software/parallel/). As long as I write a sensible argparse, it saves me a lot of headache. But I've learned the hard way that I need to build up the pace very slowly. I usually run only a few experiments in the beginning, see if the lead measures make sense. So it's like depth-first-search, not bfs. If not, I go back and test, test, test. Make sure everything is fine. I just find it very depressing that I delete 100 runs just because I made a very stupid silly mistake."
MachineLearning,Wait forever. They won't release it.
MachineLearning,"we're exploring how to do this with CLIP right now. fixed labels are easier, but we're also testing free-form sentences. can you describe your use case in a bit more detail?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,My friend got accepted to one of those schools without having any first-author paper at a top-tier conference. He is an international student. NLP field though.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,depends on the healthcare company. I get paid very well in ML for healthcare but again will vary company to company. Gov yeah thats gonna be low salary
MachineLearning,"I think hes saying quant firms and HFTs (e.g. Two Sigma, Citadel) pay AI researchers the big bux. And you shouldnt confuse them with with old school finance"
MachineLearning,The captions are like free-form sentences (not fixed label set). So this is more like an image to text generation task (which is why I'd like to use a more advanced text-related models like transformers and not LSTMs).
MachineLearning,"DeBERTA-large (1.5B parameters, score=90.3) marginally beats T5 (11B parameters, 89.3) and Roberta-large (355M parameters, score=84.6). Good to see we are trending to better perf with ""relatively"" smaller models"
MachineLearning,[deleted]
MachineLearning,"Working on something else is important to prevent me from watching the loss/reviewing the code. Otherwise, I will inevitably stop the training to update a hyperparameter here or there."
MachineLearning,do you know what zoom uses now for background removal?
MachineLearning,"how complex are your images and captions? specifically, are your captions part of a fixed label set (e.g., black shirt, white shirt) or do the captions need to describe images like a copywriter might?"
MachineLearning,"Do you mean to say that some people who work in AI for finance earn a lot, while others don't earn that much?"
MachineLearning,"Well now we ofc know gpt2 is not what good. Gpt3 is not good too. Better for sure.

But when they just made this model and showed a few selected generated texts. They said it was too dangerous. We need to discuss everything. Think about the consequences.

So what? Gpt3 advised the person to kill himself.

Don't seem like openai discussed it well lol.

They only talk about the danger and the consequences. But in fact, they make the same model, only bigger.

And they will take the money and make another bigger model.

This all sounds like it's just marketing.

Look, we've made an AI that's too dangerous to share.

Only here and now only 10 cents for access to very dangerous AI.

When they don't share the model, they just postpone the problem.

Because some competitors will also want to sell gpt3 api.

People are already working on training it.

They will release the model to the public.

Then I can take a virtual machine from Google and do all sorts of horrors.

You said the business would use a bad and biased model.

They are doing it right now.

I don’t know if they moderate API (some bad and biased moderating model at best) and what such terrible thing can be done with a text generator.

&gt;All I'm saying is, I don't know, and probably you don't know either, how impactful GPT-3 can be. So we need to be cautious.

Less, than gradient descent for sure.

Good old convolutional models are being used by the Chinese for repression right now.

And the Chinese have enough datacenters and scientists to make any terrible models."
MachineLearning,I just found this thread. Do you have any recommendation for books that focuses on statistics &amp; math (linear algebra &amp; calculus) that's specifically **for data science and machine learning**?
MachineLearning,"That is some funny coincidence. Also this cake day is due to the paper I mentioned.

Yeah, universal consistency is a way of proving that a ML model is ""sensible"" and you can even prove convergence rates for certain classes of true functions $f$, but unless you know more about the properties of the function/distribution to learn, you can't know *a priori* whether a certain amount of data is sufficient to achieve a good performance. This follows from a ""No Free Lunch"" theorem. So you'll have to test your model on a test set anyway, regardless of whether you have some consistency guarantee or not."
MachineLearning,"&gt;You die without food.

Without the internet, I can't work, can't get money, can't get food.

Without electricity farmers cant work.

I will die without a lot of things.

&gt; The only thing we can do is to understand food better and make it  healthier, which is what we have been doing, according to the science. 

Yes, kind of.

Read about the crop selection methods of the 20th century, when they just used radiation to mutate new species.

Now the situation has improved for a bit.

I think this is a normal situation for new technology.

Many people have died or been injured by x-rays for example.

&gt; What do you want to say here? Because we allow food poisoning, so it is okay that we allow AI issues? 

We can't allow or disallow it.

Just use and see what problems this leads to.

So far, this buzz about bias seems overrated to me.

I have not heard of any real problems with this.

&gt; Food industry is not the cause of the virus, and even it is, it's unrelated to what you said before regarding food chemicals. 

Coronavirus appeared in the Chinese market, where they sold different animals without basic sanitary rules.

It's part of the food industry.

&gt; My whole point is the potential harm of AI in the future. Of course you will say ""it only kills two people now"". 

Honestly, sounds like a potentional alien invasion.

There are real problems on this planet.

&gt; But why are they safe now? Regulation and clear rationale behind each of them. 

It's not safe, it's just better than in past.

People reduce the damage from using technology, it's a natural process."
MachineLearning,Don’t mistake finance and working for a regular old bank. Anything smaller that Chase or Cap1 and you’ll barely see a return on your education costs.
MachineLearning,"I'm an engineer so my advice may only apply somewhat. But for me the answer is infrastructure.

For 1, setup an experiment tracking framework. I found Sacred to be helpful https://github.com/IDSIA/sacred.

For 2 + 3, setting up a workflow that will allow you to deploy and monitor several experiments simultaneously should help with this. What I did is setup a CI/CD workflow that can be managed from Github or one of its equivalence. Basically each experiment is a branch in my Git repo. I then use Gitlab's CI/CD features to build the relevant files for each experiment into a Docker image. I then deploy that Docker image to my training infra. My training infra consists of deploying a containerized Ray cluster to GKE, which trains a Tensorflow model. But you can use AWS Sagemaker or Google AI Platform if that's too complicated. The trained model then gets saved to cloud storage, and its URL is saved in Sacred. All testing and evaluation metrics are saved there as well. 

For 4, yeah I only work about 8 hours a day. The key is to automate tasks that are time consuming. For example, one thing DL practitioners often waste time is constantly checking if their model trains are running correctly. This is something you can automate. Setup a Slack alert that will send you a message if the train ran correctly or failed.

A setup like this let's me prototype new changes and test them fairly quickly in the cloud."
MachineLearning,"Do not monitor experiments, that is a waste of time in most cases. Just let them run until completion and look at the results at the end."
MachineLearning,"1. Any good task manager, trello will do.
2. Code one, let it run. While it is running code another. Never had a problem with long verification times. If there is a bug in the code, it should crash pretty quickly in less than  a minute, if there is a problem with data/configuration just let it run till the end and check up on it after. In general if something takes more than a minute to run, I will just switch tasks and come back later.
3. ? What do you mean? This question makes no sense to me.
4. Yes, but you need to be organised with your experiments. If an experiment takes 4 hours to run, you need to have 4 experiments ready before you leave work, so that when you come back in the morning you have results ready to analyse.
5. [http://karpathy.github.io/2019/04/25/recipe/](http://karpathy.github.io/2019/04/25/recipe/)  
I sense that your experiments are not very organised. I would recommend using a configuration approach, where each experiment can be described by config such as [https://github.com/facebookresearch/detectron2/blob/master/detectron2/config/config.py](https://github.com/facebookresearch/detectron2/blob/master/detectron2/config/config.py), see [https://github.com/facebookresearch/detectron2/tree/master/configs](https://github.com/facebookresearch/detectron2/tree/master/configs) for example of usage. Most experiments should only require changing parameters in main config. For experiments that require code changes, use git branches to try and if they are successful implement them as config keys.

I would say running one experiment per day is pretty productive if they are sufficiently different. If you are just trying different augmentations or datasets than it is probably a bit slow."
MachineLearning,"Until the ""keeping fit"" I didn't realize it wasn't math exercises. I must really be off the deep end..."
MachineLearning,A few downsides though: 1) html 2) css 3) javascript.
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Working on it. Now more seriously, suppose someone generates in the future a cast of virtual porn actresses (and actors I suppose) what is the ethical dilemma (if any)"
MachineLearning,"Whoa, Getting more productive in deep learning research is basically my crusade - I could go on all day discussing this, but this thread's timing is not so good. I will sum up, and any interested researcher may hit me up (I'm LSTMeow almost everywhere) to get a private 1-on-1 lecture or get pointed to existing materials.  


**I call this ""Research MLOps"":**

1. Track everything
2. Automate what you can
3. Orchestrate your automation
4. Build interfaces (pipelines) between the different roles you switch back and forth from.

1-4 should be doable with 0 changes to the research code that already works. 

All the DevOps and infrastructure work to make this possible should be somebody else's problem."
MachineLearning,The pretentiousness of it is unbearable
MachineLearning,I run my experiments and play [krunker.io](https://krunker.io) in the browser
MachineLearning,"Yeah, they whored themselves out for a language model but they definitely will open source an AGI model (I know they will never achieve it, but still). I am not saying they are bad people, just hypocrites, and banal with their greed. Their research is way less ground-breaking of what they think it is, but that it is usual  marketing."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"They are the ""DPRK"" of ML. The name a complete oxymoron given their behavior."
MachineLearning,I would guess it’s pretty similar across the board except for specific types of finance. The most important factor is not subarea of ML research but suburban area ;) aka in SF and NYC your total compensation &gt;&gt;&gt;&gt;&gt; elsewhere in the US or world
MachineLearning,"Code up your experiments during the day, run them at night when you're sleeping anyway. Each experiment runs in its own (rented) VM. Set up your environment then image your VM and deploy said image to as many VMs as need be. The various parameters are ingrained in each VM. You can get fancy and automate all of this, too."
MachineLearning,"I really like this thread. I feel like a genius :) after discovering this sentence ""We all just do our best and hope our best gets better with time."" - our best getting better when it's already best :) :) ROFL. I forgot about my imposter syndrome for a jiff."
MachineLearning,"The new paper from OpenAI

&amp;#x200B;

 Zero-Shot Text-to-Image Generation: [https://arxiv.org/pdf/2102.12092.pdf](https://arxiv.org/pdf/2102.12092.pdf)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,They probably noticed that every time researchers release their code into the wild it turns into dozens of shitty monetized mobile apps.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Web dev baby
MachineLearning,"Hi! Just wanted a clarification here. I observe that the stats for NFNet (something I haven't come across before) is high in pretty much all aspects. For example, #FLOPS are really huge nos. Does that mean that that is more preferred due to a large number or is it less preferred because it has high complexity? Also, is it suitable to be executed in normal apps in normal PCs or does it require high level complexity &amp; operational capacity and only suggested to be used in Research Areas? Just wondering. Thanks 🙂"
MachineLearning,It's no surprise that there are a lot of people that *want* this to be true. Doesn't make it true.
MachineLearning,"What kind of questions are you looking to answer? Are you trying to information retrieval to retrieve documents from a database or actually querying and reading fields from a database? Based on your example, it looks like you're trying to do the latter. The easiest way to do this would be to train a classification model to learn different intents for questions that a user would ask the bot. E.g. ""I want to know how much I earned last week"" -&gt; MONEY_QUERY_INTENT and then performing DB lookups from the intent detected."
MachineLearning,To be fair MSFT gave them half a billion dollars in azure credits to train GPT3
MachineLearning,"Well it’s more than just data loading, really. I don’t really want to have to write more than a few lines of code to get my data ready, so calling `DataLoader.from_file` fetches me the data and performs a random 70-30 split, for instance. Things like this where I know I’m always going to do several things together, I’ve bundled up. And for cases where I need to do something different, I’ve thrown in a hooks mechanism.

So my code pretty much looks like a call to `DataLoader.from_file`, followed by a call to the `Transform` class (pre-processing), and then an instance of a `Learner` class. I can focus on getting things done rather than the more routine stuff."
MachineLearning,"Good point, I agree with you that the opportunity cost of un-openness can be also great. However, that is based on the assumption that ""hobbyists"" and institutes make the equal amounts of contribution. I don't know how much exactly it costs, but API is not too expensive for companies and universities, who in my opinion make the most contribution in the field of AI, (considering the fact that many companies require researchers to at least have Ph.D. degrees. I'm only talking about AI research here, of course everyone makes different amount of contribution in different aspects.)

""Perfectly"" is a strong word I used, my bad. But there are progresses in quantify biases and interpretability in recent years, so I will not say it is completely impossible. We don't need a perfect bias-free model, but at least a transparent and mathematically verifiable model that minimize biases is achievable and important.

On the other hand, a big concern is the monopoly of AI without openness of sources. After all a company makes money, but this is my personal belief that hopefully researchers as a whole are striving to do right things."
MachineLearning,"Notion is a great way of keeping track of your tasks, todos and progress in a project."
MachineLearning,Why do you need your own package for dataloading? Wouldn't your framework of choice's data loader work well enough?
MachineLearning,"This thinking displays a status quo bias. The danger of not releasing a model is equal to the sum of all opportunity costs in all sectors in all nations. We should not underestimate that either.

&amp;#x200B;

Besides, your criteria ""perfectly get rid of all bias"" is beyond state of the art and probably can't be provably demonstrated outside of a long term evaluation of real world application."
MachineLearning,"Thank you. I have also found some papers where they present logic similar to what you said about causality/independence - mostly about unsupervised disentanglement. By tying the factors of variations to actions do you mean supervised learning to force previously known meaning onto each dimension? If its possible can you share the paper link. 

I am mostly looking for a good understanding of the literature so as to properly proceed with thinking about my problem - disentangling the factors of variation in time series.

Thanks again."
MachineLearning,"&gt;It is so bad that it is not even worth refuting.

I would agree if it was some blog somewhere.But given that it's a peer reviewed paper in Nature, I think it is worth taking seriously. I think it implies that there are a lot of people out there who think this way."
MachineLearning,"Would it be a good thing if anyone could access the most powerful models available, with no regulation?"
MachineLearning,notify2 and slack integration as someone else pointed out in other thread could be interesting things to test out.
MachineLearning,I use `notify2` for this usecase; a little popup letting me know experiment progress every so often and completion allows me to more or less ignore it for the duration.
MachineLearning,Writing one experiment at one time is I think major bottleneck on my productivity. That's really something I want to improve upon.
MachineLearning,"Do you have any of your projects open sourced? I am curious how you tag different versions of your model.

I also use git regularly, and do extensive logging. Slack idea is good - it's something I could experiment with."
MachineLearning,"Awesome, thanks Garrett! I think the best way for me to figure out how to best adapt this to my use case is to simply start tinkering with it. Cheers!"
MachineLearning,You have to warp the authors words by a huge amount to get to that argument. I don't think it's easy to conclude that was their argument from the paper.
MachineLearning,"Well, you are on a tight track generally. That was the first paper on the topic in ML. The idea is a little controversial because the dimensions are mostly arbitrary, and how does the system even knows the casualty/independennce? It is an interesting paper nevertheless.

 In my lab there was a paper that tied the factors of variation to actions to give it more rigorous treatment. In general, I am not sure what you are looking for, but I view it, as an attempt to tie our random abstract concepts to actual mathematics of perception."
MachineLearning,"Thanks again, these ideas are really helpful!"
MachineLearning,"Unfortunately, I can't follow this advice. I would become Mr. Olympia before getting my degree. :D"
MachineLearning,"And why can't computers be in the world? Computers are already in the world in so many applications, and there are plenty of machines interacting with the world that are controlled by machine learning.

&gt;I think this is an opinion piece without much substance. 

It's not though. It's a peer reviewed paper published in Nature. If this was just a random blog dismissing it outright would be right. But given that it's not, I think this view must at least be widespread, and it appears as though it just comes from a place of total ignorance to me."
MachineLearning," Thank you for sharing, this is wonderful for beginners"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Personally I think this comment of yours is a bit cherry picked, but I like the civil discussion.

&gt;Electricity and internet (neuronet models are part of it) are as basic as food. 

No, you don't die without electricity and internet. You die without food. My point is, whether toxic or not, you need to eat. The only thing we can do is to understand food better and make it healthier, which is what we have been doing, according to the science.

&gt; And just as I don’t know how my model works, I don’t know what my food was made of.  People are constantly dying from bad food or from car accidents. 

What do you want to say here? Because we allow food poisoning, so it is okay that we allow AI issues? Don't try to strawman you but I don't see what you meant.

First, it's incorrect that all models black-boxes, like linear models. Similarly, not all foods are unknown - and quite frankly, a lot of people know what ingredients they are allergic to. That's what I meant by ""we know food well enough"", unless you grab random things and stuff them into your mouth.

&gt; By the way, the coronavirus has happened due to the poor food industry. 

This is weird to connect coronavirus to the topic. Food industry is not the cause of the virus, and even it is, it's unrelated to what you said before regarding food chemicals.

&gt;So far, the number of people killed by neural networks is about two or something like that.  
&gt;  
&gt;You have much more serious reasons to worry about.

My whole point is the potential harm of AI in the future. Of course you will say ""it only kills two people now"".

&gt; We are surrounded by things ""pretty dangerous if you think about it."" 

I agree, we are surrounded by dangerous things, and I don't want to trivialize any of them. Scientists are solving dangerous things everyday, and today's life are the product of their dedication and carefulness. Sometimes we take modern technology granted and claim ""cars, internet, electricity"" are absolute necessity, forgetting the challenges and danger they exert at the beginning. But why are they safe now? Regulation and clear rationale behind each of them."
MachineLearning,"1. Usually it’s a grid of experiments, so I design my run order that way.
2. Depends. Does the total time taken exceed a couple of days? If yes, simultaneous execution, otherwise no. For the second part, I don’t. I might spend the time checking the code, but in general I like having plenty of print statements to make sure things are going fine.
3. Different terminal tabs lets me do a mental context switch. I have a similar pattern of code so sometimes I will borrow code. I’ve made a package for myself to help with common tasks. (`pip3 install raise-utils`)
4. Sorry, I can’t answer that as a PhD student :)
5. Make your own package for things like data loading. Shaved off an incredible amount of time."
MachineLearning,"I am currently working on multiple research projects. My productivity increased greatly by being efficient on automation, with least boilerplate coding and testing. When I start a new project, I build it so that it can be scaled to handle different experiments without my interference.

I have 3 parts in my experimental setup for a project(pytorch), 

\- utility codes ready in a modular fashion, usually a one-time setup to get the end-to-end ready

\- model selector file to access different model structures and/or hyperparameters 

\- model files, whenever I want to try something new, I define the model, change the tag in training, it's ready to go.

I keep the codes backed up with git. Along with the saved model, hyperparams, I store the model files, a log with the overall result, another log with all values, visual inspection figures if any etc. So if I want to do some inference or fine tuning or anything, I can do the prediction with a single tag. I send notifications to slack on errors or training completions, so minimum time spent."
MachineLearning,I train when it trains. I've started doing exercises every time I run an experiment. It kinda works like a regulariser by punishing me for making simple mistakes. I've wasted a lot of computation on useless runs. I now run experiments with much more purpose and less mistakes all the while keeping fit
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It’s not a perfect analogy, but there are some parallels between what that would look like and the recent electric grid failures in Texas. 

Under qualified people using tools they don’t fully understand to make consequential decisions that impact huge numbers of other people who don’t realize they’re fucked until it’s too late."
MachineLearning,How long before someone sets up a porn site with only generated porn?
MachineLearning,"Ah, misread! Yes the various text file lists and issue tracking schemes are good for that. A simple option if you're using GitHub is GitHub issues. This lets you track status of an experiment (whether it was run, etc.) and colleagues can contribute via comments, refs to commits, email notifications, etc."
MachineLearning,"Their name ""OpenAI"" is very misleading."
MachineLearning,"We have internal experiment tracking tool, so that is not something I worry about. My issue was keeping track of which experiments to run.

And yes, I have different virtualenvs for each project. Makes life a lot easier."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"There are a number of experiment tracking systems out there. mlflow, wandb, Guild AI, etc. (disclaimer I developed [Guild](https://guild.ai)). I would look at adopting one of those. While you can roll your own experiment tracking tool, there's just no point IMO.

Any decent experiment tracking tool will let you run and track experiments concurrently. This is not easy in the roll-your-own case, trust me :)

One challenge you always face when working on multiple projects at the same time is that of isolation. There are various ways to isolate your work. Obviously separate source repos is start, but that's trivial. Runtime/buildtime isolation is more challenging. Jupyter Notebooks can help as they run in explicitly defined kernels. Unfortunately it takes a little effort to use project-specific kernels/VMs with Jupyter Notebooks as they want to use shared VMs by default.

As a pattern, I use separate virtual envs for each project. It's hard for me to imagine not doing this unless you have a common sets of frameworks that you're using. ML work tends to use a LOT of Python libraries that are ever-changing and that can cause painful dependency conflicts. Simple Python virtual envs, either conda or standard venv/virtualenv environments work well for this.

If you're doing your work in Jupyter Notebooks, keeping things separate is challenging. If you don't maintain separate copies for each experiment you need to implement herculean discipline to ensure that ongoing work doesn't impact running work. That's not remotely feasible IMO. So typical notebook based workflow is a bottleneck to concurrent experiments.

Guild's support for [Jupyter Notebook based experiments](https://towardsdatascience.com/reproducible-experiments-with-jupyter-notebooks-and-guild-ai-3bd3c0d84456) creates experiment-specific copies, which lets you freely edit your project notebook(s) without impacting running experiments. Again, you can roll your own with manual copies, but why?

As for working long hours, welcome to life in tech :) You'll spend a lot of time finding a healthy, sustainable balance. Experiment to find something that works for you and don't any one stage of life worry you too much. You can always make adjustments."
MachineLearning,"Good for teams, not for individuals. I prefer Dynalist."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Here's my approach: While you wait, try to develop different hypothesis for why your code won't work. Then, once your test is complete and you have the data, you can select a fix to implement."
MachineLearning,"Yes for deciding the order of experiments, I also like a Kanban board, like the other commenter suggested. There is a VSCode plugin that displays the content of a [TODO.md](https://TODO.md) as kanban board: [https://github.com/coddx-hq/coddx-alpha](https://github.com/coddx-hq/coddx-alpha)"
MachineLearning,"Yes I use it for keeping track of all my projects, to do and even personal items. The best part is the organization in terms of ""pages"", so you could divide your Trello into topics like home/work/ML/petcare etc along with richtext &amp; attachment. Its free to setup. Give it a try"
MachineLearning,Have you used Trello personally? Would love to hear your experience.
MachineLearning,"Trello provides excellent choices for Kanban boards, as well keeping reminders &amp; to-do lists"
MachineLearning,"Back to the gpt3.

Does openai even tried to reduce bias?

They just took a very large model and threw the internet at it.

And they won't open source this model because they want to get money from selling the api.

Likewise, they did drama queen with gpt2.

People replicated the model and made it public.

The world did not collapse.

So will be with gpt3."
MachineLearning,"I definitely also sometimes spend too much time just refreshing and watching the loss go down, but what I try to do is: code until I start an experiment, then do something else until that is done and forget about the running experiment, then look at the experiments again."
MachineLearning,"Electricity and internet (neuronet models are part of it) are as basic as food.

And just as I don’t know how my model works, I don’t know what my food was made of. People are constantly dying from bad food or from car accidents. By the way, the coronavirus has happened due to the poor food industry.

So far, the number of people killed by neural networks is about two or something like that.

You have much more serious reasons to worry about.

We are surrounded by things ""pretty dangerous if you think about it.""

Nothing new."
MachineLearning,"
I see you've posted GitHub links to Jupyter Notebooks! GitHub doesn't 
render large Jupyter Notebooks, so just in case here are 
[nbviewer](https://nbviewer.jupyter.org/) links to the notebooks:

https://nbviewer.jupyter.org/url/github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/0_liners_intro.ipynb

https://nbviewer.jupyter.org/url/github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/2_multilingual_translation_with_marian.ipynb

https://nbviewer.jupyter.org/url/github.com/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/NYC_DC_NLP_MEETUP/1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb

Want to run the code yourself? Here are [binder](https://mybinder.org/) 
links to start your own Jupyter server!

https://mybinder.org/v2/gh/JohnSnowLabs/nlu/master?filepath=examples%2Fwebinars_conferences_etc%2FNYC_DC_NLP_MEETUP%2F0_liners_intro.ipynb

https://mybinder.org/v2/gh/JohnSnowLabs/nlu/master?filepath=examples%2Fwebinars_conferences_etc%2FNYC_DC_NLP_MEETUP%2F2_multilingual_translation_with_marian.ipynb

https://mybinder.org/v2/gh/JohnSnowLabs/nlu/master?filepath=examples%2Fwebinars_conferences_etc%2FNYC_DC_NLP_MEETUP%2F1_NLU_base_features_on_dataset_with_YAKE_Lemma_Stemm_classifiers_NER_.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)"
MachineLearning,I like the idea of the notes.txt. I already take notes but they tend to be centralized. Your approach feels better for experimentation.
MachineLearning,"I'd agree about Brexit, but this guy being from the US would be a bit different, wouldn't it? Unless there has been a change regarding international students as a whole which I'm not aware of? 

Thanks for the point about funding, I wasn't entirely aware of that situation. So studentships aren't usually given for internationals? I'm in a CDT at the moment, and they have announced that the funding will from now on be available to international students, too. Though they may be in a minority."
MachineLearning,Interesting! So your approach is use some task tracking system like Kanban.
MachineLearning,Ok!
MachineLearning,This is pretty cool although def has an assumption being made about a strong correlation
MachineLearning,"1. Create a git repository for code tracking, then go to the Project item in the navigation bar. Create a new project board. I like to use the basic KanBan

2. I have a repo called self_study with 9 different project boards. One for physical fitness, one for AI/ML projects, one for chores/misc... Focus on getting todo tasks(like debug current project) and try not to stick to a hard schedule. Move tasks from todo to in progress based on which projects you want to get through in the moment. This is agile development methodology

3. With multiple project boards

4. I have no idea, but up untill now the source of all my productivity came from Adderall when I couldve been using a KanBan board

5. Tasks on a KanBan board should take anywhere from 1-12 hours.
Maybe a first task you can write would be ""Start TensorFlow tutorial on word embeddings"", and another could be ""scourge the web for ML textbooks and read for 30 minutes"" etc ...

Most importantly, no matter how few tasks you accomplish, establishing frequency is what matters. I may only work out 20-30 minutes but my KanBan board got me in the habit of doing it everyday, and im slowly adding more.

Bored in quarantine, I don't think ""what to do today"", I go to my board and pick one of the 9 projects I'm working on, and make a little progress.

Only downside is I've got my foot in 9 doors but I haven't really opened any of them. At least I remember where I left off"
MachineLearning,"I cannot address all of your points, but I will describe my current workflow for my Master's thesis, which I am quite happy with at the moment.

I usually code new stuff in a notebook first and I maintain a number of code snippets that I need to easily try something, i.e. loading a dataset into memory or performing a mini-training loop. Then I can implement a model until there are no runtime errors and it is able to decrease the loss on a very simple toy case, for example, I check if the model can overfit to a single batch.

If that is the case, I move code to the real code base (i.e. not notebooks) and write a config file for the new component (or many config files if the new component shall be tested in combination with other things). Then these experiments can run and in the meantime, I can do something else, e.g. work on the next component or evaluate models from previous runs, take notes, etc.

Some kind of systematic logging is also really helpful to avoid doing things again and again.

To switch contexts faster, I always have a \`notes.txt\` in every project where I write down the next thing I want to do at the end of every session.

I'm also curious about other people's workflow."
MachineLearning,"Got it, thanks!"
MachineLearning,"The problem with NFLT is that it's making a statement about all possible datasets. Most data we care about has properties in common and that significantly reduces the domain of models we're interested in. Especially within a particular data/problem domain, NFLT doesn't really apply."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"For international students, the UK became massively less enticing as a Ph.D. destination because of Brexit. The only way I would consider it now is through the ELLIS program. Most other sources of funding are not really available for international students, including CDTs."
MachineLearning,"What data source do you want to use. Image, video, radar, or accelerometer/gyroscope"
MachineLearning,"Learning C++ vs C# for AR/VR + ML? I have been learning C++ for machine learning and was planing on playing around with Unreal to start learning AR/VR. I noticed a lot of documentation and examples use Unity and/or C# for AR/VR applications, but the ML packages seem to only be for the ML.NET framework by amazon. Can anyone give advice on choosing a good path to play with ML in these environments? (less on the SLAM env building, but building ML to work with objects within the env, if that makes sense?)"
MachineLearning,"Hi,

I would like to create a program that collects and identifies current fashion trends. My first thought was to collect this information from instagram hashtags, but it seems that this is not allowed according to their ToS. Do you guys have any tips on where i could get data that has images of fashion trends that i could use, like models or influencers?"
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The issue with your assumption is that the Average Joe may more obviously use technology like this for nefarious purposes, with things like deepfakes and whatnot existing.

Not to put on too much of a tin-foil hat, but the real trouble comes from the things we don't even know is possible - things that companies like Microsoft and Google do, often behind closed doors."
MachineLearning,"For a current NLP perspective on the subject, check these:

[https://www.aclweb.org/anthology/2020.emnlp-main.703/](https://www.aclweb.org/anthology/2020.emnlp-main.703/)

[https://www.aclweb.org/anthology/2020.acl-main.463.pdf](https://www.aclweb.org/anthology/2020.acl-main.463.pdf)"
MachineLearning,"new social media for tech bros, like zoom but audio only, availble in iOS"
MachineLearning,"It's just a language model, not AGI, and that $1B could fund a lot of research."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,ClosedAI
MachineLearning,"**Beginner problems go elsewhere** according to the posting rules.

This sub may not be the best avenue."
MachineLearning,[deleted]
MachineLearning,An AGI built by a simple evolutionary algorithm wrote a paper on why AGI is impossible. Ironic.
MachineLearning,[https://www.theguardian.com/technology/2021/feb/17/clubhouse-app-invite-what-is-it-how-to-get-audio-chat-elon-musk](https://www.theguardian.com/technology/2021/feb/17/clubhouse-app-invite-what-is-it-how-to-get-audio-chat-elon-musk)
MachineLearning,"&gt;except that electricity kills people immediately, and AI can harm society without people having a notice.

Then take food chemicals or something."
MachineLearning,"Sure, except that electricity kills people immediately, and AI can harm society without people having a notice. When you notice the harm, it’s pretty too late isn’t it - we already are using everything that embedded with bias.

Don’t forget that Tesla spent quite a time convincing people to change from DC to AC."
MachineLearning,"I don't know for sure that the changes aren't public, but I'm assuming they were not because the behavior was still present when I wrote that comment."
MachineLearning,Tom scott made a video about this check it out on youtube
MachineLearning,"If you want a readable intro, I'd recommend [this summary](https://www.scottaaronson.com/democritus/lec15.html) of Scott Aaronson's lecture on the matter. Probably doesn't go as far in depth as you'd like (and tbh I don't know anything about either subject beyond what's in the lecture), but I enjoyed it."
MachineLearning,"ah

Yeah I did see the update (my comment didn't make that clear) but I didn't know his updates weren't public."
MachineLearning,"I bought that line for GPT-3, but I think their reticence to release more than this for DALL-E undermines that story. They're just not releasing stuff because they want people to pay for it."
MachineLearning,"The developer has reportedly fixed the white blotches issue (see update in the post), but as of this writing these changes don't seem to have been made public yet."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I tried it out, but I'm getting white blotches?

It's a real shame they're not releasing DALL-E in its entirety. I'm imagining it'll be like GPT-3 and they'll do an API eventually but..."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Never applied to CS PhD....but what is involved in the application? In biological sciences, in my application essay, I very briefly outlined a research thesis with very quick descriptions of experiments I would run and techniques I would use. I would link that back to my previous experience. In this case the PI basically was happy to take me on because he knew I would be basically doing my own stuff. I'm not sure if that is kind of allowed in CS PhD applications, but that might also be a method? Also I name dropped a lot."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"assuming you can't build a simulation environment on the cloud server, and have to deploy it in some kind of real life setting to collect interactions, with off-policy agents, you can do something like

1.  collect a bunch of data from the deployed environment using the current agent
2. train/update agent with collected data on a server

and repeat the process. an alternative may be to go fully offline rl which would be the same thing, just without repeating the process"
MachineLearning,"**Really interesting project**.

 Do you know if someone worked on AWS equivalent for **budgetml**?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Clubhouse may refer to:


== Locations ==
The meetinghouse of:
A club (organization), an association of two or more people united by a common interest or goal
In the United States, a country club
In the United Kingdom, a gentlemen's club
A Wendy house, or playhouse, a small house for children to play in
The locker room of a baseball team, which at the highest professional level also features eating and entertainment facilities
A community centre, a public location where community members gather for group activities, social support, public information, and other purposes


== Film and TV ==
""Clubhouses"" (South Park), a South Park episode
Clubhouse (TV series), an American drama television series
Mickey Mouse Clubhouse, a Disney TV series


== Music ==
Club house music, a form of house music played in nightclubs
Club House (band), an Italian dance-music band
Clubhouse (album), a Dexter Gordon album


== Other ==
Clubhouse Games, or 42 All-Time Classics, a compilation game for the Nintendo DS
Clubhouse Model of Psychosocial Rehabilitation, a program of support and opportunities for people with severe and persistent mental illnesses
Clubhouse sandwich
Clubhouse Software, a company producing a team project management application for software developers
Clubhouse (app), an invitation-only audio-chat social networking app for iPhone


== See also ==
All pages with titles beginning with Clubhouse
All pages with titles containing Clubhouse

More details here: https://en.wikipedia.org/wiki/Clubhouse 



*This comment was left automatically (by a bot). If something's wrong, please, report it.*

*Really hope this was useful and relevant :D*

*If I don't get this right, don't get mad at me, I'm still learning!*"
MachineLearning,What's Clubhouse?
MachineLearning,"I'm not in this field,but i'm happy that you did that job ,very good!"
MachineLearning,"&gt;It has been tough to discuss this without the full mathematical formulations, even during the last episode of Karpathy &amp; J.C.Jonson on Clubhouse

Thats incredibly exclusive ):&lt;"
MachineLearning,It's decent. I have used it for a similar project (tweets and comments)
MachineLearning,I also updated the post with a tweet from the developer on progress in eliminating the white spots in output images that often happen with the current version of the notebook.
MachineLearning,"&gt;Talkwalker

Thanks a lot, but unfortunately most of these seem to be non-free solutions"
MachineLearning,"Didn't know about that, I'll check it out, thanks a lot"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I trust microsoft more than some average Joe
MachineLearning,True. I updated the post for clarity.
MachineLearning,"Im one of the makers so I am biased, but I'd recommend [Humanloop.com](https://Humanloop.com). You can use it for free as an individual and as you label it will train a sentiment model for you. It will also select the highest value data to label so you minimise how much labelling you have to do."
MachineLearning,Can you build interface for current model for users to be able to use it and ability to manually override decisions made by model? Corrected wrong predictions are forwarded to model as new training data.
MachineLearning,"so instead of that let's let a giant multi billion dollar company that is only interested in profits abuse it. yeah, nothing that can go wrong there..."
MachineLearning,"Hi everyone, I need some opinions from you. How do I train Reinforcement Learning Agent in Cloud GPU (Google Colab, Azure or GCP)? 

How do I pass the observations/results from the environment after specific action to the Cloud GPU? 

Thanks in advance 😊"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,What else would you do with it?
MachineLearning,"Happy cake day!
Thank you, I had something like ""consistency"" in mind but didn't knew the word.
Your paper helped me understand/get an idea of the concept of consistency quite good.
So basically we need to train the NNs and hope for the best, if it doesn't work we can change the architecture/hyperparameters and repeat till we get good results.

Fun fact: I am studying at the Universtity of Stuttgart."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Is there a result of casual language modeling such as gpt?
MachineLearning,"I can help you I will rewrite the research for you if I can put my name in it as second author 

I need it For my studies application 

I can help you in many ways"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I'll join you. Where should we head? My criteria are simple: 1) a lot of money 2) little work.
MachineLearning,"I see what you mean.

I think I have an explanation here.

PCA itself is unsupervised, and the authors use labels only for evaluating the PCA. I think it is okay, though I'd prefer not to use labels directly, but use PCA features in the model and see its performance on the validation dataset. You can consider PCA simply as a tool for feature engineering.

As for clustering, there is an elbow method to select the optimal number of clusters: [https://www.scikit-yb.org/en/latest/api/cluster/elbow.html](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)"
MachineLearning,Thank you for your answer! It helped me.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"No school sets a hard cutoff of having 3+ first author pubs at top conferences for acceptance into the PhD program, let alone combined with co-organizing a conference workshop. There might be two or three students total who can pull this off, but not the hundreds that eventually make it in to the top schools every cycle.

Please don’t spread fud. Imposter syndrome is already prevalent enough, no need to add to people’s anxieties."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hi, I""m a senior high student, I've done a research on predicting the binding affinity between medicine and protein with ML models, if you're interested you can just send me a message."
MachineLearning,"I think the statistical concept you are asking for is (universal) consistency. Universal consistency (i.e. convergence for ""all"" probability distributions / true functions) is known for some methods like k-NN and SVM, but for NNs only in the case where you can find global optimal of certain underparameterized NNs, which is hard in practice. Also, universal consistency requires to let the number of neurons grow with the size of the training set. I wrote [a paper](https://arxiv.org/abs/2002.04861) on a setup where NNs are not universally consistent because the optimization gets stuck in local minima too frequently. Maybe especially the introduction of this paper helps to give you some context. For most practical NN setups, it is not known whether NNs are universally consistent.

Regarding your second question: Even for regression, it is possible to converge to discontinous true functions $f$ if you measure the approximation quality in a $L\^p$ norm for $p &lt; \\infty$. This is because the space of continuous functions is dense in these $L\^p$ spaces."
MachineLearning,"&gt; I guess making lots of money won over altruism.

Works every time."
MachineLearning,I am an undergraduate student and have taken ML recently. I wanted to do ML research that could lead to a conference publication. Please guide me what should I do as in what kind of papers to focus on etc. Thanks
MachineLearning," Calculate the loss function of Bayes classifier of 2 classes with the same a priori probability. 

I need some help with this."
MachineLearning,"Dall-E share notebook example
*Happy.
Only d-VAE
*My disappointment is immeasurable

Still waiting"
MachineLearning," 

**Sentiment analysis** is simply the method of identifying positive or negative sentiment in text. It’s frequently used by businesses to distinguish sentiment in text data, measure brand reputation, and listen to their customers.

So, to assist you I have compiled the list of **7 best sentiment analysis tools** based on my experience-

1. **BytesView**

Their sentiment analysis tool, helps you easily gather text data from multiple sources like reviews, suggestions, opinions, social posts, opinions, support queries, and convert it into useful insights that can help in making data-driven decisions.

**Link** \- [Sentiment Analysis - BytesView](https://www.bytesview.com/sentiment-analysis)

You can also train it to support and analyze 30+ languages, just get access to their API and integrate it with your system, also you can train custom sentiment analysis models too with the data related to your enterprise to further increase the accuracy of the result.

You can effortlessly analyze public sentiments, conduct market research, gauge brand reputation, and evaluate user experiences if you are a data analyst.

The results and insights from the text data are then shown on the analytical dashboard. You can easily understand the conclusions from the data and use the understanding of the text now to develop strategies.

2. **Rosette**

Rosette is also great for international businesses because it can review text-based data in over 30 plus languages. This makes your work easy you won't have to translate conversations before you upload them, which is not only faster but ensures better precision.

Its structure is built to analyze the text in the language that it's written, so you won't lose any valuable feedback even if it's written informally.

3. **MonkeyLearn**

MonkeyLearn is a sentiment analysis tool that is known for its customization. All you have to do is create tags then manually highlight different parts of the text to show what content belongs to each tag.

Over time, the software learns on its own and can process multiple files simultaneously

4. **Brandwatch**

Brandwatch is also one of my go-to analytics tools as it comes to sentiment analysis. It examines brand sentiment shows trends and has a really cool feature called “image insights”.

The feature recognizes images associated with your brand’s logo in the same way that topics can be linked with your brand’s name.

5. **Lexalytics**

Lexalytics is an intelligence solution that examines different kinds of text. Lexalytics works with social comments, surveys, reviews, and any other text documents.

In addition to sentiment analysis, the tool performs classification, theme extraction, and intention detection, which can make it easier for users to see the expanded context.

6. **MeaningCloud**

MeaningCloud is also multi-language sentiment analysis. It follows the aspect-based approach for sentiment analysis to find which topics are discussed positively, negatively, and neutral.

It also allows you to create a dictionary to add valuable vocabulary to its system.

7. **Talkwalker**

Talkwalker is another great tool for sentiment analysis too. It claims to have the best available sentiment analysis technology, which enables it to distinguish sarcasm and other unclear forms of undesirable mentions.

This tool works best with your social media channels as it can tell you exactly how people feel about your company's social media accounts.

***I expect this answer might have helped you***

***Thank you***"
MachineLearning,Because it would be misused by people
MachineLearning,"I m just a ML beginner, so is something like BERTweet better than VADER to be used as a pretrained model for sentiment analysis?"
MachineLearning,"In my previous job, we pretty much tried all of them, but decide to go with labelbox for most projects because I worked for a large company with a ton of experts to label the images - easier than training outside folks. LabelBox had good options to customize the UI but useless if you didn’t have good devs. For NLP, I really liked Prodigy because of its integration with SpaCy and active learning capabilities. Overall, from a cognitive load perspective (on annotators), active learning is amazing no matter which annotation tasks you are going through. Feel free to DM me.

Edits: typos"
MachineLearning,"That makes sense, thanks for all the insights! I think I'll focus more on getting a part-time RA role keeping my current job. This will atleast get me some direct experience and another academic rec."
MachineLearning,"I say this quite a bit, but do consider the UK. I believe there is less emphasis on publishing during undergraduate here, and a US degree will be looked upon favourably, too. I'm wholly unremarkable with just a little above average grades and managed to get into a pretty damn good Robotics PhD program  (which encompasses many AI-based labs, such as vision, swarm intelligence, a Bayesian Networks lab, etc), and I can guarantee you are far more capable than I. 

I'd wager that you'd be able to get into Imperial College London, Edinburgh, Southampton etc which are all very good for AI, and I reckon you have a good chance for Oxford/Cambridge too. Just something to consider."
MachineLearning,"Yes I agree ; it’s good to also look at the universal learning literature which is implausible in an asymptotic rather than finite sample way , as a counterbalance."
MachineLearning,"&gt; It’s pretty dangerous if you think about it.

Like electricity."
MachineLearning,"Ok I forgot to mention that I have no training set, I need to have an already pretrained tool to label new data"
MachineLearning,That's what it looks like when you have to compete against *the rest of the world*. What did you expect?
MachineLearning,for a company claiming that their main goal is for AI to benefit all of humanity they're way too closed imo. also the fact that they sold exclusive rights to GPT3 to microsoft doesn't help in that respect. i guess making lots of money won over altruism.
MachineLearning,"If you've wanted to use DALL-E for artistic purposes you'll still have to wait, as they only released the VAE and this notebook can't replicate the results shown in the paper."
MachineLearning,"Although BERT based models take care of good extent of preprocessing, it's still a good idea to consider balancing the two classes in your training set."
MachineLearning,"Using github might be suboptimal. I would just setup a makefile to submit batch jobs and bind a hotkey in my vim. Otherwise you could also set up a CI on your platform to automatically pull changes to your cluster and to submit a job there. This way, you get rid of 2) and 3).

Remote shell is called ""SSH Interpreter"" in pycharm."
MachineLearning,"that doesn't sound very open if you ask me. what they've been doing the past few years goes against their own mission statement:

""OpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity. ""

how does it benefit all of humanity when you exclusively license it to one of the biggest tech companies on the planet?"
MachineLearning,"It doesn't steer dall-e, it steers the discrete VAE used in dall-e.

Very cool nontheless"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Don't dismiss that completely: machine learning can be used to fill the gap between directly computing a function and looking up the result from a hash table: [http://www.theorangeduck.com/page/machine-learning-kolmogorov-complexity-squishy-bunnies](http://www.theorangeduck.com/page/machine-learning-kolmogorov-complexity-squishy-bunnies)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The thing is, those are, in a sense, the same problem. People just don't want to explain their Math, because it makes them seem smarter, though it's a detriment to the quality of their writing. I blame Hegel."
MachineLearning,Oh wow i didnt notice this was such a recent update. Crazy.
MachineLearning,These cuDNN changelogs are much more interesting than i would have expected haha. 🙏
MachineLearning,"Yes, they provide some fundamental limits of ML, but they are very commonly misunderstood. They make some assumptions that are not really valid in real life, such as test and training data are independent and features and labels are also independent. Andreas Muller, of sklearn fame, goes into details in his blog: [https://peekaboo-vision.blogspot.com/2019/07/dont-cite-no-free-lunch-theorem.html](https://peekaboo-vision.blogspot.com/2019/07/dont-cite-no-free-lunch-theorem.html)"
MachineLearning,"No, approximation won't converge to the true function. If the size of the network is fixed, there's a hard bound how small approximation error can be. If you want better approximation, you need a bigger network. Approximation error could be made to converge to zero in principle, but only if you keep network size increasing infinitely. However, network size normally doesn't change during training, so perfect reconstruction is impossible.

There are methods that are guaranteed to converge to local minimum, but global convergence is much harder problem, especially for such highly non-convex functions as neural networks. Basically you have no guarantee that local minimum is any good.

Now second question. Classification function is categorical function, that is, the value of this function is an element of a finite set, it's not even a number. Neural network is a continuous function. How do you compare them? There are many ways to do that, but cross entropy loss is probably most often used for classification. NN produces a vector of probabilities and loss function tells how well they match training data. Loss function is smooth and we don't try to fit it to anything, we *minimize* it."
MachineLearning,"This is so sad, there are so many people eager to try creative things with this model"
MachineLearning,"as others have mentioned, focus on a specific part of the ML/AI chain, and offer a service around it, there are so many pieces of the puzzle"
MachineLearning,"In the figure showing the peformance for the different versions of YOLOv5, each version of the model is shown with 7 datapoints. Does those corespond to different resolutions of the input image and if so, is it stated which resolutions are used?

see figure here:  
https://github.com/ultralytics/yolov5"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I have heard about this kind of gullibility like, in 2010. It is incredibly sad that in 2021 there are people who are still completely uninformed about all the violations of privacy and human rights coming from tech giants all around the world. And you even believe that you can actually win a lawsuit agains companies that are so much of a crony that they don’t need to even pay taxes. Really, you should do a minimum of research before posting."
MachineLearning,"Thanks, I used a slight modification of [https://github.com/pvanberg/flux-beamer](https://github.com/pvanberg/flux-beamer)"
MachineLearning,Step 1: start researching.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,First you need to collect the data with various gestures then you can process the data and feed into a neural network
MachineLearning,"It’s could be some business reason, but it could be also very likely because GPT-3 has not perfectly got rid of bias and other sorts. Once you open source such a powerful tool, companies are going to use it. It’s pretty dangerous if you think about it."
MachineLearning,[deleted]
MachineLearning,I honestly thought this post was being sarcastic until the very end. Damn.
MachineLearning,"Feeling tired is not just about the device, it is more importantly about your overall self-care routine and screen time.

Before changing your current device, consider using 'f.lux' to reduce the 'temperature' of the screen brightness, and/or use 'blue light filter' glasses. It will make a big difference.

Also - Try to stop reading/working on-screen at least 2-3 hours before your sleep time. Take breaks, drink water regularly, eat healthy food (good for eyes) - and such regular stuff.

Since 1995, I must have spent more than 8 hours a day of screen time, on average. I managed to keep my eyes healthy, by eating healthy and having some exercise routine - as much as possible. 

For the last 8-10 years or so, I started using f.lux - as much as possible. It is always on the lowest temperatures, by default."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning," Thank you for your reply. I should really have said that the torch module cannot be found after I've installed it (so not quite an installation issue). A simple test returns: 

Traceback (most recent call last):

File ""pytorch-geometric-test.py"", line 1, in &lt;module&gt;

import torch

ImportError: No module named torch  


I installed using pip (20.3.3). The torch installation is 1.7.1 and torch-geometric is 1.6.3.   


I have seen other users having similar issues and the advice has always been to use conda instead. However, I am hoping to not use conda if I can help it."
MachineLearning,"Thanks for the help!  
So Gazebo with ROS it is!

I already used CUDA to render the mandelbrot ensemble. I also have some experience with OpenGL so CL should be fine. (It should be fine...)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks :) this worked
MachineLearning,American born Indian
MachineLearning,Off topic: I am curious about your username. An American knowing about relatively less famous part of Hindu mythology?
MachineLearning,I've had no problem with installing it on Mac OS. I just followed the official instructions and set CUDA=cpu. Everything worked fine. It would help if you told us the specific version of libraries you are working with.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"[Text-to-image Google Colab notebook ""Aleph-Image: CLIPxDAll-E"" has been released. This notebook uses OpenAI's CLIP neural network to steer OpenAI's DALL-E image generator to try to match a given text description.](https://www.reddit.com/r/MachineLearning/comments/ls0e0f/p_texttoimage_google_colab_notebook_alephimage/)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"alternatively

`
z_logits = enc(x.to(dev))
`"
MachineLearning,"Just a guess. Some conferences take into account whether a submission comes with code or not. The repo and code might be have been put up to help the submission get past reviews, then subsequently removed once they are past."
MachineLearning,"Mathematically, OR should cover ML. OR solves optimization problems with or without constraints. ML is an optimization problem. However, it is not efficient to develop ML while doing an OR PHD. The problem context and technique skills would be very different."
MachineLearning,Use cpu instead of cuda
MachineLearning,Beautiful slides. What latex template are you using?
MachineLearning,"Thank you so much for your detailed reply ! 

I am actually confused between switching job to a ml research role or carrying on my current role as a data scientist/analyst and try for a part time RA role at a university. Getting a full time paid RA role is difficult and my current job leaves me with sufficient time to pursue a part time role. No doubt both options are difficult. 

I also applied this cycle, however it doesn't seem things will go my way this year. I chose 2 academic recommenders and 1 current job supervisor (holds a PhD, however has been in the industry for 10+ years now). Not very sure regarding the how strong the letters were but they definitely would not have been bad. After reading your reply I feel I could have done better in translating my experience to fit more under ml modelling work in SOP. 

Do you mind sharing the field you are working in ? I plan to apply to model based RL methods for robot control."
MachineLearning,YES YES YES YES YES YES YES
MachineLearning,"Welp, I guess I'll switch fields."
MachineLearning,"No vid, but below are links to a tutorial and a checklist, focused in data mining, but pretty general.

[https://www.cs.ucr.edu/\~eamonn/public/SDM\_How\_to\_do\_Research\_Keogh.pdf](https://www.cs.ucr.edu/~eamonn/public/SDM_How_to_do_Research_Keogh.pdf)

[https://www.cs.ucr.edu/\~eamonn/public/ChecklistforRevisingaDataMiningPaper.doc](https://www.cs.ucr.edu/~eamonn/public/ChecklistforRevisingaDataMiningPaper.doc)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"OpenAI does not maintain its open source projects, just make the source available for people to use."
MachineLearning,"I'm getting this error on GPU run on colab:

 `Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _thnn_conv2d_forward` 

Any Idea?"
MachineLearning,"That's definitely true! Here are two things in particular that are different about us: 

\- Higher quality on complex projects, like hate speech and misinformation. We have a strictly vetted pool of workers who can complete complex tasks, including things that require outside research or good judgement, and can you the results back in hours rather than weeks.

\- More flexibility when setting up labeling workflows. You can create labeling projects with a WYSWIGY editor that provides the customization of something like Mturk without requiring you to write HTML. We have NLP/CV tools you can plug in as well.

We decided to build this platform after trying out several of the options at our previous jobs and not finding anything quite right. Have you tried any of the other solutions? Is there anything you liked or didn't like?"
MachineLearning,"Interesting. If you wouldn’t mind elaborating..

To continue with the anomaly detection: let’s say we are using PCA to explain some of the variance in the data, but we don’t want to explain too much of the data otherwise the anomalous observations won’t be able to stand out as observations whose variance is not well explained.

The performance metric for PCA is variance explained. We want it to explain at least some variance (otherwise all observations would be poorly explained and thus look like outliers), but we don’t want the PCA to explain 100% of the variance (otherwise there are no observations to be investigated as outliers). The book then goes on to find the sweet spot (at about 96% of variance explained) by evaluating it using the labels.  And again, it compares the PCA model against other algorithms by comparing the labeled fraud that is caught.

Is there something I’m missing? Even if we used a neighbor approach like KNN I still am not quite understanding how K is chosen to optimize that the outliers are actually the right outliers. So what’s the right approach?"
MachineLearning,"I found this annoying at times, not only writing that is over complicated things, but also the math is confusing sometimes,"
MachineLearning,write research for publishing in conference or journals
MachineLearning,"&gt;eading good research papers and other literature is crucial to becoming better at writing.

I agree, that even great result but badly written will not be publishable. Sometime, I found trivial idea got publish because of great writing. Thanks for the link"
MachineLearning,"Issue: Any plan on releasing the text encoder?   
[https://github.com/openai/DALL-E/issues/4](https://github.com/openai/DALL-E/issues/4)

So they basically release a d-VAE (not their contribution), not the DALL-E. Welp, nice one, OpenAI for close door research."
MachineLearning,"Title:Image Completion via Inference in Deep Generative Models  

Authors:[William Harvey](https://arxiv.org/search/cs?searchtype=author&amp;query=Harvey%2C+W), [Saeid Naderiparizi](https://arxiv.org/search/cs?searchtype=author&amp;query=Naderiparizi%2C+S), [Frank Wood](https://arxiv.org/search/cs?searchtype=author&amp;query=Wood%2C+F)  

&gt; Abstract: We consider image completion from the perspective of amortized inference in an image generative model. We leverage recent state of the art variational auto-encoder architectures that have been shown to produce photo- realistic natural images at non-trivial resolutions. Through amortized inference in such a model we can train neural artifacts that produce diverse, realistic image completions even when the vast majority of an image is missing. We demonstrate superior sample quality and diversity compared to prior art on the CIFAR-10 and FFHQ-256 datasets. We conclude by describing and demonstrating an application that requires an in-painting model with the capabilities ours exhibits: the use of Bayesian optimal experimental design to select the most informative sequence of small field of view x-rays for chest pathology detection.  

[PDF Link](https://arxiv.org/pdf/2102.12037) | [Landing Page](https://arxiv.org/abs/2102.12037) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.12037/)"
MachineLearning,"Do not feel discouraged by this. Most schools are not like this, but those 4 (and some others) can have those tough requirements for a reason. All of my PhD friends and I got into our school (pm me or look in my comment history if curious) with NO publications and are all working with professors that have a very good reputation in our respective fields"
MachineLearning,"Finally! 🤩

It has been tough to discuss this without the full mathematical formulations, even during the last episode of Karpathy &amp; J.C.Jonson on Clubhouse - they alluded to the difficulties that they faced in its implementation… 

It’s frustrating to see such cool implementations and not being able to backtrack &amp; replicate those formulations"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I really hate academia.
MachineLearning,"Hi dear, great!!

I'm a master's student in Robotics. I've just started learning ML and all the stuff. Would be super happy to be mentored and to collaborate on some projects."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"At top universities, you essentially need a professor to say that ""we want this student in my lab"". This can come from a lot of routes, including:

1. The letters of recommendation are extremely strong, particularly if the letters are from somebody the professor knows well. For example, many of my friends got into schools because one of their advisors were {advised by/advised/in the same lab} with the professor who interviewed them. This is probably the most common route.

2. You connected with the professor through some route, and he was personally impressed with you for one reason or another. In the best case, you work with your desired POI before you're even applying and impress them.

3. The professor knows and was impressed by some of your research. If none of the above were true, but the professor likes your published paper, this is also sufficient.

In my opinion, at the top schools, everything else is only helpful insofar as it allows for one of these 3 pipelines. If you publish a paper with your advisor it's likely to improve their recommendation letter. If you publish a good paper then it's more likely that your POI will like your paper. 

However, let's say you pump out 6 ok first author publications at top conferences that none of your POIs are particularly interested in, and let's say you come from an unknown university with advisors that nobody knows. You're gonna get rejected."
MachineLearning,Can someone ELI5 this to me?
MachineLearning,Yeah I had one very strong letter from a professor who has reasonable notoriety in this space e.g. regional neurips chair
MachineLearning,"wow

crazy"
MachineLearning,"Generally, there are two approaches:

* any unsupervised model has some loss or metric. We can tune hyperparameters based on the values of the loss/metric;
* usually, unsupervised models are later used in downstream tasks. We can tune hyperparameters based on the performance of the downstream tasks;"
MachineLearning,"Well, they sold it for Microsoft for 1 billion for starters..."
MachineLearning,"Check out work by Shai Ben-David. Specifically for measuring domains.  It's been a while since I dug into his stuff, but he references methods for measuring the ability of a model to represent the information it was designed to model (a measure of complexity?).  Interesting regardless. 

https://cs.uwaterloo.ca/~shai/publications.html"
MachineLearning,"I would write a wrapper that generates the Slum script as a part of executing the run. The Slurm script would then execute the real operation (e.g. training, etc.)

The run is staged in a directory on the remote system. As Slurm runs the job, the real script (e.g. training, etc.) will update files in that directory.

If you have more questions, hop on [https://my.guild.ai](https://my.guild.ai) and someone can lend a hand!"
MachineLearning,"These slides, videos and code are amazing...going through function approximation stuff now.  It’s clearer than Silver.  This guy is a great teacher.  I feel like I found a gold mine.

Thank you for this!  I’m doin a masters now and these undergrad lectures are gonna save me lol"
MachineLearning,[deleted]
MachineLearning,"Whoa, this looks to be exactly what I need...(?)
I'm not too sure how I would translate this to my use case in submitting jobs though? I cannot run my scripts directly, it needs to be sent to a scheduler (Slurm).

edit:

Oh wait... does that instead of running say, `example.py`, I run `submit_job.sh` (which has the command for submitting `example.py`)?"
MachineLearning,Could someone enlighten me why OpenAI archived GPT-3 repo?
MachineLearning,At least machine learning models have brought maintenance challenge 2.0. Data and the whole training process need to be tracked.
MachineLearning,My understanding was that the 'encoder' part turned the text into numbers and the 'decoder' part turned the numbers into an image. Is that not correct?
MachineLearning,"u/topsykretsz I created an example of how one might use Guild with a Hydra based script.

[https://github.com/guildai/guildai/tree/master/examples/hydra](https://github.com/guildai/guildai/tree/master/examples/hydra)"
MachineLearning,"&gt; 2. you may equally use rsync into ssh to submit jobs as a part of your job script

I don't think I understood what you mean. In particular, what does it mean to ""rsync into ssh"" to submit jobs as a part of my job script? 
For clarification, my current workflow: 

 1. Write code locally, push to GitHub
 2. SSH into cluster, pull from GitHub
 3. Run `sbatch myjob.sh` (Slurm command for submitting jobs to the queue) in the interactive node on the cluster.


&gt; In case job submissions are instant you may use remote ssh transparently in pycharm and many other python IDEs (I am assuming you are using python)

What is meant by ""use remote ssh transparently""? Does that mean I can, in some sense, run pycharm remotely?
I'm working in Julia using VSCode, but it does sound promising. I will look into extensions if they're available. Is there a name for this feature?

&gt; I just googled an interesting option to keep you connection alive across many consecutive ssh logins (presumably, including scp and rsync). This should make your remote shell initiation almost instant.

Thanks for this! That's also very helpful, anything to trim down the disruptive workflow :)!"
MachineLearning,Related news: [OpenAI has released the encoder and decoder for the discrete VAE used for DALL-E](https://www.reddit.com/r/MachineLearning/comments/lrroom/n_openai_has_released_the_encoder_and_decoder_for/).
MachineLearning,Thanks :). I created [this post](https://www.reddit.com/r/MachineLearning/comments/lrx40h/r_openai_has_released_the_paper_associated_with/) for the paper.
MachineLearning,"Title:Zero-Shot Text-to-Image Generation  

Authors:[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh%2C+A), [Mikhail Pavlov](https://arxiv.org/search/cs?searchtype=author&amp;query=Pavlov%2C+M), [Gabriel Goh](https://arxiv.org/search/cs?searchtype=author&amp;query=Goh%2C+G), [Scott Gray](https://arxiv.org/search/cs?searchtype=author&amp;query=Gray%2C+S), [Chelsea Voss](https://arxiv.org/search/cs?searchtype=author&amp;query=Voss%2C+C), [Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford%2C+A), [Mark Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+M), [Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever%2C+I)  

&gt; Abstract: Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.  

[PDF Link](https://arxiv.org/pdf/2102.12092) | [Landing Page](https://arxiv.org/abs/2102.12092) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.12092/)"
MachineLearning,"publications matter like 10x more. If you go to a garbage school but have many good publications you can go anywhere, if you go to the best school and have fewer or lower quality publications, you will not be able to do much."
MachineLearning,"I just more or less completed this path but towards ML industry rather than post doc/academia. I think either are possible but it does depend on your advisor. For example there's the theoretical optimization side of OR and there's a lot of work that needs to be done on generalizability bounds and guarantees for different ML systems. There are coordination problems in OR that RL can be very useful to analyze, the whole data science part of ML is extremely close to ML...etc

It really depends on how your PhD goes. A lot of people kind of let their PhD happen to them and their advisor chooses their projects...etc. But if you steer things towards ML you can make most things work.

If you really want to do ML academia I would say an ML Masters where you make connections with professors for a bit and try to transition into a PhD even before you finish can be a good idea. I know in my school people who did masters tend to graduate earlier than those who did not, especially if they got some research work done in their masters.

Also, I know people doing ML research that did their PhD's in really random application areas like BME, just with really heavy emphasis on applying ML to their BME application. OR is going to be much easier to do real ML work than that."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"If you didn't talk to the professors beforehand, I don't think you stood a chance."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I'd also mention that some applicants come with secured scholarships
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'd add to 1. that even without a bad latter, ""good"" rec letters might not be enough. What you really want is one prof who knows you very well and is an expert in the field say ""you should absolutely accept X, they have incredible potential to do good research."" Then when one of the prof's close colleagues/collaborators at another university reads it, they will fight for you to get accepted when decisions are being made."
MachineLearning,"[The paper](https://arxiv.org/abs/2102.12092 ""'Zero-Shot Text-to-Image Generation', Ramesh et al 2021"") is now also up."
MachineLearning,"Since no one has mentioned it, for (3) you can also reach out and ask the programs you were rejected from. You may not get a response but I know several people who have, and who then were accepted the next year. 

But yeah, it's a crap shoot, especially in machine learning. You may want to consider looking for programs that use machine learning (ie engineering, analytic sciences, etc) but that aren't explicitly ""machine learning"" programs, you may both have a better shot and actually land working on projects you're interested in rather than just being thrown in the general mix."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"First you got to understand how GANs work. GANs are not really used to improve classification performance.(i.e. it will not improve performance on flower classification) Note there's a difference between classification and discrimmination. Secondly if you look at the loss function for the generator, the generator is not shown the real images at all. Think of generator as a student stuck in a classroom having not see the outside world at all learning how to draw flowers. And the teacher(discrimminator) as some one who is seeing drawings of real flowers and fake flowers and telling the student why the fake flowers why their drawing is bad. The purpose of the student then is to fool the teacher that their drawing is indeed real. And the purpose of the teacher is to get better and discerning which drawings are real/fake. Through this cyclical process both get better."
MachineLearning,"Tons of vendors/solutions for labeling now. Everyone claims they are better than X (X=Mech Turk, Hive, Figure8, LabelBox, etc). What’s your point of differentiation?"
MachineLearning,"Maybe try ROS with Gazebo it can run headless too.
You should go deep into OpenCL and should know Cuda.
What Model?😊"
MachineLearning,"those results look pretty good, are other VAEs usually that good?"
MachineLearning,[deleted]
MachineLearning,"Did you list any professors in your statement or application? It could have came down to the professors you listed weren’t looking for new students. That happens very often at my university. My recommendation is to always reach out to professors you’re interested in before applying to see if they’ll even consider bringing you into their lab. As a side note, if you’re applying for PhD, it’s about advisor, not university. You could have an advisor that is top 5 in their field and goes to a #80 school, you’d be insane to not consider that."
MachineLearning,PhD
MachineLearning,Was this for a phd or a masters program?
MachineLearning,"Pretty good summary. There is the rigor of OR which makes it slightly less accessible. 

Also some academic types don’t like the Reddit / blog culture of ML."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'd guess one of two things:

1. You have a bad letter. If your advisor knows anyone at any of the places you applied, he can find out which one it is. When applying for faculty jobs, some people will apply to places they know they can get feedback on letters to fix for later applications.
2. There are one or two people at your university that are better candidates than you. From a top 15 place, the top grad programs might only accept 1-2 people."
MachineLearning,"I'm an American applicant and I received interviews from two schools, the most interesting being Stanford. Ultimately they rejected me. Going in I honestly wasn't expecting an interview (unfamiliar with this process) and wasn't that prepared. 30 minute zoom call also just seems like a bad format to figure out whether or not someone is a good culture fit or not...

But yeah it seems connections was really the important thing here that I was lacking."
MachineLearning,"1) Yes, standards are high + admissions is a very stochastic optimization process.  You can think of the admissions process as an MDP with extremely high variance in the state transition dynamics. Even the best policy may just fail catastrophically due to aleatoric noise. 

Having first author papers (that are not workshop papers) will get your foot in the door, but you need to find and convince a potential advisor you can do research with them. Did you receive any interviews? Are your LoR good, and more importantly, are your LoR writers connected with any potential advisors you want to work with? 

2) No, math competition is unlikely the tiebreaker. The top candidates probably had better connections. Connections &gt;= LoR &gt;= publications &gt;&gt; GPA, GRE, Math competition, etc. Also, international students are at a disadvantage to domestic students for US Universities. I know many labs and PhD committees that filter by US applicants first. 

3) Given that you already have a first author paper, you are already ahead of the pack. You should continue doing research, perhaps visiting at another lab and build more connections. Reach out to PhD students whose work you are interested in, and chat with them.  The best thing to have is a LoR writer who knows your potential advisors, and can strongly advocate for you. Remember that the grad admissions process is very stochastic, and don't take rejections from your dream schools as a failure on your own part.  Good luck!"
MachineLearning,[deleted]
MachineLearning,Brazil.
MachineLearning,"As Doug Hofstadter's protegee, you'd expect her to be big on analogies. I'll check the paper out. Sometimes I wonder how much emerging science I miss, not having a Twitter..."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"yeah i guess, i live in Malaysia"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,i would say like 65 %
MachineLearning,"That article is a complete load of codswallop.
It is so bad that it is not even worth refuting.

*Nature*, really?? Has it come to this? Why do you sink so low as to publish crud like this?"
MachineLearning,"Thanks for open sourcing the vision model! I am more interested in image and text similarity for ranking/retrieval purposes. CLIP has a text encoder and an image encoder which allows to compute cosine similarities. How can we achieve something similar with microsoftvision resnet50 as there is only a image encoder, e.g. the task that is optimized for text-image pairs (two [web-supervised](https://arxiv.org/pdf/1906.03219v1.pdf) datasets containing 40 million image-label pairs collected from image search engines.)?"
MachineLearning,"This might be of relevance: A paper published yesterday by Melanie Mitchell, ""abstraction and analogy making in artificial intelligence"". https://twitter.com/MelMitchell1/status/1364228937753133065?s=19

She suggests that analogy is an understudied field area of AI that will be essential to overcoming the narrowness of today's techniques."
MachineLearning,"Wikipedia is probably a good entry point. Both articles have references and suggested reading if you want to dive deeper into these topics.

* https://en.wikipedia.org/wiki/Probably_approximately_correct_learning
* https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension

You might also be interested to read about the manifold hypothesis:

* http://www.mit.edu/~mitter/publications/121_Testing_Manifold.pdf

While we're at it, the ""double descent"" phenomenon is pretty weird and flies in the face of our traditional understanding of the bias-variance tradeoff:

* https://openai.com/blog/deep-double-descent/

You might also enjoy this related investigation on the relationship between parameter count and the ""capacity"" of a model:

* https://openreview.net/pdf?id=rknJHfXBz"
MachineLearning,"I posted [this news](https://www.reddit.com/r/MediaSynthesis/comments/lroigk/for_developers_openai_has_released_the_encoder/) a few hours ago, but thank you for mentioning it anyway :). I don't have a lot of technical knowledge about this area, but hopefully this can be steered by CLIP just like BigGAN etc. already are by the apps in this list."
MachineLearning,"Try this expert search tool: [https://www.expertkg.com/app/](https://www.expertkg.com/app/)

Type in any technology like  deep learning, it will give you a list of experts and their recent work on that. Most results are masked though, if you are not their users. I only look at the unmasked part :-)"
MachineLearning,"Chaitin's work is really cool. I read Meta-math! in college and a lot of it went over my head, but I got enough to tickle my fancy. I should definitely revisit it."
MachineLearning,I am not! What should I Google?
MachineLearning,Multivariate Bayesian structural time series sounds perfect for your needs.
MachineLearning,"Researchers in operations research see machine learning as lazy. They view machine learning as throwing shit at the wall and see what sticks. I wrote a master thesis on Machine Learning applied to finance in a OR heavy faculty. We aced it, but not without a sneer or two thrown at us."
MachineLearning,Sounds like you might be interested in PAC learning/VC dimension type stuff. Are you familiar with these topics?
MachineLearning,It might be worth looking at Chaitin's work [https://en.wikipedia.org/wiki/Kolmogorov\_complexity#Chaitin's\_incompleteness\_theorem](https://en.wikipedia.org/wiki/Kolmogorov_complexity#Chaitin's_incompleteness_theorem)
MachineLearning,Openai just released the encoder for Dall-e. How can it be used for generating images?
MachineLearning,I would really like to chat. I am an undergrad and I am considering pursuing ML research. It would be really interesting to hear what you have to say about your experience
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The runtime speeds of EfficientNets have been improved with cuDNN 8.1.

&gt;EfficientNet performances have improved. Depthwise convolution is now optimized in NHWC layout in cuDNN 8.1.0. From EfficientNet, we see an average of 2.9x speed-up for 5x5 layers, and 1.7x speed-up for 3x3 layers. - [Release Notes :: NVIDIA Deep Learning cuDNN Documentation](https://docs.nvidia.com/deeplearning/cudnn/release-notes/rel_8.html#rel-810)"
MachineLearning,"You would be very interested in the work of the late Hubert Dreyfus, his primary focus, outside of teaching Heidegger, was to provide sophisticated, analytic criticisms of A.I., insofar as any ML project relies on the capacity of AI, his work remains relevant. Also google Sean Kelly, one of his prodigys"
MachineLearning,"Condescending, the word is condescending. Or patronizing.

But thanks for the informed discussion though, that much appreciated."
MachineLearning,"As others have said they are not exactly comparable. However it's worth mentioning one relation between them: when you train a model with parameters vector w, many training algorithms, and most notably gradient descent methods, can be described as a discrete (possibly stochastic) dynamical system. 

For instance full batch gradient descent would be described by the equation: w_{n+1} = w_n - learning rate * gradient of loss wrt w."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;No one is talking about representing functions.

&amp;#x200B;

&gt; function"
MachineLearning,No one is talking about representing functions. Polynomials of sufficiently high degree can approximate any continuous function defined on a compact domain. This is just the Weirstrauss approximation theorem.
MachineLearning,"&gt; The entire field of harmonic analysis revolves around trying to understand universal approximators

That's not true. You are really referring to the field of ""approximation theory"" sure, some applied/computational harmonic analysis focused on approximation rates with all the wavelet stuff, but there's a whole lot more to applied/computational harmonic analysis and actual harmonic analysis is honestly a different beast.

I say universal approximation is boring since classical methods are optimal in the sense of approximation (i.e., how many terms you need for epsilon approximation) for many classical function spaces. For example, Fourier series are optimal for L^2 based Sobolev spaces, wavelet type systems are optimal for Besov and Triebel-Lizorkin spaces.

There's some recent work about how deep nets do better than these classical methods, but the problem is less so much as approximation with deep nets, but rather what function spaces to deep nets beat in terms of approximation rate compared to classical methods. This is akin to how wavelets are better than Fourier series in Besov and Triebel-Lizorkin spaces, but in L^2 based Sobolev spaces, they perform the same.

I was being pedantic with my comment, but I dislike how people mention universal approximation without the discussion above, which is actually how universal approximation needs to be studied."
MachineLearning,Ok thanks!
MachineLearning,"If you limit yourself to the infinite regime then sure, it's not particularly interesting, but having meaningful bounds on the size of a finite system needed to achieve a particular approximation accuracy, or how much training data is needed, or what configurations have what growth properties are all highly non-trivial. The entire field of harmonic analysis revolves around trying to understand universal approximators."
MachineLearning,Actually I don’t use Discort but I opened an account so we can discuss there.
MachineLearning,"Polynomials of degree n can't represent a function of degree n+1, and if x is large that error is unbounded."
MachineLearning,"Here's ~~a good~~ one published in nature.

Why general artificial intelligence will not be realized - https://www.nature.com/articles/s41599-020-0494-4"
MachineLearning,"This a very interesting optimists take on ML and its limitations.

https://www.gwern.net/Complexity-vs-AI"
MachineLearning,"Every non-boring function system is a universal approximator (polynomials, splines, Fourier series, wavelets/ridgelets/shearlets/curvelets, etc.). Universal approximation is actually a very boring property of a function system."
MachineLearning,"This is an interesting concept, but that Wikipedia article is in dire need of an edit."
MachineLearning,"Yes, Guild supports Hydra and other config file based schemes that use YAML, JSON, or Python INI files - though Guild only supports one such configuration file. You tell Guild to use a config file for an operation this way:

    train:
      flags-import: all  # can limit imports here if need be
      flags-dest: config:flags.yml

Guild imports default values and types from `flags.yml` in this case and rewrites `flags.yml` for each run with any modified values. The rewritten file becomes part of the experiment, separate from the original source file."
MachineLearning,"Look into using Ansible, sounds like a good solution for this use case"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Is guild ai compatible with hydra.cc? Hydra has become the standard way to configure code on many frameworks.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,More related theory: [https://www.cs.cmu.edu/\~vaishnan/papers/GAN\_memorization.pdf](https://www.cs.cmu.edu/~vaishnan/papers/GAN_memorization.pdf)
MachineLearning,[https://en.wikipedia.org/wiki/Ugly\_duckling\_theorem](https://en.wikipedia.org/wiki/Ugly_duckling_theorem)
MachineLearning,"They're different like apples an bicycles are different.

Something you're likely to see in a dynamical systems course: Here's the equation of motion for a pendulum. From it, please deduce the deduce the stable and unstable equilibria of the system.

Something you're likely to see in an ML course: Here's a dataset containing a million  cat pictures and a million pictures of other  things, as well as a bunch of ~~statistical models~~ algorithms that can be fit to that data build a cat detector."
MachineLearning,Hey. My man. You’re a good person.
MachineLearning,"I second the comment about reading things besides academic papers. Most of the people that write academic papers really just aren't good at writing, and exposing yourself to quality prose is going to require venturing outside of scientific research.

Anybody trying to understand what you're saying will thank you for writing clearly, instead of trying to make everything as terse as possible."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"To serve as a starting point, here’s Steve Brunton’s video on an intersection between machine learning and dynamic systems: https://youtu.be/0MNVhXEX9to I highly recommend his other videos as well, with many discussing data driven solutions to problems in control."
MachineLearning,"Thank you very much, appreciated!"
MachineLearning,Wherever the mind map takes you should be a good starting point
MachineLearning,"Thank you very informative, but speaking about the simpler one which one is more simple"
MachineLearning,No free lunch theorems are probably what you are looking for.
MachineLearning,"&gt; Having been an applied researcher for 2 years was probably the number one contributing factor for me landing on the short list for a Dean's fellowship in my PhD program. (Like 10 years ago now.)

Sorry but 10 years ago, admissions was far less competitive and there were fewer applied scientist roles in industry. 

Today. Amazon gives anyone the “Applied Scientist” title. It’s mainly used for applying existing research papers to company datasets. Not really “research” at all."
MachineLearning,"That stuff is very interesting (and I'm familiar with the basics), but I'm really interested in the general principles of drawing inferences from data (as opposed to computing arbitrary functions)."
MachineLearning,I'm not but it looks interesting. Thanks for the link!
MachineLearning,"Cool, thanks."
MachineLearning,Yes
MachineLearning,"Stick to the simpler stuff and your go to is guaranteed to be part of Scikit-learn, they have a useful mind map [here](https://scikit-learn.org/stable/_static/ml_map.png) that can guide you to selecting the right method."
MachineLearning,RemindMe! 1 Day
MachineLearning,Duckytown looks cool! I’m looking at the prerequisites and wondering if you think it’s doable for a 16 yr old.
MachineLearning,"You might want to explore the limits of computation. The kinds of things that can or cannot be decided using a ""Turing machine"" and the halting problem, etc. The types of problems that can't be solved by machine learning tend to be the ones that aren't computable anyway, though they do solve some previously intractable NP-Hard problems."
MachineLearning,"I am clearly biased but I think it is an awesome experience. Usually what happens is that you get to work with a large company of which there is an important problem they want to solve (e.g. optimizing warehouses, reduce loss in supply chain, personalized pricing etc), and you research to not only just ""use"" the latest ML research, but also modify/change/adapt it so that it is applicable to the current problem. Think perhaps technical consulting but much longer project deadlines (3-4 years) so you actually get to research and understand a problem deeply. Therefore, I would not think it is playing with other people's toys, more like redesigning toys so that they actually can work. E.g. one of my senior's paper here:https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3360622 . You get to see something that actually gets deployed in real-life while making methodological contributions. Sometimes your methodological contributions are not first-order, but sometimes they are! (You get used to how many headline grabbing ML methods don't actually work in real-life scenarios and you have to find a way for it to work)"
MachineLearning,"DuckieTown https://www.duckietown.org/mooc, is excellent, older versions used raspberry pi (newer ones are based on nvidia jetson). 

I really enjoyed http://ai.berkeley.edu/home.html, where you code agents to play Pac-Man. Generally RL in games if you’re into gaming."
MachineLearning,AAAI 🤮
MachineLearning,"I think a lot of it is not uploaded on dspace. ORC at MIT is actually pretty ML heavy these days with people like Bertsimas, Jaillet, Farias, Gamarnik and Mazumder all working on learning. Also, a lot of professors working on ML typically have two appointments - one in ORC and one in another department."
MachineLearning,"I think it's explained in Kilcher's video [https://youtu.be/TrdevFK\_am4?t=758](https://youtu.be/TrdevFK_am4?t=758)

Basically it's a trainable positional embedding associated with the position while in the usual transformer the embedding isn't trainable. It's like an usual word embedding in pretrained models."
MachineLearning,"A dynamical system is a system of equations that describe the derivative of a state space, given its current state. The interesting question in dynamical systems is ""how will this system evolve from some initial conditions?""

Machine learning is chiefly concerned with finding some unknown function, given a random sampling of that function's paired inputs and outputs.

Different ML methods basically make different assumptions about that function, eg, a linear regression assumes that the output is just a weighted sum of the inputs.

In the last 4 years or so, there's been a lot of work that combines ML and dynamical systems. The short version is, ""Let's assume our unknown function is some dynamical system, and that our inputs and outputs are paired examples of initial and final states of that system. Can we discover what that system might be from these observations?"""
MachineLearning,"Doing some information extraction tasks (e.g., NER, event extraction, relation extraction, coreference resolution, ...) but for the biomedical domain. 

Then apply your trained model to COVID literature and see if there is anything interesting."
MachineLearning,"I have a few ideas, but it might depend on my understanding of your question...  


You could take a feature map of your network, preferably on the layer before the classification, and use a knn.

[A naive approach...](//imgur.com/a/X7PZOJ0)  


If your algorithm generalizes well, the feature map should reflect that. If somehow your input data changes, you should notice the clusters getting wider, moving around etc...  


The issue is that you have to choose an arbitrary value after which you trigger the re-train.   


NB: Only comparing the position of the center of mass of your clusters may not be enough. if the cluster gets wider without moving, the center of mass will not move much, so you may also want to calculate the variance or something like that..."
MachineLearning,"tl;dr - All ML research experience will count in your favor (for the most part).

The truth is a bit more complex. I had a bit of industry research experience before applying to graduate programs and based on my experience applying and being on the other side of the admissions process I have a few thoughts and takeaways.

First, it's important to note that every professor will have their own take on what makes a potential students profile ""attractive"", so there is a lot of variation into what strengthens your application.

1. What type of research experience you have matters a lot. If you're applying for labs that focus on ""Bipedal Robots in Rough Terrains"", your industry research experience in computational advertising may help your case and may hurt your case. It may help by demonstrating your mastery of the mathematical concepts required to do this type of research, especially if your background doesn't previously display this. However, it may also hurt your application in the sense that you may be less familiar with current challenges, recent breakthroughs, and the current state of this subfield than an undergrad who has spent the last 9 months on a related topic. Some profs may value you bringing in external knowledge, some may worry about the ramp up time. I'd assume industry research closely related to your target area would be the best case.
2. Who writes your letters of recommendation matters a lot, (and what they say). Would your potential advisors in the graduate programs be familiar with the industry researchers who would likely write your letter of recommendations? Faculty value endorsements from people they know and people who know how to write a letter of recommendation in the manner faculty want to read it. Letters of recommendation from other professors are often written better and carry more weight. Will you be able to maintain relationships with faculty from undergraduate institutions, so that they could still write strongly in your favor? Would the people you're working with in industry know how to write a letter appropriate for admission into graduate programs and/or are they known by these faculty.

All of this to say continued success definitely helps your case. Success that is clearly applicable to your future academic research further strengthens your case.

&amp;#x200B;

Best of luck, also happy to clarify anything I said here."
MachineLearning,"Absolutely it does. Having been an applied researcher for 2 years was probably the number one contributing factor for me landing on the short list for a Dean's fellowship in my PhD program. (Like 10 years ago now.)

That is even without having published much during that period. 

Whether you get opportunities for conference or journal articles in R&amp;D depends on the organization. But demonstrating you can do the numerical analysis and numerical software development at a professional level is a big win in terms of admissions."
MachineLearning,As an OR grad student I see huge overlaps in OR and ML. I don’t think anyone really denies this. But there seems to be many in OR that fight the branding of ML. Calling similar methods something like statistical learning to set themselves apart from the ML community. I don’t fully understand why there is a need for the distinction. Why fight it if another community is producing beneficial work?
MachineLearning,"I agree, CNN filters act as template matchers. I think it is important to mention the contribution of pooling, which allows the model to identify that ""template"" even if it is translated a few pixels"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[deleted]
MachineLearning,"Professor in Analytics (so, ML applied in OR) here. As others have said, in OR we don't really focus on conference papers, focusing on journal papers. If you want to move to pure ML work afterwards this can be an issue for you. 

However, we tend to be much more focused in certain industries (I work in Fintech and Insurtech), and it has been my experience my students are preferred over ML graduates, as they have background knowledge of the field as well as knowledge of the methods.

My advice then would be to go the analytics route if you know what to specialize in and you have an advisor that's sufficiently well-known in the area. Otherwise, an ML route would be probably better for you."
MachineLearning,"Hi,

I'm planning on training Drones to fly in a 3d environment.I was thinking about using an already existing solution to do so, such as Unity/Unreal. But that's just because I don't know better. I feel like using game engines might be a good way to waste processing time on bloated software.

**What kind of software should I use to simulate a 3d environment in which I train a DNN/CNN (or whatever might work well for that purpose)?**

Should I go deep into OpenGL or CUDA an implement everything myself? **I just want to prototype.** My model will be trained on a simple NVIDIA GeForce GTX 980. So that's that..."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I didn't have any industry experience when I applied to my graduate program but I imagine it would be extremely helpful on your application, especially if you have any publications."
MachineLearning,"Universal Approximation Theorems might be interesting to you, classical works show that letting number of parameters go to infinity lets you approximate any function to an arbitrary degree of accuracy. However, they don't really tell you anything about how to learn such networks.

[https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1991.3.2.246](https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1991.3.2.246)

More recent papers improve these bounds, but also introduce assumptions on the distribution/regularity of the training data 

[https://openreview.net/pdf/a7214aa6dce0e71061cc0251ce2bc1f3d5c8f3af.pdf](https://openreview.net/pdf/a7214aa6dce0e71061cc0251ce2bc1f3d5c8f3af.pdf)

[https://arxiv.org/pdf/1912.10094.pdf](https://arxiv.org/pdf/1912.10094.pdf)"
MachineLearning,"Hi, what is it like there to work on the application side of ML, which I guess what I would be doing? 

Personally, I felt like I was lagging behind the ML stuff being currently done in the research world. Like playing with their toys only after ML researchers have tried them out. (excuse my weird metaphor). So I wonder if it was just me?

I could see several interesting recent theses that are using ML from MIT ORC, loosely judging from their titles."
MachineLearning,Yeah I agree with this post. It’s not the same and it’s different than ML.
MachineLearning,"⏩ Paper Title: BERT-QE: Contextualized Query Expansion for Document Re-ranking
⏩ Paper: https://arxiv.org/pdf/2009.07258.pdf
⏩ Code: https://github.com/zh-zheng/BERT-QE
⏩ Author: Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates
⏩ Organisation: University of Chinese Academy of Sciences, Amazon Alexa, 
Institute of Software, Chinese Academy of Sciences, Max Planck Institute for Informatics"
MachineLearning,"You're right, it was specifically the deep learning part that is/was debated"
MachineLearning,"Will definitely read that, thanks for sharing."
MachineLearning,Thank you for the great explanation!
MachineLearning,"Thanks for the link, I'll check this out. 

My understanding is that Thishby et al.'s results about neural networks (multiple phases of training, etc) are definitely up for the debate, bu the Information Bottleneck itself is still a mathematically sound structure relating the tradeoff between accuracy and compression when representing the relationship between two variables X and Y jointed by a joint probability distribution P(X,Y)."
MachineLearning,"There's lots of branching and often recursion in the implementations which doesn't work well on GPUs. OR methods on large problems also have high memory requirements and quite random memory access patterns, which also have made them a poor fit for GPU hardware.

Modern GPUs have much larger core counts and available memory capacity, and have more robustness to poor memory access patterns (although that is still a major issue when implementing these algorithms). Branching is still a big issue, as GPU threads are grouped together as SIMD units, so if one thread in a group branches one way all the others have to wait and then they can go the other way at the branch after."
MachineLearning,"I just started my ML learning journey, so it would be awesome to see what you are working on"
MachineLearning,"Current MIT ORC PhD. Would not recommend coming to OR to do ML, unless you are interested in doing the optimization brand of ML, which is much more theory focused (in fact closer to TCS in my opinion), or the application side of ML, which many faculty at ORC constantly do. 

If your interest is to develop new deep learning architectures or high-power machine learning methods, OR is not exactly the best place for you. If you want to develop new optimization algorithms or work on application of OR/ML in various industries, then coming to OR is best."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"well said, very elegant and clear abstraction."
MachineLearning,these are excellent analogies for GAN creativity. thanks for sharing.
MachineLearning,[removed]
MachineLearning,"Not an expert and probably wrong. The latent space is much smaller than the feature space. So to succeed the GAN has to develop an abstract model of “flowerness”, that captures the key features. The functional form of converting that latent space to new images is effectively smoother than the original data mapping of feature to latent space. So you can sample from that function and get flower images not seen by he GAN. 

Or something like that. Please let me know how I’m wrong."
MachineLearning,"what would be some papers for me to look at for this? if not papers, what would be some good key words to search on google scholar?"
MachineLearning,"Were they traditionally written on CPU, and, now that the GPU programming has accelerated due to Deep Learning, OR is lagging behind in terms of high-quality parallelized GPU implementations of its models?

Sounds like it's in high need :)"
MachineLearning,"so if your sliding window takes 50 passes/windows to finish processing your image\*. That's how I would phrase it. As for the 50 neurons thing, I wouldn't use neurons when describing convolutional layers. What you usually care about are (learnable) parameters, and in the conv layer the parameters are the weights of the convolution kernel/filter, which can be 3x3  (shape of filter) or some other number, times the numbers of filters (let's say 32 filters), so you would have 3x3x92=828 parameters. Asides from that then you would also add some bias, and that would add 1 extra parameter per filter for a total of 828+92=920."
MachineLearning,"I made a provisional server in case anyone's interested [https://discord.gg/6FwTT3Wr](https://discord.gg/6FwTT3Wr)

  
But someone would have to take over as admin for the server as I'm not that familiar with discord"
MachineLearning,"Edit: I made a provisional server in case anyone's interested [https://discord.gg/6FwTT3Wr](https://discord.gg/6FwTT3Wr)  
But someone would have to take over as admin for the server as I'm not that familiar with discord"
MachineLearning,"I’m working on a social app for creating image classifiers. It has built in automation features.

https://palapa.ai

https://apps.apple.com/us/app/palapa/id1518753364


We have an AutoML backend that needs work.

If you or anyone on this thread is interested let me know."
MachineLearning,"May I ask, how do you compute the FLOPS of a net?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It's really cool stuff. The wiki is as good a place to start as any:

[https://en.wikipedia.org/wiki/Information\_bottleneck\_method](https://en.wikipedia.org/wiki/Information_bottleneck_method)

But some papers that have really interested me are:

The OG paper:  
[https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf](https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf)

A neat application to neural networks:  
[https://arxiv.org/abs/1703.00810](https://arxiv.org/abs/1703.00810)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Right now is a really exciting time to be studying OR. My recomendation would be to learn how to program GPUs using CUDA, OpenCL, Vulkan Compute ect. The ability to write really robust and well optimized GPU implementations is one of the big skill voids in the OR community at the moment. But it is easily one of the most promising research avenues."
MachineLearning,Use jupiter notebooks to run and study the results right on the cluster node itself: https://josephpcohen.com/w/jupyter-notebook-and-hpc-systems/
MachineLearning,"Hi! 

This is one of my favourite topics on AI, although I'm barely an expert.

I know you didn't ask for an explanation of what XAI or Interpretable ML is, but this book is fairly complete and has a lot of contents, regarding ""old"" and new methods: [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/) This is by far the best recommendation I could give (togther with basically any work of professor Przemyslaw Biecek)           

I'm working on making large Bayesian Network even more interpretable and make the explanation generation more efficient and this paper was of my interest: [https://arxiv.org/abs/2012.05773](https://arxiv.org/abs/2012.05773) The research lab that elaborated this has a bunch of interesting stuff as well: [https://clarg.doc.ic.ac.uk/](https://clarg.doc.ic.ac.uk/)

This one is kind of a classic and doesn't include a lot of tehcnical issues, but I still think is very interesting: [https://www.nature.com/articles/s42256-019-0048-x?fbclid=IwAR3156gP-ntoAyw2sHTXo0Z8H9p-2wBKe5jqitsMCdft7xA0P766QvSthFs](https://www.nature.com/articles/s42256-019-0048-x?fbclid=IwAR3156gP-ntoAyw2sHTXo0Z8H9p-2wBKe5jqitsMCdft7xA0P766QvSthFs)

If you are interested on Deep Learning, I honestly don't know much more other than the more famous model agnostic methods, such as LIME and SHAP (already explained in the C. Molnar Book). Also, this one is abour reinforcement learning: [https://www.sciencedirect.com/science/article/pii/S0950705120308145](https://www.sciencedirect.com/science/article/pii/S0950705120308145)

I hope it is enough with this! Any doubt, please, tell me"
MachineLearning,Thanks for the paper. Will give it a go.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I would  be interested in collaborating on a NLP project!  Reducing bias in text is one if my favourite topics but I need guidance in it, and I'm fluent in English so I'd be able to help you too :)"
MachineLearning,I'm interested. I don't have any experience with ML but would love to participate and learn.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for the answer. Looks like, I might need to have a talk to propose some grounds beforehand."
MachineLearning,That would be a nice idea
MachineLearning,Thanks for the answer. That is almost my research experience although it's a different combinatorial problem :D
MachineLearning,"Are you familiar with LIME (Local Interpretable Model-Agnostic Explanations)?  
[https://homes.cs.washington.edu/\~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)"
MachineLearning,"This one is actually about explainable ML but maybe it can help you find some nodes that lead you to what you're looking for:  
[https://arxiv.org/abs/2004.14545v1](https://arxiv.org/abs/2004.14545v1)"
MachineLearning,Great idea!
MachineLearning,"I work with HPC workflows pretty frequently. I have setup an MLFlow server for logging and tracking experiments, artifacts, and data. I have ssh keys setup between the cluster and the server (in a simple case this could be your local machine and MLflow can be setup on a local folder rather than a DB). 

I don't even log into the cluster anymore. A python script dispatches a job to the PBS/SLURM scheduler on the cluster, and another script gets the results back. These Python functions can be integrated into a pipeline tool of your choice.

The end result is a single command line execution with my desired run parameters, walk away, get an email from PBS/SLURM that job is done, open browser and view plot artifacts on MLflow's dashboard.

While I usually have my own templates for PBS/SLURM jobs, Python libraries like Dask provide some built-in ones that let you dispatch jobs from your local machine and get the results back automatically. The downside for this (Dask-jobqueue) is that your local machine has to act as the work scheduler, so it needs to be running for the duration of the job. Upside is that everything is collected in your machine after. You never have to log into the cluster.

I recommend Dask-jobqueue as it is the easiest to setup and start working with right away. But if you want a more flexible system, MLFlow + pipelining is a decent option."
MachineLearning,"Everything exists somewhere on the internet. Though perhaps this is why you're having trouble finding this platform (or it's not popular):

* your basic search criteria is ""ML-powered"", but grandma doesn't know what that means
* you want a simple download and install for grandma

Mainstream end users are not looking for ML-powered software. They're looking for software.

I do like the idea of a crowd-sourced curated list of the most interesting and fun mainstream ready end-user ML-related software though."
MachineLearning,I don’t understand how this paper is different from regular mixup... the distinction between static and dynamic in sec 2.3 doesn’t make sense to me
MachineLearning,"There's some interesting work on SAT solving and other combinatorial optimization using differentiable sorting and ranking methods from ML. So there is some overlap and it's a really exciting area at the moment.

To give you my perspective, my PhD was in Computer Graphics and Stochastic Approximation, and I went on to publish work on using ML for Image Quality Metrics. Now I work for a biochemistry research institute developing a framework much closer to OR than ML for doing combinatorial optimization of configurations for physics applications, where evaluating the solutions uses a lot of the same calculus I learned in my Stochastic Approximation and ML days.

Note that I am a computer scientist, a graphics and ML person, I work for biochemists, but my job is in OR and high energy physics.

So take it all with a pinch of salt. But if you're good at what you do and you love the work, do what you want when you want and let the quality of your work open the doors to do whatever you want to do next.

Ultimately your PhD is about showing you have gained the skills to be a competent and independent researcher, and that there is a large swath of computer science, mathematics, physics, optimization, w/e that you have mastered to the point where even if you don't know the answer you know how to find it out efficiently. Whatever you set your mind to next is up to you."
MachineLearning,[deleted]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Could you share about those information bottlenecks you mentioned? Sounds interesting.
MachineLearning,"Me too I have this kind of problem 
But tell me you want to write a research for publishing it or what 

Because it’s different ways to write it"
MachineLearning,"[Guild AI](https://guild.ai) does this. You configure an ssh host and run an operation like this:

    $ guild run train --remote &lt;some host&gt;

This command packages your project, uploads it to the configured host, and runs the operation. You can apply various Guild commands to the remote (e.g. show results, list files, etc.) or pull the run down back to your local machine for more detailed analysis (e.g. view results in TensorBoard, etc.)

    $ guild pull &lt;some host&gt;  # This sync's the remote files locally

Guild is agentless so you just need ssh connectivity to the host to do this. There's nothing to install remotely.

If you interested in the functionality, refer to the [Remotes](https://my.guild.ai/docs/remotes) docs for details.

I'm the author of Guild and happy to answer any questions."
MachineLearning,"I would want to be in a place where I have a good degree of freedom to pursue the questions that I would find interesting and important while **having the capability to make a good attempt** at answering them with whatever I would have in my toolbox. I still haven't decided whether to continue within the academia or in the industry, both have their own appeals. Ideally, I would want to have made at least a single establishing contribution already, which is I guess what PhD is."
MachineLearning,"You tried your best, random Reddit bot"
MachineLearning,"Why does the MIT OR: [https://dspace.mit.edu/handle/1721.1/5066/discover](https://dspace.mit.edu/handle/1721.1/5066/discover), has very less research publications track record?"
MachineLearning,Or share it with the rest of us in the thread 🤦‍♀️
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Can you get marked down for an informal tone?
MachineLearning,"Interesting though that all our political ideas were at some point non-existent. They might at some abstract level have features that coincide with tribal societies, like ""cooperation"" and ""hierarchy"", but we did add complexity and came up with new concepts that define something like e.g. ""political parties""."
MachineLearning,"Do a PhD on the area you love the most. If you have a good track publication record and good LORs you may still get a direct PhD in the domain you actually want to work in. Else, you can try for predoc or residency programs like those of Google, DeepMind, Facebook to get a hang of what PhD actually is.

PhD may not get you a more high paying job, if you are getting one now. (would love if someone disagrees with me and reason with it)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"OR research is published primarily in journals, not conferences, less so ML conferences. 

If you want to do ML in your OR PhD discusss this with your supervisor. You need to be in agreement on expectations. 

If after this you are not happy, it's probably best not to do it. Starting a PhD in an area you are not interested in is a bad idea."
MachineLearning,Where do you look yourself in the next 10 years?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You just might though... Once it figures out that the difference in colors is wavelengths.
MachineLearning,"Hello, 
Thanks for your post. Recently took on a project working primarily with AI for customer experience. The project is geared towards improving efficiencies with conversational AI and ML to better anticipate the needs of the clients. I’d be more than willing to expound and pick each others heads. Reach out. Thanks"
MachineLearning,Interesting discussion! I've thought of the template matching interpretation for a while. Please take a look at https://arxiv.org/abs/1908.05168. We found that for classification tasks template matching is too difficult to achieve for CNNs. Bias parameters are actuality the major contributions to end results instead of the overall effect of filters. This is also consistent with known problems like CNNs biased towards textures.
MachineLearning,I’m more than interested in hearing this. Chat me if you can. Thanks
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Then where did yellow come from?
MachineLearning,"We always assume that there is an underlying true distribution of something that we want to learn. In your classification task, it is p(y|x), for the generator of a GAN it is simply p(x).

If you don't have enough data to train a classification model, you will also not have enough data to train a useful GAN. In particular, if your dataset does not reflect a true relationship between x and y (the classification task) good enough for your classifier to learn it, you can also not expect your GAN to understand this relationship."
MachineLearning,"Indeed the optimal ""generator"" just spits out the training data, at which point the discriminator can't do better than random guessing. The trick is that NNs are a continuous function, coupled with SGD which resists plain memorization. Also dataset size matters. If the generator only memorizes a subset of the data, the discriminator can use that to its advantage.

As with NNs in general, it will be better at interpolation than extrapolation. You will want a dataset that covers as much of your true distribution as possible."
MachineLearning,Oh sweet. I'll give it a read for sure. This will be very helpful. thanks!
MachineLearning,"The way I think of it is this:  Imagine our real data is a set of points in 2D space that are sampled from within some circle.  Not every point in the circle is in our dataset - we only have a sparse sampling.  Now our discriminator is trying to come up with a rule to tell whether a given datapoint is in the dataset.  The discriminator has limited capacity - it doesn't have enough expressive power to just memorize the specific points that it's seen.  It has to come up with something simple that still captures the data.  So it might learn to make its decision based on whether a presented point is within the circle.  As the generator catches on, and starts generating samples that are also within the circle, the discriminator will not be able to memorize which points are generated and which are real, because, again, it has limited capacity.  Instead, it reduces its loss by becoming uncertain about which points in the circle are real and which are fake.

This is a simplified, fairy-tale version of what's happening, but I think it conveys the right basic idea.  Obviously real data isn't so simple, and there's likely to be at least a small amount of memorization mixed in with real networks.  The space of real images in whatever dataset you're using forms a high-dimensional, more complicated version of the circle in the previous example.  The best the discriminator can do is draw some decision boundaries in this high-dimensional space and try to estimate the manifold of valid images.  If the generator can sneak its outputs into that same manifold, the discriminator will be hard pressed to tell them apart, because it lacks the capacity to draw ever-finer delineations."
MachineLearning,[https://www.ias.edu/sites/default/files/math/csdm/2017-2018/AroraRiZh2018.pdf](https://www.ias.edu/sites/default/files/math/csdm/2017-2018/AroraRiZh2018.pdf)
MachineLearning,"I see, okay I’ve only built CNNs so I guess I’ll explore RNNs firsy"
MachineLearning,"1) No significant prereq as this is undergrad level, but it may be worth watching the probability part of DL lecture 2 and/or reading chapters 2 and 6 from the [MML book](https://mml-book.github.io/book/mml-book.pdf). 

2) Not for the main theory behind the fundamental RL algorithms, but when you start trying to scale up the methods with function approximators, it's useful to have had some practice in building DL models, especially CNNs &amp; RNNs."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks everyone but I have found the solution 

X_new = X_old *(new_width/old_width)

Same goes for Y axis. I think there was precedence error but it works now."
MachineLearning,Thanks!
MachineLearning,Awesome suggestion! Seems affordable too.
MachineLearning,Great suggestion! I like the idea of using RL because lil bro can “teach” the robot! I wonder if $400 is too much.
MachineLearning,Thanks!!!
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I am a software dev with 7 years of experience who dabbles in deep ML on the weekends, but works daily on a product that does forecasting using decision trees. The majority of my work is pipeline construction &amp; maintenance at scale, and delivery of our model's outputs to customers. Our models are quite basic.

I love all the newer techniques and would love to grow my knowledge to be able to actually apply those to our business problems.

There are so many issues that we run into when putting models into production that I'd love to chat with anyone who does this stuff regularly.

For example, currently we train 5000 individual models, each with about 100k rows of data, but the models represent the same kinds of devices just in different environments. I thought if we could train a global model on all the data using deep nets, and then use transfer learning to fine tune the individual models we may improve accuracy.

I started working on this but ran into a ton of questions such as what NN architecture makes sense, how to load data in batches so we can train without running out of memory, etc.

Anyway, happy to talk more with anyone if they are interested."
MachineLearning,"The skill of writing a good paper is often neglected in computer science mostly because we think that the research result is what makes a paper great. However, even incremental findings that are well written can attract attention to your work. 

Personally, after months of research and coding, I struggle to start writing EVERY SINGLE TIME. Here are some things that I find useful to get going:

* Check out this book written by P. Silvia, PhD (in psychology) [How To Write A Lot](https://www.amazon.com/How-Write-Lot-Practical-Productive/dp/1433829738/ref=pd_bxgy_img_3/140-8968766-9643504?_encoding=UTF8&amp;pd_rd_i=1433829738&amp;pd_rd_r=4bb735bd-4d9e-46d6-bb06-040fcae3e0b5&amp;pd_rd_w=A7eGi&amp;pd_rd_wg=z1G1n&amp;pf_rd_p=f325d01c-4658-4593-be83-3e12ca663f0e&amp;pf_rd_r=GBXNE4WZY0KA8PHAQ80V&amp;psc=1&amp;refRID=GBXNE4WZY0KA8PHAQ80V). This tiny book is no B.S. guide to writing with some notes on style. I tend to look through to remind the key points.
* Simon Peyton Jones has great lectures on [how to write a good paper](https://www.microsoft.com/en-us/research/academic-program/write-great-research-paper/) and [how to give a great research talk](https://www.microsoft.com/en-us/research/academic-program/give-great-research-talk/). They will provide you with a structure (like an algorithm) that you should follow. 
* After going over the two points above (spend no more than a couple of hours) and organizing a paper in your head, start [freewriting](https://www.lynchburg.edu/academics/writing-center/wilmer-writing-center-online-writing-lab/drafting-a-document/freewriting-techniques/). Use time intervals to write non-stop. The writing is mostly about editing. After a few sessions and a few drafts, you will see your paper shaping into something publishable. 

As others mentioned, reading good research papers and other literature is crucial to becoming better at writing."
MachineLearning,"Hi, thank you for answering. How much does it matter that who has what PhD from where in research work setting as opposed to their publications?"
MachineLearning,Hello I would also like to collaberate with you on projects.Would like to discuss research papers.I have completed my masters in physics and ML.
MachineLearning,"This is something your advisor would hopefully help you with. When my employees or students send me papers, I go through and mark changes or edits I would make, and _why_ I would make them. Having a conscious list of things that you want to do/avoid can help improve your writing a lot. 

What I did early on, which I think helped, was to try and emulate the flow / layout of papers that I really liked from a reading perspective. Find a hand full that you felt were the easiest to understand or replicate. Then go through and annotate the papers on their _writting_. Mark what the the purpose of each paragraph is, purpose of each figure? Are there any patterns between the papers on how they layout their information/content, how they communicate it, or other shared styles? 

Once you have that list, try to replicate it in your own writing! 

Another great thing is to have other people (besides your advisor) read your work and give you _honest_ feedback. You want someone who can and will tell you when parts are confusing or hard to follow. Being able to satisfy multiple different people with your writing style is important, because you want multiple people to be reading it! (i.e., don't over-fit your writing to what your advisor says is good)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"They do not have to be intuitive, e.g., the recently-hot idea of [mixup](https://arxiv.org/pdf/2010.02394.pdf) is in some aspects similar to SMOTE... Nevertheless, you are right that data augmentation is used in NLP to rather enlarge than balance the dataset. Moreover, balancing can be counter-productive for evaluation metrics like micro-average F1 as it may lead to an increased number of false positives."
MachineLearning,WOW this looks super useful o.o
MachineLearning,"Over  a period of time a deep learning  model becomes stale and needs to be  retrained as the  distribution of images that it was trained on changes  over time. But is  there any way do track this change in distribution of  images so that i  can quickly retrain the model without having to  manually look at  results. We have an object detection model currently  deployed in  production and we have few hundred new images coming in  every week but  we anticipate it to increase to few thousands in the near  future and  obviously this becomes impossible to track confident false  positives  until its too late.

I  have tried  reading up some articles on online on model-drift etc but  they all talk  about tabular data but none about image data. I remember  reading some  where that unsupervised methods like VAEs can be used to  find  reconstruction errors and using this error we can track any drift  in  data but neither have i been able to track that article down nor was I able to find a good research paper which has tackled this problem.

Can some one help me to how to approach this problem of data-drift, model-drift detection? Thanks in advance"
MachineLearning,"Let's say there are only two things, the reds and the blues, please create a new thing which doesn't take from red or blue."
MachineLearning,Yall fake wannabe rl practitioners if you downvote this lmao
MachineLearning,"Dear Redditors,

I'm Dr. SL from Berkeley the god king of Deep RL. The field would be mostly dead without me. I'm hiring students with straight A+s who want to argmax clout. Let's chat if you want more clout.

Best,
Dr. SL"
MachineLearning,[deleted]
MachineLearning,"Looks amazing! I've used PyTorch for a bit and then switched to Keras bc I didn't want to go through a bunch of PyTorch errors haha. Looks like you've gotten me right back to using PyTorch :D.

&amp;#x200B;

Will recommend this to all my friends, looks amazing. Major respect to you for being able to pulling up with something like that. Cheers!"
MachineLearning,"From my experience, an OR PhD will depend heavily on program/advisor, and the different programs vary a lot. Most OR programs focus primarily on applications, mathematical programming, and probability theory, with relatively little emphasis on ML, but studying ML in an OR program is becoming more common.

One big difference is that most OR advisors still publish in journals, and do not have much experience in the ML conference model. This could impact your ability to get ML research positions, as I'm not sure 1-2 publications in Operations Research or Management Science has as much weight as 5+ in top ML conferences

I work for an OR team, and we primarily recruit OR PhDs, although my background is in ML."
MachineLearning,"&gt; therefore not truly imaginative/creative

Nothing ""truly"" is. Humans aren't much different. If I ask you to draw a new animal, you'll use things you know from other animals. If I ask for a new political idea, you'll take things from already existing political ideas etc."
MachineLearning,It has now evolved cat ears
MachineLearning,"That is a nice way to explain it, I agree with the mixing of existing ideas. I guess its the same with all the face generation that's been going on. They learn a data distribution from a couple of input images of faces, which are essentially part of a larger distribution (combinations of different ears, eyes and many other characteristics). The combinations you can make by sampling from random vectors can be endless but they are still part of that said larger distribution, therefore not truly imaginative/creative."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks for sharing
MachineLearning,"Actually that's the problem with ""imagination"". We're never really creating new things from nothing.

You can either mix existing ideas or randomly explore the space around existing ideas.

Same goes for GAN. They can start from a random matrix or vector and turn that into an image. If we say it's a vector with 3 values. The first flower it watched could be generated with the features \[0,0,1\], the second by \[0,1,0\]. 

The algorithm will learn to give a meaningful representation when there's 1 in the second and third position. Now if you try to generate from \[0,0,1\], you'll have the first flower (or something close because GANs don't memorize everything exactly and it's not always easy to know how to generate what it saw during training). But if you try from \[0,1,1\], you may have a mix of the 1st flower features and of the 2nd flower features. It'll learn to generate an image that contains the features from the first and second image.

You can see that here: [https://carpedm20.github.io/faces/](https://carpedm20.github.io/faces/) or [https://www.reddit.com/r/MachineLearning/comments/ijkkbb/p\_crossmodel\_interpolations\_between\_5\_styleganv2/](https://www.reddit.com/r/MachineLearning/comments/ijkkbb/p_crossmodel_interpolations_between_5_styleganv2/)

So ""new information"" can either mean ""mixing things we already know in a way that was never done before"" or ""being able to generate something that is coherent from a vector and trying random vectors""

Of course, it doesn't mean that the results will be great. GANs work on paper but making them exactly do what you want properly is hard in practice."
MachineLearning,"Depending on which kind of projects 
I’m a geophysics and Recently I complete my studies in ML 
So if you want to start at geophysics,geology or petroleum field projects 
I’m in"
MachineLearning,Can you inbox me
MachineLearning,"I think there are two different barriers to entry.

1. The one you mention, for people to actually learn DS
2. The one I think is critical - companies understanding how to consistently turn data science outputs into a product

The truth is, most companies suck at (2.) and this isn't something you learn in a course. For a company whose core product will be ML/AI they need to do this well. For companies that want to do DS for optimization but not as a core product will generate the demand for commoditized DS products in the long term.

&amp;#x200B;

The commodification might not necessarily benefit just giants like Google. But consider this - Google will be building general tools suitable across many verticals. You will still be able to compete within a vertical by going more in-depth and overfit for the domain that is your market."
MachineLearning,Thanks for sharing! Saving the playlist
MachineLearning,[deleted]
MachineLearning,"That's called mode collapsed. Theres a GAN called WGAN that's used to address that problem, to generalize the distribution instead of collapsing at a local. There's multiple other types of gans too. I'm not too versed in that's stuff but like you that was my number question after learning about Gans."
MachineLearning,"""Researchers hate this one little kernel trick..."""
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanks
MachineLearning,"When you are building business, you are focusing on solving customers problem with product market fit. You might use emerging technology in it or might not. If you use it, then you can hire Expert to do it or leverage no code environment. Remember that you are user of Technology to build product and Business. Not other way around.
Having said that, the startup space which builds tool for others to consume technology, will be specialised and need deep expertise. Hope that helps!"
MachineLearning,[deleted]
MachineLearning,"https://ayansinhamahapatra.github.io/2019/06/15/tech-writings-django-vs-flask.html
This link has good overview for flask and Django for projects."
MachineLearning,That is not equivalent at all.
MachineLearning," Thank you for the article, it is quite good! 
Tackling wild expectations would be critical for success of AI. 
One comment:
Neural network are in general function approximations. The CNN is a special category of approximators.
They don't match but based on learnt approximations, try to provide answer about closeness."
MachineLearning,Is there any equivalent for Django?
MachineLearning,You might want to take a look at flask api with model hosting on cloud and using client server mechanism.
MachineLearning,"Hebrew University of Jerusalem and Weizmann Institute are probably good too, even if the Weizmann Institute is perhaps not a traditional university."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I’d recommend checking out RoboFlow’s blog. They’ve got some good resources
MachineLearning,Really? Could you please share the mail address you get it from coz I don't find any.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Look for the former students of professors, or just use LinkedIn."
MachineLearning,Hello! Im new in machine learning but have a big ideas and projects. I will be happy to chat with you.
MachineLearning,"In my view, a startup is about finding a market, which I think does not require a science background, engineers and scientists can be hired. I think in terms of improving ML/AI you need to be an expert but a layman can still find innovative applications of ""hello world"" level AI/ML."
MachineLearning,Thanks for the reply! Do you think people without cs/math/stats degree like doctors or dentists will still be able to launch their own startups which have ML/AI as its core product?
MachineLearning,"The barrier of entry is relatively stable, for our generation AI might seem difficult but there is plenty of online resources that some high school kids are already engaging with. The biggest challenge I see is the power that Google, Facebook, Amazon, etc. whield where your business can die before it takes off because of one or more of these giants. The trick is not to compete directly with the major players, but that is how business has always been approached."
MachineLearning,"Wow, thank you so much for sharing !"
MachineLearning,"I have a PhD in Neuroscience and find all of this fascinating.  If you would find it useful to explain your work to a novice, I would volunteer."
MachineLearning,"The consensus is that beyond the first few bits it's impossible.

Here's a repo I found: [https://github.com/trevphil/preimage-attacks](https://github.com/trevphil/preimage-attacks) that concludes that same."
MachineLearning,"
I see you've posted GitHub links to Jupyter Notebooks! GitHub doesn't 
render large Jupyter Notebooks, so just in case here are 
[nbviewer](https://nbviewer.jupyter.org/) links to the notebooks:

https://nbviewer.jupyter.org/url/github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb

https://nbviewer.jupyter.org/url/github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb

Want to run the code yourself? Here are [binder](https://mybinder.org/) 
links to start your own Jupyter server!

https://mybinder.org/v2/gh/tensorflow/models/master?filepath=research%2Fobject_detection%2Fcolab_tutorials%2Finference_tf2_colab.ipynb

https://mybinder.org/v2/gh/tensorflow/models/master?filepath=research%2Fobject_detection%2Fcolab_tutorials%2Feager_few_shot_od_training_tf2_colab.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)"
MachineLearning,Check out the [TF2 object detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) including the inference [colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/inference_tf2_colab.ipynb) and the few-shot training [colab](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb).
MachineLearning,"I also had a problem with not receiving a confirmation email for my application to the AI residency in London. However, I have applied to a couple more roles and I've received confirmation emails for those. If you use Gmail, they get put in the 'Social' tab."
MachineLearning,"Also check out this paper https://ieeexplore.ieee.org/document/9286182

FLOPs when running on GPUs actually have very little correlation to actual performance. The structure of the network has a much larger impact."
MachineLearning,The official documentation is pretty good.
MachineLearning,"Try this book might be helpful. The Minto Pyramid Principle: Logic in Writing, Thinking, &amp; Problem Solving"
MachineLearning,Thank you! 🤩
MachineLearning,"**still available, if the offer is.** 

*-fallendeveloper*

***



^(Commands: 'opt out', 'delete')"
MachineLearning,If the offer is still available.
MachineLearning,Still in my pre final year of the undergrad but I would love to collaborate as I am planning to pursue an MS afterwards.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Brain.js, what are some simple but cool things you can do with it? I can’t find many simple examples of it, and there’s really not too much documentation for somewhat js beginners."
MachineLearning,Thanks !!
MachineLearning,"Yeah I'd recommend that too. Introducing smaller anchors should help. Also, a quick histogram plot about the distribution of ground truth boxes would help adjusting parameters such as aspect ratio, scale, etc.

In addition, your focus should be on the high resolution feature maps which contain better localization information for objects."
MachineLearning,"Wow yes you're right. Crazy. I just did some benchmarks of efficientnet-b0 vs resnet50. Even though resnet50 has about 11x more FLOPs and should naively theoretically take 11x as long as efficientnet-b0, it only takes 1.5x as long. So, not too much slower.

Results are on an RTX 3090 with 32 bit precision and cuda 11.2."
MachineLearning,?
MachineLearning,"Oh okay thank you, I better get documented on it then, is the official documentation a good place to start, or do you recommend other resources?"
MachineLearning,"I don't think it can. But it specializes in keyword / index matching, searching and predicting through Naive Bayes algorithm. It also helps in parallelizing and optimizations."
MachineLearning,I want to work with you sir. I am still learning machine learning. I want to gain more knowledge and experience.
MachineLearning,"Why downvote? 

We have used combined RL + physical model to optimise our work for years( because dataset is sparse, physical model is unbounded ) .

I genuinely want to know what does robust ML mean ? Google tells me nothing but some buzzwords. 

Thanks."
MachineLearning,"I said from well known university, BTW Technion (Israel) is not the only top tier university in the Middle East."
MachineLearning,"Very interesting - I actually had no idea why spinning up GPU instances took time and kind of just accepted that it did, but this makes a lot of sense!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Ok thank you!
MachineLearning,Does anyone have any interesting but not too challenging NLP problems? I am doing a final year project soon.
MachineLearning,"""Raw data flows into Elasticsearch from a variety of sources, including logs, system metrics, and web applications. *Data ingestion* is the process by which this raw data is parsed, normalized, and enriched before it is *indexed* in Elasticsearch"" is data ingestion part of what Elasticsearch can do? I mean will it help structure and normalize the data?"
MachineLearning,"&gt;a) alone will help a ton. And don't just read papers, read long-form content like fiction and non-fiction books, magazine articles etc. You need to learn through exposure what works and what doesn't.

Thank you for your suggestion, I am also read papers, trying to reproduce them"
MachineLearning,"&gt;rs a course, mostly targeted towards master student

Thanks for your suggestion"
MachineLearning,"Without direct use of machine learning, you should look into Elasticsearch.
 
 
Take a look here: https://www.elastic.co/what-is/elasticsearch"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,So many thirsty undergrads and international students 😂
MachineLearning,Thanks a lot. Been flip flopping on starting the David Silver one this week. Will definitely check it out.
MachineLearning,"`ssh` is super versatile and there are many ways to make it behave as if it was a local action.

1. Use keys for login if you do not use them yet
2. you may equally use rsync into ssh to submit jobs as a part of your job script
3. In case job submissions are instant you may use remote ssh transparently in pycharm and many other python IDEs (I am assuming you are using python)
4. I just googled an interesting option to keep you connection alive across many consecutive ssh logins (presumably, including scp and rsync). This should make your remote shell initiation almost instant.

https://serverfault.com/questions/758628/how-to-maintain-ssh-connection-for-multiple-scp-transfers"
MachineLearning,[deleted]
MachineLearning,"hello. I am a startup in ML AI, very glad to see you and do you like predictive projects. I am predicting gamble and finance market. my website is AISoftwareEngineer.com . I have the good benefit projects."
MachineLearning,"Getting better at writing requires two things: a) Reading a lot b) Practicing

Just a) alone will help a ton. And don't just read papers, read long-form content like fiction and non-fiction books, magazine articles etc. You need to learn through exposure what works and what doesn't."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Our university offers a course, mostly targeted towards master students. They use this book:

https://www.amazon.com/Academic-Writing-Graduate-Students-Essential/dp/B072DT4Z51

It's quite good."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thanh you
MachineLearning,"*Beep boop*

I am a bot that sniffs out spammers, and this smells like spam.

At least 50.0% out of the 4 submissions from /u/jeromeharper appear to be for Udemy affiliate links. 

Don't let spam take over Reddit! Throw it out!

*Bee bop*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It's a problem in itself if you have such a distribution shift. Its virtually impossible to for the network to learn if distributions on train and test differ, it's like training in football and then trying to win a basketball match. As far as I know layernorm doesn't make distribution shifts worse, but I don't know everything. Its definitely worth a try. After all layernorm was created specifically for recurrent neural nets"
MachineLearning,Thank you so much
MachineLearning,"I’m trying to understand convolutional neural networks, but having a hard time visualizing the matrices. I understand it involves a sliding window that moves over the input image, does a dot product with the filter, and outputs a number. So if my sliding window takes 50 screenshots to finish processing my image, does that mean my cnn has 50 neurons in the first layer?

Also please correct if I got any terminology wrong."
MachineLearning,DM me too
MachineLearning,"You've got it backwards, his is by no means a recent trend. ML has a long standing tradition of naming papers creatively instead of informatively. Whether it's LeCun naming a regularization method [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html) in 1989 or Tishby talking about [The Power of Amnesia](https://proceedings.neurips.cc/paper/1993/hash/08419be897405321542838d77f855226-Abstract.html) in 1993. Our boys Welling and Hinton liked to discuss how [
Wormholes Improve Contrastive Divergence](https://proceedings.neurips.cc/paper/2003/hash/03cf87174debaccd689c90c34577b82f-Abstract.html). People name their paper after a song( [No Label No Cry](https://proceedings.neurips.cc/paper/2014/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html) ) or clickbait-y questions like [Do Deep Convolutional Nets Really Need to be Deep and Convolutional?](https://arxiv.org/abs/1603.05691) or [Are Hopfield Networks Faster Than Conventional Computers?](https://proceedings.neurips.cc/paper/1996/hash/68a83eeb494a308fe5295da69428a507-Abstract.html), our field is choke full paper titles like this. And when you have to compete for the attention of a gazillion people at a poster session, this does make a lot of sense. And to some degree it's nice that not everyone is dead serious all the time.  But what *is* fairly new is people rehashing old naming memes (X is all you need) instead of coming up with their own non-informative titles."
MachineLearning,I am interested
MachineLearning,"Text is a bit tricky for feature extraction. If you have metadata, you could use things like user IP, # of user connections, other user metrics. For the text, you can treat the text as a bag of words (https://en.wikipedia.org/wiki/Bag-of-words_model) and train a Naive Bayes model for classification. For deep learning featurization, you could try taking the tfidf weighted embeddings for each token in the text. Or just feed the entire text into BERT."
MachineLearning,"Thank you. The more material, the better."
MachineLearning,"I believe you are trying to maximize precision: (true positives/(false positive+true postiive))

https://en.wikipedia.org/wiki/Precision_and_recall"
MachineLearning,"Some examples:

* Different types of gradient descent: https://en.wikipedia.org/wiki/Hill_climbing
* Genetic algorithms: https://en.wikipedia.org/wiki/Genetic_algorithm"
MachineLearning,I’m interested
MachineLearning,Hey I’m currently doing my masters in data analytics with an interest in machine learning. Im native English so would love to collaborate on a project with you
MachineLearning,I am working in the field of NLP and semantic web(learning it) and would like to work n explore this field with u.
MachineLearning,Super thanks for sharing especially the reinforcement learning
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I think this title structure worked well with Google's paper on Attention, as it also sounds like some life advice. Others are just mimicking Google."
MachineLearning,Please dm me if you found something helpful i am facing similar problem
MachineLearning,i want to work with you sir. But I'm still learning and want to gain more and more knowledge on machine learning.
MachineLearning,Thank you
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"If you use images from the 110 labels you don’t care about for training, you have some choices:
1. Put them in a category called other
2. Train your network on everything, remove the final layer of the network and replace it with a 10 neuron layer. Lock updates for most of or all the network except the new final layer and retrain using only images from the 10 labels you care about

Now I wonder how you will use this network. Will it only be used to classify images from those 10 labels or will it be presented with images that should have been classified with the other labels. If the former, approach 2 should work while if it’s the latter then you need approach 1 is better."
MachineLearning,Hello I am currently working on hybrid data science/SWE role and i'd love to collaborate with you on this as well !
MachineLearning,"Have you checked out the ""Tensorflow Object Detection API""?

I've used it in the past and it worked great.

[Video Tutorial by Sentdex](https://m.youtube.com/watch?v=COlbP62-B-U)"
MachineLearning,"Thanks a lot for the encouraging words and your good advice, which I'll try to head."
MachineLearning,"Hello there ! I am currently an undergrad and have no experience in research, although I do have an interest in pursuing masters further and have some grasp of basic concepts . Being able to connect with someone who already has knowledge about the research side is quite good for me also on the other side it will help you in your endeavor of improving your communication skills by guiding a junior :)"
MachineLearning,May I ask at which university are you? How was the process to apply? I'm a senior hoping to get graduated this May and I'm really considering doing an ML master afterward.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I'd love to collaborate with you! I'm a beginner and I'm passionate about this fascinating field. I could be all day reading papers.
MachineLearning,"I agree, this does seem possible."
MachineLearning,It is possible to ssh and run a script rather than going to interactive mode. Iirc it's the '-t' option for the ssh command.
MachineLearning,[deleted]
MachineLearning,"Check out decision trees, SVM, linear regression (using the closed-form equations), graphical models."
MachineLearning,[deleted]
MachineLearning,"Hi Yes, I would like to collaborate with you, I am also interested in machine learning research. Send me a dm"
MachineLearning,"I am a MS in Machine Learning international student in USA , would love to collaborate on some projects and take some guidance too"
MachineLearning,love it
MachineLearning,"&gt; robust machine learning 

In AI industry.  Is this buzz word == sort of robust optimisation ?"
MachineLearning,"Hey would love to collaborate! I've worked in academic research labs and have had publications in computer vision. I also speak Chinese (assuming you're Chinese based on your username) and am fluent in English so I can help with the transition :) 

Best of luck with your job search"
MachineLearning,"Sure, love to do that."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"In the news you'll occasionally see stuff about 'AI ethics'. Particularly in the past few months there has been some drama at Google about disagreements and employees being fired on their ethical AI team. This isn't about how an 'AI' is treated, because as other comments have explained, it's just math that's complicated enough to seem like real thought to an amateur. The ethics are about how to responsibly use AI without unintentionally deploying racist or sexist tools.


Machine learning is basically really good pattern recognition, and it learns from existing data. If it is trained on data that contains racial bias, the models will also be biased. And by the time a product gets to the final customer, it is so far removed from the engineers, and passed through so many layers of marketing speak and salesmanship, that the end user is overconfident in the results and tends to blindly follow it. This can be very detrimental if the end user is a judge or a hiring manager, etc, and the AI has learned to make racist recommendations because it's consistent with historical data.


And the problem is non-trivial to solve from a technical standpoint. You can force an AI to ignore race or gender information, but there are so many other variables to learn from that it can figure out demographic information anyway. If you give an AI a person's age, height, weight, zip code on birth certificate, youtube history, netflix history, everyone they follow on twitter, etc, it will figure out a lot of cultural information whether you want it to or not. There's some special stuff you can do, but it requires lots of extra effort on the engineering side.


Lots of technical folks sort of roll their eyes at the topic. They build an incredibly effective model, and people complain that they didn't solve societal racism while they were at it. Its kind of like you can never do enough, and science progresses in small steps, so it can be frustrating when people complain about progress. But its something that everyone needs to at least be aware of. The recent focus on social media funneling people into extremism is a good example. Recommending a video or news story to a user seems so harmless. But if an AI learns to identify poorly educated and angry people who are susceptible to conspiracy theories and white supremacy, and recognizes that they click on more videos if you feed them extremist content, that's a big problem."
MachineLearning,I would definitely like to collaborate with you. A Kaggle competition would be great.
MachineLearning,quality [https://i.imgur.com/QlSjc6u.mp4](https://i.imgur.com/QlSjc6u.mp4)
MachineLearning,Following your (and Thomas Kipf's) work for a couple of years. Great work. Huge fan.
MachineLearning,"""On the dangers of stochastic parrots: Can language models be too big""

Yes, that's the controversial paper that got Gebru fired from Google and ""stochastic parrots"" is the uncommon bigram."
MachineLearning,"Directly taken from an old newsletter from Jack Clark:

**Acronym** **police, I'd like to report a murder:** Panda is short for giga**P**ixel-level hum**AN**\-centric vi**D**eo d**A**taset. Yup."
MachineLearning,"Man, that's awesome, i am just breaking into the filed of RL, and i really appreciate your effort. Cheers 👏👏"
MachineLearning,I have machine learning ideas :)
MachineLearning,Do you have an example of the uncommon bigram technique?
MachineLearning,[deleted]
MachineLearning,"Deep learning looks like it follows the [Lindy Effect](https://en.m.wikipedia.org/wiki/Lindy_effect). 

In software anything which follows lindy effect becomes an abstraction over time. Example: Very few people are proficient in assembly but it’s used ubiquitously. Same way powerful NNs will become strong abstractions over time. But Pure software can’t be eliminated. You need mix of both. Deep learning can approximate complex things well. Normal software can do simple things exactly. U need mix of both to make really robust systems"
MachineLearning,Towarddatascience I guess
MachineLearning,Yes please! Would love to talk more!
MachineLearning,"google search does a lot, so the paper will show up as

    &lt;b&gt; A MEMY NAME &lt;/b&gt;
    ...has shown that gaussian processes... indeed stationary kernels are necessary to..."
MachineLearning,the deep learning slides are sooo good. exactly what they need to be on
MachineLearning,"There was a discussion on github about this:

https://github.com/keras-team/keras/issues/1802"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you. Are the originals located elsewhere?
MachineLearning,"yes! but the paper itself is fairly well-written, at least in my opinion. Explains the concepts in a good enough manner for someone in the field."
MachineLearning,"I'm a PhD student with experience in both computational biology and DL, if you are not just looking for someone to implement your idea, maybe we can chat. :D"
MachineLearning,I'm an engineer working in aviation and self-taught DS. Looking for ways of combining them. Or some other side projects. Count me in.
MachineLearning,Thank you good dir
MachineLearning,"Hi, I  would recommend using [cloud.jarvislabs.ai](https://cloud.jarvislabs.ai). Our early adopters love us for simplicity, cost, and service. Some of the supported features are :

*  1 click Jupyter Lab &lt; \[30 seconds\]
*  Pause the instance and Resume from where you left.
*  SSH to the instance.
*  Scale-up/down GPUs on Resume.
*  Auto-Pause using jarviscloud.pause() in your code.
*  Pay per usage – Minute Billing \[After first 15 minutes\]
*  Affordable pricing.

Disclaimer - I am the founder of the company [Jarvislabs.ai](https://Jarvislabs.ai)."
MachineLearning,"One of the coolest things I've seen (and that I am working on myself rn) is a quadracopter that is controlled by python scripts.

There is a lot of documentation on the subject and examples all over the web for this project."
MachineLearning,"Nice, tks"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Seeed Studio sells a [Grove AI HAT for Edge Computing](https://www.seeedstudio.com/Grove-AI-HAT-for-Edge-Computing-p-4026.html) which might give your project more processing power.
MachineLearning,Maybe you might have luck searching on LinkedIn? OR I would look up conference websites in computational biology and search attendees/organizers that fit what you’re looking for
MachineLearning,Hot Dog or Not Hot Dog!
MachineLearning,"Isn't ""Attention is all you need"" the paper that introduced ""transformers"" and then people around the world had to produce 800 video blogs and 2000 medium articles (for free) just to try to explain what was actually done in the paper?"
MachineLearning,"I think this goes also for names of algorithms or models as well.

There are like hundreds of papers now solving the saturating ""non-saturating GAN"" problem.

I mean if you name your GAN non-saturating it better doesn't saturates..."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I was literally reading a paper just now where the authors changed a coefficient of an set of equations that has studied since the 60s (mind you, minor changes) and somehow managed to write 8 more pages.

And then the experiment section proceeded to completely violate their entire theoretical result section."
MachineLearning,"Couldn't agree more. What I hate on top of that are the papers that overclaims or even put a wrong result, in the title of the paper.

Back in the old days people tried to hide their lack of innovation (or inconclusive finding) by shoving huge numbers of equation and text in the body of the paper to obfuscate.

But how has things became this desperate for grant money, research funding and (oversea) conference vacation opportunities that people are putting it in the title?"
MachineLearning,Thanks l!
MachineLearning,What are the prereqs for learning reinforcement learning? Do I have to be really good at most of the DL architectures before learning about RL?
MachineLearning,Thanks mate ! You are a hero with a cape !
MachineLearning,"In a sub filled with mediocre self-promoters with their n-th crappy Medium articles and online ""courses"" you opted for suspecting the free resources from actual university courses shared by the professor."
MachineLearning,Do you happen to have some links for these predoc programs?
MachineLearning,I am looking to collaborate on research projects and learn from scratch to become a research fellow.
MachineLearning,"Thank you for sharing, this is wonderful for beginners"
MachineLearning,"The idea still resonates. I've seen good progress on ""git for data"" projects or platforms. There's been good progress on developer tools for ML.

I haven't seen a surge in problems that are easier to solve with machine learning though."
MachineLearning,Yes
MachineLearning,"Mechanisms for the model to be trained live while in production. Rather than having separate training and inference phases, having the model additionally tune during its deployed lifetime."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,The donkey car project is pretty cool too.
MachineLearning,Don't listen to the haters Chrissy boy....excited to take this course...thank you for sharing!
MachineLearning,"Mother Fugger! 

This full paper title tries to do both: ""Mother Fugger: Mining Historical Manuscripts with Local Color Patches"". 

&amp;#x200B;

&amp;#x200B;

\[a\] Mother Fugger: Mining Historical Manuscripts with Local Color Patches. Qiang Zhu, Eamonn J. Keogh:  ICDM 2010: 699-708

[https://www.cs.ucr.edu/\~eamonn/Mother\_Fugger\_Mining\_Historical\_Manuscripts\_with\_Local\_Color\_Patches.pdf](https://www.cs.ucr.edu/~eamonn/Mother_Fugger_Mining_Historical_Manuscripts_with_Local_Color_Patches.pdf)"
MachineLearning,I thought about running this but the main problem is that all the data we have access to is conditional on an acceptance (otherwise we wouldn’t see the student’s profile). If there is somehow a way to get a dataset (that is not anecdotal) with both accepts and rejects to a school then things would get very interesting.
MachineLearning,"Maybe I've misunderstood something, but if the objects in the cloud are distinct (non-overlapping) then a clustering algorithm like DBSCAN or OPTICS would very easily partition objects."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"My main worry is a lot about the times when I am not productive, I'll try on those advices ! thank you !"
MachineLearning,"after my first rejection, I came to realize what you learn during the process is more important than the result, I usually keep notes of everything I learned while doing this paper !"
MachineLearning,"Thank you for your advices ! I actually have started doing some sports with some of my lab mates, but we still talk research while doing it ahahaha"
MachineLearning,"I see, thank you for that advice ! I pretty much don't have a life beside my work, most of my free time I just enjoy studying other stuff but related to Deep Learning, mostly because I feel like my skills are not on point so I don't feel like taking a break. I currently haven't officially started my PhD so I haven't talked a lot to my supervisor, but he did say he thinks that research during a PhD SHOULD be done in pairs in order to help students cope with this feeling. I am currently just doing research for one of those leading AI company and feel like trash sometimes having no output for weeks"
MachineLearning,"sorry im late, what do you mean by ""online learning ingestion mechanisms""?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"So in case of regression, trying to find the right parameters by using different algorithms is ‘modeling’ right?"
MachineLearning,Nice
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You’re procedure covers almost everything but a few things are missing. This is how I see the end to end pipeline:

1) understand what problem you’re trying to address. Is it a supervised or unsupervised ML problem. This helps you narrow down the choice of algorithms you can choose later during data modeling.

2) choosing evaluation metrics. We have a wide range of metric which are specifically good for specific problems. So it’s best to understand the evaluation metric.

3) Perform EDA, and pre processing steps like handling missing data, handling categorical data

4) perform feature engineering ( also includes performing dimensionality reduction techniques)

5) performing feature selection

6) fit a model by performing cv( also use pipelines to avoid data leakage)

7) save weights and deploy the model

Data modeling is the nothing but trying to estimate a function that is as similar as the orignal function that is generating the data. Consider your dataset is modeled by a LR which is given by eq f(y) = mx+c. Now we want to model the data to estimate a function as close it can represent the function f(y)"
MachineLearning,"My project might be useful to you.

It’s a no code social app for building image classifiers.

https://apps.apple.com/us/app/palapa/id1518753364

Let me know if you have questions."
MachineLearning,Don't forget a good acronym!
MachineLearning,"I know people that have done research internships (not necessarily during summer but all around 3-6 months), and from what I observed, it really helped them publish in a good conference and enter a strong PhD program. I feel it might help in terms of getting more recommendation letters and more time to focus on your project (since you don't have to TA or take courses).   


As for the programs, I've seen PIs post links to their pre-doctoral fellowships on Twitter a lot, but I would presume that a lot of the hiring would happen privately rather than an explicit posting; it might be worth reaching out to PIs to learn more about what they are offering and whether you would be eligible."
MachineLearning,"The first conversation that my supervisor had with me when I asked him if he would consider me for a PhD position was about resilience. The thing isx you have to accept that you are not gonna be productive 100% of the time. You have to learn to stop worrying about the times when you are  not productive and reflect upon those where you are. when you have new ideas, explore them,if they fail, discard and move on. 

He was also kind enough to recommend me books to read on my spare time. one of those was ray dalio's principles. one of his principles is: ""accept reality"" what he means by that is that you have to learn how the world works and cope with that. Also be true to yourself. dont try to convince yourself that you are being productive when you are procrastinating. Otherwise, try meditation, music, and sport. I find it pleasing to listen to Allan Watts mind blowing non-sense to refresh the mind. 10+ years in academia here, if that matters."
MachineLearning,"What is modelling exactly:

Types of modelling include: fashion, glamour, fitness, bikini, fine art, body-part, promotional and commercial print models. Models are featured in a variety of media formats including: books, magazines, films, newspapers, internet and television. Fashion modelling as a profession is sometimes featured in films (Prêt-à-Porter and Looker), reality TV shows (America's Next Top Model and The Janice Dickinson Modeling Agency) and music videos (""Freedom! '90"", ""Wicked Game"", ""Daughters"" and ""Blurred Lines"")."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yeah. Compare to an architecture like ResNet which uses normal convolutions. Even though ResNet has a lot more parameters and flops, it computes in a fraction of the time."
MachineLearning,"Also never use the words ""neural"" or ""quantum"" in your title. 99.9% of people (*and* perhaps 99% other scientists), will conclude, sometimes even after claiming to have read the paper, that it involves artificial neural networks or physics. Just like most people today can no longer differentiate between AI and machine learning. The title is quite literally everything. I remember that even ""fuzzy logic"" back in the day was much more popular in Japan due to the name alone."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Oh jheez thats interesting. Thank you so much for telling me! I was hoping to have someone give me an answer like this. Knowing this will help me a lot!
MachineLearning,Not too much as this is a year 2/3 undergrad course. The required math is introduced in lecture 2 and doesn't assume a strong background. The mathematics for machine learning (MML) book accompanies it quite nicely: [https://mml-book.github.io/book/mml-book.pdf](https://mml-book.github.io/book/mml-book.pdf) (especially if lecture 2 is too difficult)
MachineLearning,"THIS ONE METHOD MAKE YOUR BRAIN BIGGER, YOUR CITATION HIGHER"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for your kind notes!  


Certainly, GNNs have seen application to multiagent systems. Perhaps oddly, I only have two papers coming off the top of my head right now.

&amp;#x200B;

First, Waymo's VectorNet: [https://arxiv.org/abs/2005.04259](https://arxiv.org/abs/2005.04259)

which represents different components and participants of a traffic system as various nodes. This seems to help them to state-of-the-art performance on trajectory forecasting.

&amp;#x200B;

Second, the Social-BiGAT: [https://arxiv.org/abs/1907.03395](https://arxiv.org/abs/1907.03395)

Which uses a fun combo of Bicycle-GAN and graph attention nets to forecast pedestrian trajectories from multimodal data."
MachineLearning,"I mean. ""Attention is all you need"" is an awesome title considering how revolutionary the paper was. 

Of course, if your paper isn't revolutionary, then it makes it just silly. 

So I think people should do more revolutionary papers. Problems solved!"
MachineLearning,"I think it's partially an issue with it basically being understood that nobody writes papers because they want people to read them, and instead that people write papers because they want to have publications on their CV.

But I wanna write papers that people can read. I don't feel compelled to engage in academic dick-measuring."
MachineLearning,What kind of math background is required?
MachineLearning,"I'll give it a shot, China?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm a bit bothered when it's not clear what the acronym stands for. With BERT, we know it's about bi-directional transformers with a focus on learning an encoder representation. But a title like BART doesn't mention denoising, encoder-decoders, corruption or reconstruction, all of which are important aspects of the paper."
MachineLearning,"A bit of both, the first half of the lectures lean towards theory whereas the second half leans towards state-of-the-art methods. Colab code examples are given throughout to accompany a lot of the equations/algorithms, and the mathematical notation is introduced in the second lecture."
MachineLearning,Very interesting. Good work. It's always nice to see basic research investigating our basic assumptions of what the networks are doing.
MachineLearning,"What’s more annoying, especially in NLP, is papers with a “witty and smart” subtitle. Like, “half full or half empty? Exploring blah blah”

Just say the fucking shit you wanna say!"
MachineLearning,Does the series focus primarily on code or theory?
MachineLearning,"AWS has a reinforcement learning car racing thing that is beginner friendly. They send you a specific car though, so it might have more inputs than just a camera. It could probably give you some ideas though. It's called AWS DeepRacer"
MachineLearning,Why is one training epoch not enough? Why does doing multiple passes over the exact same dataset make the results better?
MachineLearning,Awesome thanks for sharing!!
MachineLearning,Is gradient decent used in all forms of ML? Are there any ML algorithms that don’t use gradient descent?
MachineLearning,It's because EfficientNet uses grouped convolutions which are really slow and inefficient on current gpu hardware. A Grouped Convolution layer is like 5-6x slower than a normal Convolution layer.
MachineLearning,Used to trigger garbage collection in Python 4
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,This is so cool. I'm going to need some gifs or videos showing the transitions though.
MachineLearning,The original paper said it best. Attention Is All You Need.
MachineLearning,"Petar, thank you for the excellent talk.  Toward the end, some of the discussion hinted at potential future applications of GNNs, like particle physics.  It strikes me that message passing GNNs could also rehabilitate agent-based models and bring them into the NN paradigm, where nodes, edges, and messages would correspond to agents, relations, and interactions.

Then GNNs could be applied to multi-agent environments like games, economic models, ecological models, distributed operating systems, swarms / crowds / herds, and other complex dynamical systems.  Has any such work been done in this space, outside of social network modeling?

It seems that GNNs are, as yet, a mostly undiscovered country.  Exciting."
MachineLearning,"I was talking about a respectable epidemiologist on the top of her field who had her privacy broken, her google drive files deleted by google employers. I don’t admit you talking this way about people like me who dedicated their whole lives to science, and who don’t have any sort of relationship with politics. You are a conspiracionist and a piece of shit. Pick up your political schizophrenics and shove it up your asshole."
MachineLearning,"My takeaway was that transformers are fully-connected, attention-based GNNs."
MachineLearning,IIRC Facebook's [Prophet](https://facebook.github.io/prophet/) supports multivariate forecasting as well.
MachineLearning,"Given the state of things, I would argue that ""\_ Is All You Need"" is clickbait."
MachineLearning,"""X Is All You Need""
""Towards X""
""Beyond X""
""Understanding X""

I think people need to expand their vocabularies."
MachineLearning,"Then you have retarded shit like

**HOTFUN**: tHat's not hOw The FUck you make acroNyms

I'm looking at you, Graph SAmpling and aggreGatE and a gazillion others."
MachineLearning,Thank you very much for the answer. I'll definitely look into this!
MachineLearning,AI researchers hate him: the 1 secret you need to unlock the true potential of AI
MachineLearning,*proceeds to squish itself out of the jar and waddles into the sunset*
MachineLearning,"Everyone cites it, nobody understand what the hell is written inside"
MachineLearning,"&gt;Two out of four accounts on this post have only one comment

Hmm"
MachineLearning,Technion (Israel)? That’s the only top tier university in the Middle East
MachineLearning,"During my PhD, the only thing the evaluation committee cared about was if the student had a paper accepted in a top journal. We had a list of these journals, which was pretty underwhelming and incomplete, but it was provided by the funding agencies, so no one questioned it. If we were able to achieve that, we where bureaucratically allowed to present our thesis.

It was really soul-crushing to reduce someone's research to a single checkbox. This completely corrupted the whole system and soon no one talked about their research ideas nor wanted to publish in conferences anymore, since those didn't count towards the prerequisites of the PhD program. Because of that, universities stopped funding student attendances to conferences. Students had very little contact with the field and had no way to come up with good ideas, nor anyone to talk to about theirs.

The way to push forward was just to accept that the system had failed and that we were only going to start doing real impactful research after the PhD. But those were 4\~5 of utter misery and loneliness to everyone involved.

Bonus points if anyone can guess from which country I'm from, since this basically happens on a national level."
MachineLearning,"They've become increasingly cringe of late, e.g.
'Oops I Took A Gradient: Scalable Sampling for Discrete Distributions'"
MachineLearning,"Thanks for your detailed reply and your thoughts!

Everything is politics, we all share the responsibility of pushing back against our flaws, prejudices, biases, etc. Now, that said, it's very important to separate friends from foes, picking one's battles, hills to die on, and so on.

&amp;#x200B;

Reading about this the whole mess is just a mega-mess on multiple levels. (Her communication style, Google/SF/Valley/CA/US/global politics/polarization, being right but being a dick about it, AI, ethics, AI-ethics, AI-ethics-but-at-Google, ethics-at-Google. Jeff Dean's answer and non-answer. Language. Climate Change. Language Models. Twitter [https://www.youtube.com/watch?v=osvjZOlTOGI&amp;t=48m22s](https://www.youtube.com/watch?v=osvjZOlTOGI&amp;t=48m22s) .)

&amp;#x200B;

[https://www.platformer.news/p/the-withering-email-that-got-an-ethical](https://www.platformer.news/p/the-withering-email-that-got-an-ethical) ... and based on this, my conclusion is that she is ""Damore all again"". She pointed out that Google's inclusivity/diversity team and program is a sham, because it's not taken seriously (no OKR, no monetary incentive, real day-to-day decisions are made without those who are vulnerable and affected, no transparency - so now it's virtually impossible to know why it was, if it had anything to do with that vulnerability, or the delay in PR &amp; Policy was usual or it was discrimination).

&amp;#x200B;

But at the same time micro and macro aggressions and harassment? WTF. If that's the case what are you doing there writing papers about language models and philosophizing about ethics. Get out. It's hard to take it seriously.

&amp;#x200B;

Oh, and of course everyone just sees that ""the AI company"" fired the ""AI canary in the coalmine"" ... great."
MachineLearning,you won't get a confirmation for any kind of application at facebook from my experience
MachineLearning,"My emotions run wild when I research. I can’t help it. I’m a man of culture and sophistication 

*tips neckbeard* m’lady 🥰"
MachineLearning,Sounds like someone needs to create an NLP model that creates effective titles for papers.
MachineLearning,Perhaps that's part of the joke.
MachineLearning,Why is the capitalisation so weird?!
MachineLearning,The title was actually quite clever :)
MachineLearning,"Titles are useless. With the influx of new papers, it's getting to the point where it is basically impossible to do research the old fashioned way, i.e. by reading titles and abstracts after making a search for carefully-crafted keywords in multiple scientific paper repositories. The fact that paper titles themselves do not contain the keywords we need for the papers to be caught by our search terms only accentuates this problem. Also, multiple people giving the same thing different names, usually by anthropomorphizing matrix multiplications with unintuitive (or at least non-standardized) terms is another factor in this.

Nowadays, we get almost all of our paper recommendations either from Twitter, someone who already read a paper and knows what it is all about, some website that sorts publications based on the similarity with our own research, or by directly being aware of the authors who publish in our line of research and stalking them."
MachineLearning,"&gt;I'm now realizing I know astonishingly little about GPUs... With luck an expert will chime in.

hehe, feel you :)

Thanks for the ideas. Similarly, my buest guess is just that the transformer is faster because its layers are just so incredibly simple in terms of data reorganization, like you say. 

Not sure if it would make a difference, if everything else is the same, but maybe this is also because the transformer is wide (layers) and short, and the CNN is narrower and deeper."
MachineLearning,thank you
MachineLearning,Where is dat PPO health insurance plan?
MachineLearning,I think that it is a symptom of papers that are otherwise unimpressive trying to get attention. The field may be slowing down and after attracting a horde of researchers hoping to make the same kinds of spectacular gains that have made the field popular they find that they still need to produce work that garners attention.
MachineLearning,"Except good literature searches generally focus on the text of the abstract and the keywords given by the author rather than just the title. Google search for transformers still gives the ""Attention is all you need"" paper because of this. Obviously Google does a lot more indexing than your typical publisher database but it's the same concept."
MachineLearning,Or it's a very specific dunder method
MachineLearning,[deleted]
MachineLearning,"This is just a guess, but it may have to do with how convolutional layers are represented and implemented.  A conv layer can be represented as a sparse subset of a dense layer, and there could be additional (non-floating point) operations in translating between sparse and dense representations.

Meanwhile transformers are generally all dense, which would mean more FLOPs, but less data reorganization on the GPU.  It also may depend on the GPU architecture and instruction set.  See Figure 8 here: [https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)

I'm now realizing I know astonishingly little about GPUs... With luck an expert will chime in."
MachineLearning,lol this thread is gold
MachineLearning,Is there a solution proposed other than a meta-level overview to benchmarking abstraction/few-shot learning ability? I think the paper was leaning towards probabilistic program induction compared to symbolic/DNN approaches but I haven't read it fully yet.
MachineLearning,Great learning for beginners!
MachineLearning,"This is awesome, thank you! I just ordered Sutton and Barto, so will be great to follow along with."
MachineLearning,"I once built a seq2seq model on a very small dataset and was elated to find that my model was 100% accurate. Then when I got to the inference / generation stage, I realized that I had accidentally trained the model to predict current input, rather than the next token... lol. I was crestfallen.

My advice is to learn to handle negative emotions from experience, but in a few-shot regime."
MachineLearning,"Yes, an Irish researcher named Oscar Wilde."
MachineLearning,Your forgetting the other annoying trend of giving your model some kind of brand name instead of just describing what it does.
MachineLearning,"Title:Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval  

Authors:[Wenhan Xiong](https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong%2C+W), [Xiang Lorraine Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+X+L), [Srini Iyer](https://arxiv.org/search/cs?searchtype=author&amp;query=Iyer%2C+S), [Jingfei Du](https://arxiv.org/search/cs?searchtype=author&amp;query=Du%2C+J), [Patrick Lewis](https://arxiv.org/search/cs?searchtype=author&amp;query=Lewis%2C+P), [William Yang Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+W+Y), [Yashar Mehdad](https://arxiv.org/search/cs?searchtype=author&amp;query=Mehdad%2C+Y), [Wen-tau Yih](https://arxiv.org/search/cs?searchtype=author&amp;query=Yih%2C+W), [Sebastian Riedel](https://arxiv.org/search/cs?searchtype=author&amp;query=Riedel%2C+S), [Douwe Kiela](https://arxiv.org/search/cs?searchtype=author&amp;query=Kiela%2C+D), [Barlas Oğuz](https://arxiv.org/search/cs?searchtype=author&amp;query=O%C4%9Fuz%2C+B)  

&gt; Abstract: We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of- the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human- annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.  

[PDF Link](https://arxiv.org/pdf/2009.12756) | [Landing Page](https://arxiv.org/abs/2009.12756) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2009.12756/)"
MachineLearning,"For layernorm, will it be a problem if the distribution of the training data is diff fm the distribution of the test data? I read online that this is 1 of the downsides of layernorm"
MachineLearning,have you looked at pix2pix papers?
MachineLearning,"don't worry, it will not show up in any literature search and thus your guilt will be quickly forgotten. You are free!"
MachineLearning,"They still contain data. There might be some loss of info because it's not a bijective transformation because it's periodic, but depending on the range, you can combine the knowledge of sin/cos/tanh to figure it out. Decision trees are powerful generic models, it's not really surprising. Our models exist to spot patterns in data, not just spot patterns we can spot."
MachineLearning,counter argument: a descriptive paper title has actually the chance of showing up when doing a literature search.
MachineLearning,ML Researchers HATE this one weird trick!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Good thread on the unreasonable effectiveness of clickbait titles!
MachineLearning,"I just tried this notebook. It worked fine for me. I'm not sure what is causing your issue. I recommend closing your browser and trying again if you haven't already. If that doesn't work, you might want to try a different app that is on the list linked to in this post. Perhaps try notebook Text2Image_v3."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,The same reason why the World Series is only played by American teams.
MachineLearning,"Ready for the age of clickbait in academia? 
“I put a Transformer on NSFW images...you won’t believe what happened next [gone sexual][gone wrong]!!!!”


In truth, I believe the seriousness of the entire field has been severely degraded lately. Perhaps it’s the byproduct of the pace of progress, but there’s very little attention given to creating serious, weighty papers. Even at top-tier conferences, most papers are minor variations on a theme. 

Such “minuscule improvements” need a venue, but frankly ML theory journals and conferences SHOULD NOT be it! Not sure what the alternative is, but I’d be much happier to see conferences with far fewer publications, but where each publication can be considered an actual advance and piece of relevant information. 

At present we risk burying truly revolutionary concepts under a pile of “Variation on self-attention #30000”."
MachineLearning,That'll get better with time. Your generator will improve
MachineLearning,"Hedge your bets on multiple different projects. One or two main projects where you're first author, and one or two more projects when you're second author. That way if one fail, you're ok."
MachineLearning,I’ve applied for MSc with udem profs but havent received an interview yet—does that mean I’ve been rejected? I got the email saying my request is valid until the end of next month but given that people have gotten interviews already i can only assume I’ve been rejected.
MachineLearning,"Same lol, I'm sure down the line there will be a more efficient approach :)"
MachineLearning,can it run using cpu-only? how much system memory does it use when runing cpu-only?
MachineLearning,"An error keeps happening, it says that “perceptor, preprocess = clip.load(‘ViT-B/32’) is an error when I don’t know anything about AI."
MachineLearning,It’s a spam dataset gotten from Kaggle.com
MachineLearning,"What is the type of dataset?  
Feature extraction procedure depends on the kind of dataset, for ex: tabula, image, time-series etc."
MachineLearning,"cool. Yeah I don't have that, lol."
MachineLearning,"By the way, I am open to collaborations with researchers around the world, not just those in the Middle East."
MachineLearning,"Sounds great!

Please check your inbox I sent you a message :)"
MachineLearning,"I went into grad school knowing that research is 99% failure. So although of course I'm bummed when things don't work out, I'm not surprised.

One hurdle I am still having trouble with is getting over paper rejections. It sucks when you work really hard for 1yr+ and then reviewers cut you down."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yes, this was for example done for epilepsy detection in this paper (not mine)

https://arxiv.org/abs/1907.10518"
MachineLearning,"So, /r/AIDungeonNSFW/"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You're fucking ridiculous and out of your mind if u think people should only be applying to one school rofl
MachineLearning,"I strongly dislike it as well, if that helps. It is completely uninformative and tells very little about the content of the paper."
MachineLearning,"Vector autoregression is the classical method for multivariate forecasting, and is available in statsmodels:
https://www.statsmodels.org/stable/vector_ar.html#var"
MachineLearning,"Very interesting. What's the limit in size for M? If I have a data matrix that has 100 rows, each of which is &gt;1 million dimensions, would this work (i.e., M is 100 x &gt;1 million)?"
MachineLearning,"You can try gluonts or [app.actable.ai](https://app.actable.ai) for free (shameless ads, I am the CTO)"
MachineLearning,"Title:Topology of deep neural networks  

Authors:[Gregory Naitzat](https://arxiv.org/search/cs?searchtype=author&amp;query=Naitzat%2C+G), [Andrey Zhitnikov](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhitnikov%2C+A), [Lek-Heng Lim](https://arxiv.org/search/cs?searchtype=author&amp;query=Lim%2C+L)  

&gt; Abstract: We study how the topology of a data set $M = M_a \cup M_b \subseteq \mathbb{R}^d$, representing two classes $a$ and $b$ in a binary classification problem, changes as it passes through the layers of a well- trained neural network, i.e., with perfect accuracy on training set and near- zero generalization error ($\approx 0.01\%$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like ReLU outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of $M$ we begin with, when passed through a well-trained neural network $f : \mathbb{R}^d \to \mathbb{R}^p$, there is a vast reduction in the Betti numbers of both components $M_a$ and $M_b$; in fact they nearly always reduce to their lowest possible values: $\beta_k\bigl(f(M_i)\bigr) = 0$ for $k \ge 1$ and $\beta_0\bigl(f(M_i)\bigr) = 1$, $i =a, b$. Furthermore, (2) the reduction in Betti numbers is significantly faster for ReLU activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.  

[PDF Link](https://arxiv.org/pdf/2004.06093) | [Landing Page](https://arxiv.org/abs/2004.06093) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2004.06093/)"
MachineLearning,Really great I'm an egyptian engineer works in AI research team. Really interesteing in reads paper and implement it. I implemented 8 papers in nlp so far. I have a lot of new ideas we can work on it.
MachineLearning,"in terms of quality, latency, or both?"
MachineLearning,Solid advice!
MachineLearning,"Assuming that you are talking about a pseudorandom number generator running locally, I think this is possible in theory in the sense that an expert could potentially look at the state of your system's RAM and try to reverse engineer the seed plus the algorithm used. But in practice I'm not even sure it's possible given existing technology.  


If you are talking about a TRNG or a PRNG running remotely in a black box, I'm extremely skeptical."
MachineLearning,To the extent of how much the (pseudo)random number generator sucks.
MachineLearning,I guess it's basically a form of augmenting data if the gan truly becomes a good generator. Buts hard to say
MachineLearning,"Learning something new to get some motivation and inspiration. What actually helps as well is teaching. That way you will see how much you've learned already. Doesn't directly affect the output, but can help you feel empowered"
MachineLearning,Have anyone tried if any ML algorithm could reverse  hashing like SHA1 better than random? Assuming that the input is shorter than the hash.
MachineLearning,"The generators work that even a smallest change in the input changes the outcome entirely. This for sure won’t work for on the time data because of nanoseconds, and furthermore ML is currently based on the “smallest change in input - smallest change in output” type of data, so its not useful at all."
MachineLearning,"After more than 3 years in research i would say it is now that i am able to be indifferent toward both losses and gains. It is normal to have a paper rejected or accepted. You have to teach yourself not to get too attached to a work/idea and moving on. What the worst thing that could happen if your idea doesn’t work or your paper got rejected? Not a big deal, you will find another idea or submit to another conference. The time you spent on the work is not wasted as you learned a lot. Furthermore, there is no deadline really in research (there is no client waiting for your results!!), so just relax, and try to keep your mind off work when at home, try to find a passion in parallel, for me sports helped a lot to release all the stress and negative energy. Be strong and stay positive, good luck !!"
MachineLearning,"&gt; maybe I'm doing my literature searches wrong

You're not doing them wrong! My point is how searches differ according to scale.

To get my point across, try the exercise of thinking how would you proceed if you had to go through 10, 100, 1000 times the volume that you normally go through.

In what contexts these large-scale searches happen? I can give you at least three, the first two of which I have personally gone through in my works, and the last of which I have merely observed.

First, consider the problem of attribution. For instance, in mathematical sciences, this is taken particularly seriously. It is not merely a matter of recognition. Contributions may otherwise be divorced from knowledge collectively acquired throughout generations. (People laugh at physicians rediscovering calculus [1], but when celebrities of the machine learning world disregard prior discoveries it's suddenly OK.)

Second, consider the problem of patentability [2]. It is a matter of legal compliance to the letter of the law that for a subject matter to be patentable it must possess the quality of absolute novelty. That is, no other publicly documented work (ever) has disclosed the same process, machine, manufactures, or composition of matter as you. Try proving that! Patent searches are thus extremely exhaustive.

Third, consider the problem of surveying literature (e.g., systematic reviews). Some meta-analyses in medical sciences pool upwards of 30000 papers including clinical trial reports, case reports, clinical studies, etc. Can you imagine case reports having clickbait titles?

Therefore clickbait titles in machine learning completely disregard at least one key aspect of knowledge, like /u/mcorah said, being able to classify pertinence with respect to a given topic.

[1] https://www.forbes.com/sites/alexknapp/2011/11/10/apparently-calculus-was-invented-in-1994/  
[2] https://www.uspto.gov/web/offices/pac/mpep/mpep-9015-appx-l.html#d0e302376"
MachineLearning,"IMO ""X is All You Need"" is 100 times better than ""[GenericName]Net"". Reminds me of how everyone names their Minecraft servers ""[GenericName]Craft."
MachineLearning,"counter argument: of all the most influential papers of the past decade, only the transformer paper had a ""cute"" title.  In contrast, the papers of AlexNet, GAN, word2vec, Seq2seq, Batch norm, Adam, AlphaGO, etc, all had ""standard"" titles.  For this reason I don't buy it and I expect the ""humble"" to continue to dominate.

What I think is happening is the extreme success of the transformer paper makes people copy all of its aspects, including the cute title.   I predict that the next ultra dominant paper will have ""conventional"" title, and then people will copy its style.   But at present, people will continue copying ""X is all you need"", in the misguided hope that doing so will help them be just as successful."
MachineLearning,"I don't have any expertise, just practical experience. At the end of the day what matters how optimized are the operations of a certain model on the CPU-GPU hardware. It is the reason why FLOPS and parameter counts are quite meaningless in reality many times because different models use different internal components."
MachineLearning,r/markdownfails
MachineLearning,"Yeah this is crazy. Thats unfortunate. In case you have any expertise in this, maybe you can give me answers.

The 30 GFLOPs network is a 2D CNN. The 40 GFLOPs networks is just a transformer encoder. Both about the same number of parameters. Would you have any idea why with almost the same amount of FLOPs, the CNN is 3.5x slower to perform a training step?"
MachineLearning,"&gt; crushed when hitting the first ""bug"".

First bug?

Or first bug this hour (happening 20 minutes into the hour)?"
MachineLearning,"Maybe, as data scientist, we should rely on data science (such as e-discovery kind of stuff) and not on titles to find interesting reads."
MachineLearning,"A model such as random forest (or anything really from scikit learn) doesn't use any spatial information anyway. You can permute all the features and it should learn the same thing.

Order matters for models such as convolutional networks."
MachineLearning,yeah that would be my feeling as well. Could be nice to have to supplement an already  made dataset but even then I would worry about how well it would be.
MachineLearning,"I think ""Attention is all you need"" from Vaswani et al actually is a good title. But only for this paper."
MachineLearning,"Completely agree! I think that having the method in the title can sometimes even be an _asset_. As you said, it makes browsing especially easy, like if I want to find an application of X for problem Y."
MachineLearning,"&gt;Do you know of any online advanced tutorials or other sources that you would recommend to learn JAX properly?

nop sorry, I just know that there is an increased interest in it but I have no use for it for now"
MachineLearning,"I still don't understand the whole ""X is all you need"" craze since it's really a horrible title, and not even great clickbait. Besides the reason that people want to join the ""Attention is all you need"" bandwagon due to the popularity of the paper, I really don't get it."
MachineLearning,"I know, the chances of getting into Google Brain or DeepMind are quite slim...

However I think that if I didn't manage to get accepted, I would work for a year than pursue a PhD, which I would definitely want to do with JAX.

Do you know of any online advanced tutorials or other sources that you would recommend to learn JAX properly?"
MachineLearning,"&gt;X = X.reshape((n\_samples, -1))

The problem is that I have shape = (windows, time series, timestamps), if I reshape the data, will not  I lose information?"
MachineLearning,"Yeap, e.g. deep layer aggregation (dla) models give pretty higher accuracy for ImageNet with similar parameter counts compared to MobileNet and similar small models...but their runtime-training performances are terrible. And when I checked MobileNetv3, GhostNet etc. ... their claimed actual runtime performance improvements in the papers are vaporware."
MachineLearning,"""Machine learning researchers \_\_hate\_\_ it"""
MachineLearning,"To do good science you must be objective. Unfortunately we live in a time of great human folly where intellectuals are actually arguing against the objective stance. However, if there is one thing that computer science teaches you it is that the world works according to strict rules and you will be stymied if you refuse to accept how things work.

The quickest way to move forward is to realize that you might be mistaken. People that place a huge emotional investment in always being right often get stuck on a problem.

One example I encountered is when a web developer used a tag attribute which later came to be used for form validation. He simply could not understand why his form was doing form validation when there was no JavaScript there for that. I tried to explain to him that browsers now have built-in form validation which can be triggered by using new tag attributes. But this web developer simply refused to accept that he could be wrong and insisted that there was JavaScript somewhere. I caught on much quicker because I questioned my assumptions."
MachineLearning,"Title:Towards Causal Representation Learning  

Authors:[Bernhard Schölkopf](https://arxiv.org/search/cs?searchtype=author&amp;query=Sch%C3%B6lkopf%2C+B), [Francesco Locatello](https://arxiv.org/search/cs?searchtype=author&amp;query=Locatello%2C+F), [Stefan Bauer](https://arxiv.org/search/cs?searchtype=author&amp;query=Bauer%2C+S), [Nan Rosemary Ke](https://arxiv.org/search/cs?searchtype=author&amp;query=Ke%2C+N+R), [Nal Kalchbrenner](https://arxiv.org/search/cs?searchtype=author&amp;query=Kalchbrenner%2C+N), [Anirudh Goyal](https://arxiv.org/search/cs?searchtype=author&amp;query=Goyal%2C+A), [Yoshua Bengio](https://arxiv.org/search/cs?searchtype=author&amp;query=Bengio%2C+Y)  

&gt; Abstract: The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.  

[PDF Link](https://arxiv.org/pdf/2102.11107v1) | [Landing Page](https://arxiv.org/abs/2102.11107v1) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.11107v1/)"
MachineLearning,"&gt;Basically if you read ""AI"" somewhere, it's 99% marketing, 1% maths and 0% ""artificial consciousness / artificial human-like intelligence"".

Thanks. This is a very helpful explanation."
MachineLearning,Thanks for the feedback—I appreciate it.
MachineLearning,"""Before we go to the methods section let us first quickly introduce our sponsor RAID SHADOW LEGENDS""."
MachineLearning,"Yeah. I got to witness this first hand when i was doing some testing just now. In average training step time, A model that has 30 GFLOPs took about 3.5x as long as another model I was testing that has 40 GFLOPs. Don’t know how anyone can use FLOPs for speed comparison after this."
MachineLearning,"All valid points, but.....

&gt; actually quite stupid and power hungry to train

So are babies :)"
MachineLearning,"If it's a biased random number generator (think a weighted coin, that comes up heads 70% of the time), the ML model could learn something. But for a fair and truly random, ML shouldn't be able to learn any patterns. That's kinda the definition of random."
MachineLearning,"Another common trope that I find annoying is all the Towards X titles for papers that maybe introducing newer problem settings, especially because searching for them more often than not leada to lower quality blog posts rather than the actual paper."
MachineLearning,"I think we can all agree that if your new method X will be as impactful in as many domains as Transformers have been, you too are allowed to title your paper _X is All You Need_."
MachineLearning,'predict' or 'random' : pick one.
MachineLearning,Even FLOPS do not help to judge the runtime performance between models presented in the papers. What matters in reality when you run their models on your bare CPU and GPU metals to see the actual training/inference performance. I don't give too much trust in reported FLOPS and parameter counts anymore that they use in papers to make claims.
MachineLearning,"Despite the fancy naming, machine learning AI is hardly intelligent in the way that a human or intelligent creature is intelligent.  It is far from that right now.  It is actually quite stupid and power hungry to train.  The state of the art right now is fancy math (stats, probability, linear algebra, multivariable calculus) that uses large data sets to increase computer prediction performance on specific tasks (e.g. image recognition, language processing/generation/translation).

Right now there's almost no consideration for ethics.  Ethics are a joke to computer scientists and programmers.  If there is any concern for ethics, there is some concern to the fact that human data up till now is biased, and the learning algorithms are learning the biases."
MachineLearning,"Also, coming in from other sciences, it is really frustrating to see the Abstract, instead of being an executive summary, act as a teaser for the paper. I've found it to be more prevalent in ML papers for some reason."
MachineLearning,"&gt;I'm sorry if these are naïve or science fiction-y questions

They are but don't be sorry, that's how some newspapers present things.

Neural networks we're currently using are just a bunch of matrices we add and multiply. It's just maths. Some people aim at doing maths to create higher level intelligences (AGI, Artificial General Intelligence) but no-one has done something that would be credible in this area.

Re-ask in 5\~10 years and I may change my answer. For now it's just high-tech maths, no consciousness.

Basically if you read ""AI"" somewhere, it's 99% marketing, 1% maths and 0% ""artificial consciousness / artificial human-like intelligence""."
MachineLearning,"If the generator is time based and you provide very accurate time data, maybe you could get something. But it seems like a bit unlikely.

At best you'll probably just have 1/n accuracy since at the end of the day you can't really predict something random."
MachineLearning,I believe they made a Zoom plugin for it. But to run at 60fps 1080p you need a RTX 2080 Ti.
MachineLearning,"It depends on the generator but the model will not learn the logic behind the generator (for a great generator), it'll just act like a memory"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It's a bad idea to learn something just to apply in a specific company. Think about all the possibilities. You'll probably not be accepted in Google/Deepmind because these jobs are highly competitive and when there's a high competition the randomness of the selection is amplified.

If you search for one person and you have 100 great applications, you just read the 20 first and take the best one. Nobody will care if you wrote ""jax"" in the 80 next resumes.

Learning jax is a great idea if you already know how to use Pytorch well, but don't learn jax to apply at Deepmind."
MachineLearning,"Try taking a learning orientation, not a performance orientation to research."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"People have been trying to game the paper-title system forever I find. When I was in college, paper titles were excessively long in the hopes somebody perusing the paper titles would be impressed by the supposed complexity of it.

Things will tone down, and then some new fad will come up. It's an inevitable outcome of the publish-or-perish culture."
MachineLearning,"I would be interested in using that. I created a Minecraft GAN music video yesterday at [https://boredhumans.com/minecraft.php](https://boredhumans.com/minecraft.php) but I added the song after the images were generated. Unlike your system, my GAN did not know what song I was going to use."
MachineLearning,A decent title should at least be sufficient to determine a work *isn't* relevant to the reader. That doesn't work is the title isn't even sufficient to identify the topic.
MachineLearning,[Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One](https://arxiv.org/abs/1912.03263)
MachineLearning,[deleted]
MachineLearning,"Research seems to have become the ""selling of ideas"" over ""enrichment of knowledge"". And it's understandable. When there are on average 3-5K papers on ArXiv every month for just CS then people need to put flashy titles to ""sell their ideas"". 

I also just found out that there were 3 papers published on the same day that use ""All you need"" for a title :

1. [**Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer**](http://arxiv.org/abs/2102.10772v1)
2. [**Optimism is All You Need: Model-Based Imitation Learning From Observation Alone**](http://arxiv.org/abs/2102.10769v1)
3. [**Do Generative Models Know Disentanglement? Contrastive Learning is All You Need**](http://arxiv.org/abs/2102.10543v1)

What a coincidence! (or is it?)"
MachineLearning,[deleted]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The simplest way to go about it would be to establish some distance measure for frames and then go through all videos to find the frames with the least measure.

A simple measure would be cosine distance on color histograms or [HOG](https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html).

If you have so many videos or frames that its unfeasible, then I would do it this way:

1. Reduce the framerate of videos to 5 or so using ffmpeg.
2. Go through all frames of all videos and extract feature vectors. Anything like HOG works, but the best way is to use an Imagenet pretrained neural network, like Efficientnet, with the final classifier layer removed, so that the network outputs feature vectors. You might find the timm python library useful
3. Put all feature vectors and the ids of frames (something like ""{video\_title}\_{frame\_num}"") into a searchable index. I suggest the FAISS library.
4. To find the frames that resemble your input frame (the one you are searching for), extract the feature vector from the input, use faiss to find the closest feature vectors in the index, extract ids of these vectors. 

So you don't even need to train anything, unless the frame you are looking for is very dissimilar in each video."
MachineLearning,"IMHO, this problem could be alleviated, regardless of individual views on aesthetics of research (seriousness vs lightheartedness), with a very simple approach: letting people change their titles after peer-review. A simple ""weak accept, pending to strong accept if this cringy af title is changed"" would do wonders."
MachineLearning,Seems like an opportunity for someone (or some text summarization software) to write a good one-sentence summary/abstract of papers and a search engine to index them with those in place of the title.
MachineLearning,"Are you using layernorm layers? They might help as they scale the intermediate activations of other layers, so your gradients vanish less.

Maybe it's a supid idea, but you could also multiply the latitude and longitude values by 10\^5 or something. That will help if the residuals are so small that you run into float precision issues. You could also try other transformations on output. log1p perhaps? I have no idea, worth a try."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,C.L.A.S.S.I.C.
MachineLearning,"I think some good comes out of it, in that people then model their own paper after the well-written successful papers, which in turn makes their paper much better. Best way to write a good paper is copy someone else’s style that works well. That being said, I agree that I’d rather see Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification than Rectifiers is All You Need or pix2pix."
MachineLearning,"As someone who just submitted a paper with title ""Size Matters"", I kind of feel guilty regarding this..."
MachineLearning,Obama is Using One Weird Trick to Send AIs Back to College!
MachineLearning,"Google search is an interesting case -- A recommendation system where we \*accept\* approximate answers only because no correct answer is possible (because the problem is underspecified).  But anywhere a problem is fully specified, and a correct answer possible, who would accept an approximation?  

We \*do\* want to be able to generate exact software, and some day we will get there.  But some breakthroughs are still needed, if we're to somehow leverage / distill current DL approximations into software-as-we-know-it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Reminds me of this old joke site: http://oneweirdkerneltrick.com/
MachineLearning,They became the very thing that they swore to destroy?
MachineLearning,"Oh man I wish I have 10k to work with consistently, more like 1k to 10k"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Title:Abstraction and Analogy-Making in Artificial Intelligence  

Authors:[Melanie Mitchell](https://arxiv.org/search/cs?searchtype=author&amp;query=Mitchell%2C+M)  

&gt; Abstract: Conceptual abstraction and analogy-making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite of a long history of research on constructing AI systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.  

[PDF Link](https://arxiv.org/pdf/2102.10717) | [Landing Page](https://arxiv.org/abs/2102.10717) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.10717/)"
MachineLearning,[We used Neural Networks to Detect Clickbaits: You won't believe what happened Next!](https://arxiv.org/abs/1612.01340)
MachineLearning,"The enterprise of science is at least two dimensional - (i) doing good science and (ii) presenting your science. (In fact, you could replace ""science"" with anything else too!)

Knowing that, I'd argue to the contrary. I think if the paper has substantial meat, it is an absolute necessity that the scientists behind the research make the first impressions as sticky as possible. There is nothing wrong with it at all. 

The problem is with dilettantes reading too much into the title without due diligence. That also, however, is an artifact of the popularity of any field. The ideal citizen of this ""science"" enterprise would be aware of the pitfalls, and I promise you most of them are. But then, no one owns this enterprise. If a few dilettantes venture into making a title a big deal, who cares. May be they have something real there."
MachineLearning,"The funny thing is that electronic circuits people do use titles like that. For example: ""A 280 μW, 108 dB DR PPG-Readout IC With Reconfigurable, 2nd-Order, Incremental ΔΣM Front-End for Direct Light-to-Digital Conversion"""
MachineLearning,"I would guess within the next few decades. Could happen by 2030, but hard to predict"
MachineLearning,In ALL CAPS
MachineLearning,I cope with it by reading this thread. Soothing to know I'm not alone in this.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for the alternative take. I agree with most of what you said, but still not sure what to think of this title business, since there's objectively an incentive to making clickbait titles, as things are...

Just one thing though, idk if maybe I'm doing my literature searches wrong but don't you need to skim through the abstract anyway, instead of just the title?"
MachineLearning,"Yes. FLOPS are often used to assess how well you're saturating the hardware and the memory bandwidth. If you think latency is also relevant, then report the latency too. Just make sure that what you're reporting actually show what you claim."
MachineLearning,"This is a trend that's going to fix itself over time: Currently, we have a mass influx of papers in ML, but that trend is not going to hold on forever: Either we are going to have a reduction of papers due to the increase in difficulty of creating meaningful/publishable work or we're going to have a reduction because the ML bubble pops and less funding is available for research.

In either case, the root cause of the ""viral marketing"" system for naming papers is caused by the overproduction of ML papers, which forces authors to be more aggressive when naming their work is going to solve itself during the maturation of the new-age ML field."
MachineLearning,Nice thank you for letting me know!
MachineLearning,I search for papers based on the methods used rather than the hyppthetical results so I'd like that in the title.
MachineLearning,Just sayin but please try to not use footnotes. They tend to really annoy sone people and many conference formats explicitly state not to (ab)use them. Just put it in the text or the captions.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"yes that's definitely true today but it need not necessarily be the case.   


Imagine that we just accepted that the benefits of DL were enough to tolerate some fraction of errors, there are ways to build around this. For example you can build fault tolerant UX like google. Google search is not 100% accurate but instead returns you a ranked list so even if its wrong its useful. We can also have fall back to humans in uncertain cases or defer to rules based systems in uncertain cases. If we're creative, I think there are lots of situations where we think we need 100% accuracy where we might actually not."
MachineLearning,*Marcus Aurelius has entered the room*
MachineLearning,Did you adjust your anchor boxes to account for your smaller objects?
MachineLearning,"""Reinforcement Learning with Bayesian Kernel Latent Meanfield Priors"" actually sounds like it would be pretty good. ;)"
MachineLearning,"In a paper I am aiming to publish I am also trying to compare model speeds and how much compute they take. In spite of these hardware variations, do you think it is more helpful/robust to report both FLOPs of each model and also average runtime (seconds) for a training-step, inference-step, all tested on the exact same hardware and GPU?

u/johnnydues  
u/officerblues  
u/Red-Portal"
MachineLearning,*insert image of surprised looking encoder*
MachineLearning,“Social experiment prank on artificial mind [Gone Sexual]”
MachineLearning,I feel like this is really on the journals. They should reject uninformative titles like this.
MachineLearning,I tested it. Result is much worse than MODNet.
MachineLearning,Thanks. Varying the model architecture is not an option. I'll look into the acronyms you mentioned.
MachineLearning,"Hello everyone 
I’m a newbie learning ML
I’ve been doing a lot of research 
Could anyone put me through on how to do a feature extraction on a dataset I got online using any of the ML techniques please?"
MachineLearning,"SSD is an old network to be honest, you should probably try small yolov5/4/3 / efficientdet / centernet.

There's also results on using more attention layers: CBAM, ECA, ASFF, and other stuff: using GIoU loss, Dropblock, EMA, RandAugment etc.. etc..

There's tons of things but if you're using SSD you'll probably want to switch to a more recent ""framework"" (I include the dataset preprocessing, the model architecture, the optimizer, the loss etc..) before trying other stuff

It's not always specific towards small objectfs but for having worked on that task, it's hard to improve the mAP just for small objects."
MachineLearning,"Defining intelligence is pretty simple, it requires only four things:

1. The ability to perceive the environment.
2. The ability to understand what actions can be taken.
3. The ability to predict the outcome of those actions.
4. The ability to choose and execute one of those actions based on an evaluation of desired outcome.

Quantifying intelligence is far harder and ranges on a spectrum of inanimate to omniscient. And of course an entity can have different levels of intelligence at different tasks.

While this paper is a somewhat interesting thought experiment, it lacks any real specificity about how its mechanisms would be achieved in practice. Similarly it's riddled with tons of hand waving and simplifying assumptions that I strongly doubt would hold up in even the most basic attempt at implementation (at least under current state of the art techniques). While interesting, I would treat this as only a slight step beyond science fiction."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,\*You wont believe the results at the end of the paper\*
MachineLearning,"I would like to introduce a project I created. The name of the project is PyTorch-StudioGAN :)

Github: [https://github.com/POSTECH-CVLab/PyTorch-StudioGAN](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN)

\[Features\]

* Extensive GAN implementations for PyTorch
* Comprehensive benchmark of GANs using CIFAR10, Tiny ImageNet, and ImageNet datasets
* Better performance and lower memory consumption than original implementations
* Providing pre-trained models that are fully compatible with up-to-date PyTorch environment 
* Support Multi-GPU (DP, DDP, and Multinode DistributedDataParallel), Mixed Precision, Synchronized Batch Normalization, LARS, Tensorboard Visualization, and other analysis methods.

Thank you!"
MachineLearning,It's just sense-of-humor
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; What are you thoughts on the recent trend of ML paper titles?

Have a look at 60 years of ""considered harmful"" knockoffs and ask yourself

1. why you think this is new, or
1. why you think this is a machine learning thing

Someone should let the paper authors know that you can go check citation rates, and that papers with knockoff titles are shooting themselves in the visibility foot"
MachineLearning,"yes, and no"
MachineLearning,"Business software needs to work 100% of the time; even 99.9% accuracy is unacceptable, in the vast majority of applications.  So DL needs to be able write correct (and likely human-interpretable) Python or whatever, before it will be widely applied.  

For program synthesis, DL approximations can still be useful to guide search through program-space, but need to be translated into something symbolic and interpretable in the end."
MachineLearning,"You are definitely right! And on the long run there is no alternative to give up the inner resistance to acknowledging and accepting your emotions (and to be kind to yourself).

I just feel that under acute pressure and stress (like during a PhD) that process can sometimes be too painful and too time consuming. I see the the numbness / emotionlessness as basically the result of deferring the process of getting in touch with oneself for an extended period of time.

But one should definitely avoid being out of touch with yourself if you can help it and for extended periods of time, I completely agree. I've been paying the debts for neglecting my mind and body for a couple of years now and will probably need another couple of years to fully recover from that phase."
MachineLearning,"Agreed, I don't think everyone needs to start titling their papers ""[Method] is Correlated with a 0.01% Improvement in mAP/accuracy/BLEU/etc."""
MachineLearning,"I wasn't just providing an argument from authority, I was suggesting that we have excellent examples of deep learning outperforming as you described it ""mathematical algorithms"".  E.g machine translation, speech recognition, document understanding etc. Almost all perceptual tasks that we tried to solve by traditional programming have been replaced by DL now.   


I think disagreeing with Kaparthy is interesting (thats why I made the post), what I want to know is why you disagree?   


If I understand correctly, you think the lack of generalisation guarantees will limit the adoption of DL for tasks that could otherwise be solved by conventional software.  I think I agree with you on this one.   


But there are lots of tasks that are poorly solved by traditional software (language understanding being a good example) that I think will become core to almost every application."
MachineLearning,"Indeed!

I am wondering which side to pick when publishing results in an upcoming paper. Should I report FLOPS and actually count the adds and multiplies as separate operations like seems right. Or should I just count an add and a multiply as a single FLOP like I have witnessed in these papers?"
MachineLearning,"&gt;  ""X is all you need"" is a bad title. All I need? For what?

This may be true for most uses of it but the original *Attention is All you need* may have been much more accurate than we could even imagine at the time considering all of the recent successful forays into other domains that transformers have had."
MachineLearning,Original post https://twitter.com/GiorgioPatrini/status/1361325923698675723?s=20
MachineLearning,"Exactly, that's the Scientific Attitude... carefully stating facts, aware of all nuances and complexities, paying attention to the details and not only to the more eye-catching results."
MachineLearning,Also some catchy thumbnails à la youtube would be nice
MachineLearning,"I agree, catchy titles in science are like sexy nicknames: they only work if you are sexy anyway, otherwise they make you look silly."
MachineLearning,"A small dataset can be constructed out of large datasets. So on a general level, one can do this in any field. Few-shot learning is one branch of research that is being done in many domains.  
Of course, the practical impact and importance of such research will be higher where data is scarce and/or expensive to aquire. Customized/personalized models for example. Medicine."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"\&gt; **Altering model architecture to take advantage of low resource hardware:**

That's a very interesting situation many face in production. It's also one of the reason why we don't prefer shiny new big model compare to medium-small size model with 2-3% less accuracy. For example, our initial classification model was 96% accurate and latency was like 2000ms for inference on a batch of 1000 sentences; since our goal was to bring down the latency under 1000ms, we had to alter the model architecture. With new model, accuracy was 94% which was totally doable in our use case.

**&gt; Use more than one model to take full advantage of all resources of machine:**

If you have more than one model, you can take advantage by sharing hardware resources between models. For example, put the CPU heavy model to take advantage of CPUs &amp; whenever you get occasional requests for GPU bound model, use the GPU. This can be done easily with docker + TF Serving.

**&gt; Use spot instances:**

As you mentioned the requests are like not continuous and machine is ideal for 98% of time; you can use spot instances with a fraction of price compared to on-demand instance. There is also reserved instance type."
MachineLearning,"&gt; I don't mind titles that describe the method instead of the results, it makes those papers easier to browse and find. When I'm looking for a paper, the title is (obviously) the first thing I look at, then I go look at the result section to see what their baselines were and what they concluded, and if it piques my interest I then read the entire paper.

I don't mind those either, and to the reason I would add: what about papers that don't clearly improve on state-of-the-art? Applying a method to a problem in a novel way and reporting what happened is in itself a novel contribution that should be public.

In fact, I think it's reasonable to expect most papers to not be groundbreaking. If they are, great, but otherwise, distilling the complex observations of experiments into one simplifying impressive conclusion is what the news are for."
MachineLearning,Could you please share the link to the published paper?
MachineLearning,"Yeah its not ideal.

A factor 2 in theoretical FLOP/MACCs might very well be drowned by other factors when it comes to real-life performance though, like compute vs memory intensity and access patterns."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I feel like you should be able to safely expect at least a tiny amount of imagination in the reader.

Take your example of ""Reinforcement Learning with Bayesian Kernel Latent Meanfield Priors"". I don't think there's so much ambiguity there that you'd be confused to learn that they think Bayesian Kernel Latent Meanfield Priors are good for *something*. It's not like you're expect the full title should have been ""Reinforcement Learning with Bayesian Latent Meanfield Priors are a Thing I Tricked You Into Thinking This Paper Was About But Instead It's About Gaussian Process Regression"".

Your alternate title was better, sure, but it's not like the example was actively bad. Realistically, you need to be reading at least the abstract of papers in your field that seem potentially relevant anyway. The title isn't a place you can reliably encode enough information to make that unnecessary. I don't think there's any substantial harm in a catchy title (or in a boring one for that matter). There are grades of ""good"", but I struggle to imagine a realistic scenario where the title is so bad I'd actually consider it a problem."
MachineLearning,"This is refreshing.

I spent my Ph.D. reading Dehaene, Koch, and others.  I felt it lacked concreteness at the time, but after 10+ years of working in AI/DL, I realize how much I miss the genuine search for understanding intelligence."
MachineLearning,"Ok, thanks. This helps."
MachineLearning,At some point in the future possibly but we are no where near it. I think right now its more like software 1.01.
MachineLearning,When?
MachineLearning,"I don't know if you are really looking for this feeling, because it's kinda sad. Especially if you were excited about Science or don't want to loose this excitement. Because in theory, science is a nice thing you know? :)

I would try to avoid becoming numb if you can help it. How is your relationship with you supervisor? Could you talk to him/her about this?

Maybe you just need a vacation? Do you currently enjoy things in life besides your work?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Formally yes, I signed a 1 year postdoc contract a couple of weeks ago. But my plan is to find an industry job, maybe jump on the data science hype train or do something similar like data engineering or even software engineering.

Considering my background (molecular biotechnology Master, PhD in fundamental neuroscience) I am in a relatively fortunate position because my project involved substantial software development and machine learning, so the transition should be doable (I sincerely hope I don't underestimate the challenges potentially waiting for me) . However, for many of my colleagues with similar backgrounds it is much less obvious what do except for science."
MachineLearning,"It was ok the first time, but now it's just annoying. I guess it's kind of fitting, for a research ""community"" that does so much unoriginal, minor variations on previous iterations on the same thing. 

Has anyone done *The Importance of Being X* yet? I'm, thoroughly looking forward to that..."
MachineLearning,"I disagree with everyone making excuses for clickbait titles.

If your research paper is great, go ahead and propose whatever title you would like. It may garner a little more attention in the short-term, but what matters, especially in the long run, is the actual merits of the paper.

But for most works, it does an enormous disservice by obfuscating actual contributions, especially in “lower-impact” works. Yet people keep doing it as if they inherited the qualities of these few great (referring to merit) works by association. They do not. Even as though they might have great yet smaller contributions.

I am not for making precipitous judgments of impact, but there is a large bias in binning “brand name“ authors, institutions, etc. as “high impact” and otherwise as “low.” The latter must fight against this bias before even competing for the evaluation of its merits.

(On a side note, wasn’t there a thread here where someone described that they allocated 70% of their time for brand names and 30% for the rest [1]? Full disclosure, I downvoted it. It’s incredibly bad scientific practice.)

Furthermore, the volume of works eventually catches up and it becomes impossible to survey literature (think narrowing down from, say, 1000-3000 papers) because these silly titles increase the complexity by more than tenfold. Instead of parsing a title in the order of 8-12 words, you now must read a 150–250-word abstract – just to find out what is actually being done!

[1] https://www.reddit.com/r/MachineLearning/comments/ll28yb/d_thousands_of_papers_to_read_lets_prioritize_them/"
MachineLearning,"Not a really researcher (applied ML), but man do I feel you. Kind of crushes you when you spend hours/days on experiments, things look like they are heading in the right direction, only to realize that you've either been fooling yourself all along by misunderstanding what your model/data is telling you, or you barely get any sort of incremental improvement at all... day after day, week after week.

On the flip side, the high you feel when something works... is pretty spectacular.

My approach to dealing with this is to get blackout drunk each night.

...

Ok, I'm kidding about that part. But I just try to disassociate from work as much as possible at the end of the work day. Anything to separate my own feelings about my personal worth from how well something is going at work. It works *okay*.

I know it's not super helpful, but at least know you're not the only one feeling like this."
MachineLearning,"Well yes but consider: if nobody reads your paper because it had a bland esoteric title, did you really make any impact with your research? 

Yes, having titles that say _nothing_ about what's in the paper that only try to grab your attention isn't ideal or ethical, but neither is having strictly drab to-the-point titles as OP says. The former will result in people reading papers that don't have meaningful impact, and the latter will result in actually interesting papers being ignored by most. 

A balance is always best."
MachineLearning,that's the whole point of the [first GANs of 1990](https://www.reddit.com/r/MachineLearning/comments/djju8a/d_jurgen_schmidhuber_really_had_gans_in_1990/) discussed in this [paper](https://arxiv.org/abs/1906.04493)
MachineLearning,"These kinds of titles are a private joke in the community, and as with every joke, not everyone likes them.

If you still get the main topic of the paper I don't see the harm. To get the main result you will still need to read the abstract anyway.

Also, these kinds of catchy titles are unfortunately easier to memorize and people will tend to remember them more (i.e., cite you more). It's one of the humans' biases."
MachineLearning,"Wait until someone uses click bait titles in their papers:
""I tried this new objective function and you won't believe the result""
""OMG this new method will blow your mind"""
MachineLearning,Out of interest how much does cloud usage cost? Surely it costs more than running your own machine?
MachineLearning,Towards A World Where Love Is All You Need
MachineLearning,"I see, in my case I lack of ideas, I sometimes spend weeks with no output. I believe this is first of all because of my basis that are not solid enough. I think time and experience for now is what I need"
MachineLearning,"Wonderful lecture, thank you for sharing!"
MachineLearning,"Yes, I think this feeling in the end applies to anything in life, in research its mostly this feeling of having literally zero output for weeks even months sometimes, I think I asked my question the wrong way, I should've asked how do researcher actually cope with having no output for a long amount of time"
MachineLearning,"Actually some of my lab mates have also developed this attitude towards research, I believe that is the feeling I am looking for ? I am not so sure"
MachineLearning,"Thank you for you answer I agree with you, I think it requires time, I have only been working for a year now"
MachineLearning,I followed this compromise when naming some recent papers. I think there is a difference in what I call “paper marketing” where you want to have a catchy but also descriptive title and actively misleading your audience and I don’t really see a slippery slope kinda argument applying here
MachineLearning,"""What ML will not replace is the creativity in how we design programs. Creativity in software construction comes from deep algorithmic insights. And ML isn't so great at novel reasoning as much at is in pattern matching.""  


But this exactly the point that Karpathy disagrees with. For areas that require some degree of perception we've already proven that DL + SGD is significantly better than hand crafted algorithms."
MachineLearning,"Fuck that, we can't let scientific paper titles go the same way as clickbait YouTube video titles, they're scientific papers ffs!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yes that's true in theory. Also, it depends on the cost of the cloud setup (GPUs, TPUs, preemtible or not...). I personally think a combination of on demand GPUs and Google Cloud TPUs can be very cost effective (even more when using preemptible instances). TPUs are really very efficient and can cost an order of magnitude less than GPUs when used properly (data loading
...)  

In 2 years and about 20% usage, I had these issues with the local machine I build: Ram died, disk died, 1 GPU (out of 4) died... Also, I lost a lot of time trying to fix them and backing-up OS/code/experiments... And as they say time is money.  And all of this with only 1 machine, I can't imagine the issues one might get with multiple machines and with old GPUs.  

As for the cloud services, I use GPUs on Google cloud for some small experiments and TPUs for heavier ones.  
Like I said, even if it ends up costing a little more all in all (in case your usage is higher than 15-20%), don't forget the time and headaches you save.  

For personal usage it might be a good idea to have 1 GPU to run some local experiments. But In my opinion for a small/medium company it is really the best option to use cloud solution since you should concentrate more on the ML than the infrastructure."
MachineLearning,"Yeah, if you have consistent/predictable inference workloads, I'm not convinced that on-premise is more expensive than cloud."
MachineLearning,"Hey, I am co-author of Aim: [https://github.com/aimhubio/aim](https://github.com/aimhubio/aim)

It's fully open-source and you can use it to compare 1000s of experiments at once. Check out especially if you are running hyperparameter sensitive tasks (especially RL).

Would love your feedback."
MachineLearning,"While I agree with your frustration, ""Language models are few shot learners"" is I believe less than a year old whereas ""X is all you need"" is older

See also ""The unreasonable effectiveness of X"""
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Well yes &amp; no

A lot of time we code boilerplates which can easily be guessed and replicated. MSFT already does it in it production system. You don't have to write a ton of C# code anymore. The model fills it in for you. There is also the case where we could optimize the placement of chips - so essentially we could write better VHDL or similar languages, which optimize. This isn't rocket science. It was in place by means of rule based techniques. ML just does it way better.

What ML will not replace is the creativity in how we design programs. Creativity in software construction comes from deep algorithmic indights. And ML isn't so great at novel reasoning as much at is in pattern matching.

Eventually we will be seeing a situation where we write the algorithm, and there will be a lot of auto implementation given the basic algorithmic structure in place."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I am not an expert on PyTorch -- I've been using TF for my projects.

I think that a PyTorch should be straight-forward. IMO, the hardest part is to replicate the main [fsvd function](https://github.com/samihaija/tf-fsvd/blob/main/tf_fsvd.py#L39) in PyTorch. If you could implement that, the implementation of ""Product Functions"" should be simple (e.g. see [SparseMatrixPF](https://github.com/samihaija/tf-fsvd/blob/main/tf_fsvd.py#L112) or [BlockWisePF](https://github.com/samihaija/tf-fsvd/blob/main/tf_fsvd.py#L167), with &lt;10 code lines excluding comments). I am also happy to collaborate. It is up to you, if you want to push your code directly onto tf-fsvd or create your own pytorch-fsvd and we can cross-link to each other."
MachineLearning,"Hey, check out Aim - [https://github.com/aimhubio/aim](https://github.com/aimhubio/aim)

**Disclaimer**: I am a co-author."
MachineLearning,"I very much agree, however, it is unfortunately undeniable that a ""catchy"" title increases the chance that it catches somebody's attention and stays in their memory. In my opinion, an acceptable compromise is to have something like half of a punny title and then a serious title, e.g., Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns. Or, according to another researcher (Emily M. Bender, I think), an alternative to a pun is to include an ""uncommon bigram"" to make the title more memorable."
MachineLearning,"I see your point and I completely agree, but here's the thing: they trigger our brain more effectively than a long descriptive title, everyone is now aware of this and they exploit the thing."
MachineLearning,"In some organisations you can reserve a DOI, then you already have the link but it will only work when the organisation accepts it, which might take some time."
MachineLearning,"I know everyone is used to the deranged speed of progress in deep learning, but it's too soon to say.  He wrote that column 3 years ago, which is the same as zero time in evaluating the effect of technology.  It took decades from when computers were first invented for them to become a commonplace technology.  It will take decades to know in what ways DL can replace traditional programming."
MachineLearning,"Do you have any plans to implement this in PyTorch? If not, I'd be interested in contributing.

It looks like you're roughly 7.7% faster than the current record, which is quite a nice bump. SVD is widely used in in many algorithms so this has a significant impact for the field.

Congrats on the results."
MachineLearning,"I'm not sure that learning to be emotionless is a skill so much as it is a recipe for crippling mental illness and/or personality disorders further down the line. The real skill is, to feel the emotions, acknowledge them, then be kind to yourself and carry on.

Edit: Brene Brown explains it better than me - yes, she's a celebrity now but before that she was a very serious academic and is a proper smart cookie. [https://www.ted.com/talks/brene\_brown\_the\_power\_of\_vulnerability?language=en](https://www.ted.com/talks/brene_brown_the_power_of_vulnerability?language=en)"
MachineLearning,"Title:Deep Learning and the Global Workspace Theory  

Authors:[Rufin VanRullen](https://arxiv.org/search/cs?searchtype=author&amp;query=VanRullen%2C+R), [Ryota Kanai](https://arxiv.org/search/cs?searchtype=author&amp;query=Kanai%2C+R)  

&gt; Abstract: Recent advances in deep learning have allowed Artificial Intelligence (AI) to reach near human-level performance in many sensory, perceptual, linguistic or cognitive tasks. There is a growing need, however, for novel, brain-inspired cognitive architectures. The Global Workspace theory refers to a large-scale system integrating and distributing information among networks of specialized modules to create higher-level forms of cognition and awareness. We argue that the time is ripe to consider explicit implementations of this theory using deep learning techniques. We propose a roadmap based on unsupervised neural translation between multiple latent spaces (neural networks trained for distinct tasks, on distinct sensory inputs and/or modalities) to create a unique, amodal global latent workspace (GLW). Potential functional advantages of GLW are reviewed, along with neuroscientific implications.  

[PDF Link](https://arxiv.org/pdf/2012.10390) | [Landing Page](https://arxiv.org/abs/2012.10390) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2012.10390/)"
MachineLearning,AI will at some point replace the best dev team ls in the world. Not only that but will replace probably 50-80% of corporate desk jobs.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;I applied to a total of 15 schools, and mentioned 42 prospective professors in my SOPs. I had read *at least* one of their papers, and I was familiar with their recent work, and had ideas how to extend their work. I plugged the names of the 42 professors into https://www.csauthors.net/distance and measured the distance from my advisor X to them. 41 professors had distance &gt;=3, and only one professor had distance 1 to X, the UIUC professor who gave me an offer. One of the very first questions I got asked in the interview was ""How is X doing?"".

That sounds like quite a nice test, beyond the obvious explicit social mention stuff; tracking the probability of being hired vs how close a connection they are, and comparing to what would be expected under randomness.

You'd also need to account for overlapping research topics; how much people work in similar fields (thus would be able to recognise your experience) vs their explicit social connections."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[deleted]
MachineLearning,"Blog post:
https://openai.com/blog/clip/

Paper:
https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"
MachineLearning,"Where is another logic.

Generator do not produce good samples but helps the classifier to make better clusters/generalization.

Cant find link now."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,That sounds pretty bad. What you up to now? Still in science?
MachineLearning,"Unfortunately, the only way I was able to cope with the constant frustration and setbacks during my PhD was to get completely numb. After a couple of years I didn't feel a lot of negative emotion, even after discovering that again, weeks of hard work didn't yield the expected results. Unfortunately, this numbness went both ways and I really didn't feel any positive emotions after paper acceptance/publication. 

The numbness helped me to finish my PhD but now I feel I'm done with science. Sorry for the downer, maybe you can find a better coping mechanism :)"
MachineLearning,"Can´t honestly say that I´ve been in your situation in work with regards to time-series methods. But it sounds like it really sucks. People in general tend to have difficulty understanding how hard it is to use many ts-based methods in empirical settings. I often feel that explaining why something cannot be done using statistics is like trying to tell people at a party that it is not a good idea to see who can down a bottle of vodka the fastest. People tend to look at you and question your ability to do it rather than realizing that it's just not a good idea and there might be consequences. I would probably consider that type of problem a ""specialist"" problem. Ts-stuff just is more complicated than work with approximately cross-sectional data. I´m kind of into time series analysis. However, I do not have a single general favourite toolbox. However, if I would choose a few, it would be Rob Hyndmans forecast package, the dlm package and dynlm in R."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Emotions? I haven’t heard that name in years
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm from a different field of research, but what I found might help is to try and have more than one project at a time. That way you (hopefully) always have something that you can progress in if you get stuck on one project. Because often it helps to let the frustration fade and get some fresh motivation. Of course that's more difficult when you're an undergrad or PhD student and work by yourself, but already little side projects might help. Reading other people's work can be inspiring but also intimidating. So maybe that helps. In general I agree though, I've been very frustrated many many times and it's been tough to get out of these holes so you're not alone"
MachineLearning,"There is no established convention, both are right. IMO counting multiply-accumulate as two operations makes more sense in the ML context, since it makes it easier to compare to chips without multiply-accumulate, or chips that use systolic arrays where all the math is one big blob of hardware anyway."
MachineLearning,"I think it happens with time 

The first time your novel design works well or a good paper gets accepted, you will be on the top of the world. Similarly the first rejection or abandoning a work direction will crush you for few days. As you spend more time with such events, you learn your way around and become wiser.

Over time the time points grow on your career span, and you find your new normal. /pun /truth"
MachineLearning,"The whole point is that if you are generating samples from outliers it ia good. But if the samples are being created more or less from common samples, it could massively overfit very easily"
MachineLearning,"Well, peoples use it."
MachineLearning,It can be disastrous if not created properly
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Many thanks.
MachineLearning,I like idea of using gan as data augumentation.
MachineLearning,"I got 81% by literally just removing some features that seemed unimportant and putting it into a sklearn GBclassifier with no fine tuning, so you definitely can do better than 70s with actual work. However it does seem like you hit a wall pretty quickly as you get to mid 80s."
MachineLearning,"This approach is unlikely to yield the results you are hoping for. It is particularly challenging to generate quality and meaningful content using GAN. You should instead consider using existing datasets with data augmentation.

From there, you could explore with ""Neural Data Augmentation via Example Extrapolation"" to improve on the chances to be useful in real-world applications.

https://arxiv.org/abs/2102.01335"
MachineLearning,More data beats better algorithms. Its really hard to find a field that does not benefit from more data. That being said you could do research on few shot/transfer learning.
MachineLearning,This
MachineLearning,[deleted]
MachineLearning,"The only thing I agree with is disposable income and funding/grants.

&gt; Not to mention the higher quality of life in the US.

Depends on your location. US is huge and diverse, same way EU countries differ. So there is no definite answer to this since there's no city-level stats on this. But looking at country-level, US is far from being first. [Source1](https://en.wikipedia.org/wiki/List_of_countries_by_Human_Development_Index) [Source2](https://www.numbeo.com/quality-of-life/rankings_by_country.jsp)

&gt; Not to mention more talented people (in absolute number) in the US than Europe.

Hard to measure such abstract thing, so I'm putting this into ""can't verify, empty claim"" bucket. But even googling ""tech talent by country"" doesn't show US as top candidate.

&gt; Basically if you’re bottom tier, then Europe is for you.

And I'm not even surprised to hear this from you."
MachineLearning,"yes

But I don't say it'll be better than just training on the set you used to make the GAN, it depends on how you create the new set and what you want to do"
MachineLearning,"Uni is a good chance to move away from home for a while, so maybe take that into account too."
MachineLearning,Seconds depend on hardware. Some algorithms may like some hardware better. In the extreme you could think that a sequential algorithm is faster on CPU and a batch based faster on GPU. The differences is more subtle between GPU models or TPU but it still can favor different models.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"If by tech opportunities you mean just FAANG and by salaries you mean gross salary without adjusting for cost of living or social benefits, then you're probably right."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"And that's great for your class mate, but my question was how this is a better indicator of the research potential of a PhD applicant. My undergraduate classmate in HK was a Syrian refugee who barely escaped Syria during the bombing of Aleppo. He, like me, come from an extremely poor background and has multiple first author publications in top conferences. He wasn't interviewed or accepted into any of the PhD programs.

Again, I'll make this clear. Yes Berkeley and other top schools accept great students. However, they do not necessarily accept the students who show the best potential for research from the applicants pool. This is all I am saying."
MachineLearning,"It depends on the field and dataset. Image recognition with popular datasets like ImageNet are more complete than less popular fields/datasets. Furthermore, different evaluation protocols are often mixed up in the same tables, so in the end it should only be a hint in which papers to look for results."
MachineLearning,I remember you from your talk at EEML. It was one of my first exposures to GNN. Your explanations are very lucid and I was able to understand a lot of them.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hey all,

I'm trying to get yolov4 to work with a (webrtc) livestream. I cannot find any resources on this topic. Hoping any of you guys might have an idea of how to do this. Webrtc livestream is the preferred option but really any type of livestream will be sufficient. Any ideaS? Thanks in advance!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Wonderful to hear! I was hopeful that my derivation would be helpful for getting newcomers up-to-speed, hence such feedback means a lot to me. Thank you! Writeup is definitely in the works :)"
MachineLearning,"""Well this AI seems really competent at controlling cars, I guess we'll implement it"""
MachineLearning,Yes; just check the description of the YouTube video. :)
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"One of my classmates was a refugee who fled a civil war. He travelled through 11 countries just to make it to the United States. He had to go to community college and is absolutely brilliant, but didn’t have the same kinda of opportunities as others because he was dealt a different kind of hand."
MachineLearning,"&gt;You sound very arrogant

I am sorry you feel that way. At no point in this comment did I say anything about myself, so I am not sure why you think I am arrogant."
MachineLearning,You sound very arrogant
MachineLearning,You have to cite a source
MachineLearning,+1 My PhD was on small imbalanced learning and optimization strategies. Can confirm this was a landmine with immense rewards
MachineLearning,"Yeah, I live in London at the moment so it would be a bit more convenient for me. Thanks for your reply!"
MachineLearning,"I think biomedical applications have smaller datasets like 10k-100k, most of them are probably on the lower end but also mostly imbalanced."
MachineLearning,"Yes indeed. It is thus often the case that the question ""does this really matter that much?"" pops up, which I think is what lead to the current practice of sloppy calculation of FLOPS."
MachineLearning,"Agree on both counts, but then we should be explicit by showing the formula on the paper (which is what is done everywhere if using ambiguous notation / formulas).

Also, computing absolute flops is really difficult because it can change depending on hardware implementation and instruction set. We also have quite a few abstraction layers piled on top when talking about machine learning, so this becomes really obscured."
MachineLearning,FLOPS is a very effective metric for assessing how much of the resources of the device you're actually utilizing. There's a reason numerical linear algebra people prefer to talk about FLOPS rather than latency.
MachineLearning,But that one is difficult to map to things like energy consumption. Also FLOPS or MACCs are really important if you want to have custom hardware for your model. There's more to it than raw speed.
MachineLearning,"It does make a difference across papers, though, especially if it leads to comparisons outside of the area. We should be careful when using already existing terminology to refer to our models, as it can lead to a lot of wrong assumptions. On the best case, this leads to a lot of wasted effort."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[deleted]
MachineLearning,"Yes, a system could be constructed to do that. But since it's been constructed specifically to do that, the behaviour shouldn't be labelled inquisitiveness/curiousity."
MachineLearning,"I mean, yeah it's great it shows you are capable of doing good research but most admits in Berkeley are capable of doing good research too."
MachineLearning,"I completely agree that things shouldn't be linear, and I already acknowledged that all admits are mostly very talented and strong applicants, however they are not the best applicants in the pool in terms of research potential.   


If I can borrow your argument, just because someone studied humanities in their undergrad, doesn't mean they're worse in CS research than someone who got accepted. I mean sure, but this is a decision that they made to not focus on CS research. If grad admission is a meritocracy that aims to accept the researchers with the most research potential in CS, then the most obvious indicator is the **existing** research output and research impact of that candidate. I am open for other indicators, but this should really be the most obvious and important one.."
MachineLearning,"Nah, man I don't know. But I do know you can't put a linear metric on things. You shouldn't feel so entitled to a position, no matter how hard you worked. Just because he chose to spend time getting papers published doesn't mean he's automatically a better researcher than someone who chose to spend their undergraduate years differently. The point of graduate school is to learn to do research anyways."
MachineLearning,"The book was fully used for both machine learning 1 and 2 courses in my graduate (msc) degree, so I would argue it is at least expected to know most of it for any research job."
MachineLearning,"Can you give an example of something more impressive that would act as a better indicator of the research potential of a PhD applicant, than 4 top first author papers? Do you also think that **all** the CS PhD candidates accepted to Berkeley had something as impressive as 4 first author papers in top conferences during undergrad and top 10 in Tsinghua Yao Class? Because if not, then the Berkeley admits are not necessarily the top of the applicants pool."
MachineLearning,"Most likely you will need to retrain your NN with a new last classification layer whenever the categories change (unless one category is just renamed). In theory it should be okay to keep the rest of the model as-is with frozen weights and only fine-tune your last layers on the new/re-labelled data.

Whenever you change categories, you need to think about your training data: Splitting up a category into two subcategories will require manual relabelling of this data, as a classification problem will require the network to chose precisely one class. So having the same images for two classes will lead to the model not being able to separate them at all.

If this is not feasible, you could turn your problem into a Multi-Label classification problem and replicate your training data for all valid labels (e.g. save documents that belong to drywall also as drywall-plumbing) etc. Really depends on how your classes evolve over time."
MachineLearning,"I mean, having papers is very impressive, but that doesn't automatically make him better. Other people might have done other impressive things."
MachineLearning," Hi,

I have a data set containing 20 years of Gini values ​​for a country. The latest data are for 2018. I want to predict the Gini values ​​for this country by 2025. How can I do this using ML techniques? Also, which econometric forecast model would you recommend?

Thanks in advance."
MachineLearning,"Wow, that's cool! Never thought STR could be solved as a classification problem!"
MachineLearning,"Thank you. After reading your answer I checked the text again and noticed the authors used the words ""proportional to"" which can imply a positive or negative relationship. I clearly missed that when originally reading and should have done the derivation myself."
MachineLearning,"But the guy in the link the user before you posted says using a gpu 15% of the time matches the cost of using a cloud in a year. Where do you find cloud services which are cheap enough to warrant using them?  


And if you don't mind sharing: what are the 'multiple issues' you've had with the setup?"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,/r/learnmachinelearning
MachineLearning,"&gt; I think it's not fair to claim that most Berkeley CS admits are as good as him, yet they were accepted and he wasn't.

He didn’t have connections. 

Many top PhD students do a masters (or bachelors) at the same university they eventually end up for PhD. This also adds 2-3 first-author papers under their belt."
MachineLearning,"Great work! A new version with several very recent paper such as DALL-E, Token2Token and TransGAN are also covered. 

[https://arxiv.org/abs/2101.01169](https://arxiv.org/abs/2101.01169)"
MachineLearning,"Unless the absolute FLOPS do matter, using MACC as FLOP wouldn't make a relative difference. So I guess it's fine for comparison. The exact FLOP count in the code is rarely accounted anyway. People just use ""rough"" estimates of the number of FLOP when computing FLOPS."
MachineLearning,"You’d stand out from the “entry-crowd”, but not really against more senior folk who have learned to specialize and become experts in their sub domain. In my experience at FAANGs however, they rarely had an interest in my theoretical knowledge, more so in my applied ML experience. Could be a positive for your own ability to implement systems however"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Don't forget to account for FMA instructions!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;Comon man. Anyone could tell a mile away that you're not really interested and are just fishing.

I'll kindly disagree. Many professors (who replied) appreciated the initiative and commented that it's an interesting direction that they didn't think about. There is a difference between skimming a paper, and reading a paper carefully, implementing their methodology, testing their ideas on other problems, and writing a well written email that shows new results of using their method on new problems. You seem to think this still counts as ""fishing"" which I can't really help you with. Many of the professors who \*did\* reply were the ones who asked to schedule a Zoom call; clearly they saw ""something new"" and not just ""fishing"". Have a nice day!"
MachineLearning,"I know sometimes it can be easy to see meaning where you see some evidence, and we're inclined to fit our experience to a narrative. I was involved with UIUC graduate admissions in computer science for a couple of years, and I can tell you ""connections"" aren't entirely accurate. How it worked (a few years ago) was as follows: 1) top people in the pools. People with high test scores, publishing records, and so on. They're often international students and people who have masters degrees. 2) people with pre-existing relationships with the faculty. This is sort of your connections theory, but it's not so nefarious. It's mostly just 'i had this student' or ""their advisor wrote them a letter and I know the advisor"". This is often traditional students (top school for undergrad and or famous advisor) 3) traditionally underrepresented groups. In stem, you know what that means, but it _also_ includes people from unknown colleges or from unusual backgrounds. I've seen a faculty member try to throw out any applicant that did their CS degree at a small liberal arts college, and then other faculty really went to bat for people who, y'know, didn't graduate from MIT or CMU 4) everyone else.

I know it can be frustrating that the process is so opaque, but it has to be. Lots of decisions are ""does a faculty want to extend them an offer"" and that quickly gets into the tension between the department (which wants more students) and the faculty (who have to pay for those students) and then there's intergroup politics and the rest. Deciding which phd students get accepted is _hard_, and I promise that schools are trying to do their best. There's _lots_ of institutional problems around equity, but there's not a nepotism plot. There's just a continual struggle against bias in favor of the familiar.

Congrats, and good luck to anyone out there who's looking at grad school."
MachineLearning,[removed]
MachineLearning,"Taco Cohen, Max Weilling, Risi Condor, Fabian Fuches, and Tess Smidt are people to check out. I would especially recommend [Tess’s tutorial on equivariant NNs](https://www.youtube.com/watch?v=8s0Ka6Y_kIM), [Taco’s Group Equivariant CNNs paper](https://arxiv.org/abs/1602.07576), and [this online textbook on DL](https://whitead.github.io/dmol-book/intro.html) for getting started."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"    And yet you applied to dozens of places? Looks like you used the shotgun approach you see over in r/cscareerquestions of ""help I spammed my resume to 9001 places and got 0 offers"".

It took me more than 8 months to prepare my applications. I read the Professors' papers (at least one paper), wrote them emails about it with ideas on extending them, joined Zoom calls with some of them and discussed research ideas and directions, etc. Some professors strongly encouraged me to apply and said ""they would love to have me"", and they didn't interview me after the official application. I think it's a bit strange on your part to assume I just shotgun my applications."
MachineLearning,"And yet you applied to dozens of places? Looks like you used the shotgun approach you see over in /r/cscareerquestions of ""help I spammed my resume to 9001 places and got 0 offers"".

I applied to one place. It would be unthinkable for me to apply to more than one place at the same time. You didn't dig in deep enough.

I for example worked on a review paper with the said professor before I was officially accepted, visited the lab, attended meetings etc."
MachineLearning,"&gt;You really can’t underestimate how talented the rest of the applicant pool is.

I am not; I already acknowledged that all students accepted to these programs \*are\* strong applicants. What I disagree with however is that they are the best  (academically and research wise) from the applicants pool. A Tsinghua graduate reached out to me after last post who had 4 NeurIPS/CVPR/ICLR first author papers and ranked top 10 in Tsinghua Yao Class. He was rejected from Berkeley/MIT/Stanford without interviews. I think it's not fair to claim that most Berkeley CS admits are as good as him, yet they were accepted and he wasn't."
MachineLearning,"If you genuinely understand the math and don’t just know how to train models on datasets, then you’re better than many entry-level workers in industry.

Edit: if you find some datasets and build a few repositories where you show off your skills with models and analysis, that would probably be sufficient to get interviews."
MachineLearning,"&gt;I find it weird that you applied to a school without having an idea of what you want to do or what the professors want to do.

This is not true. If you read my post(s), you would know that I already had a research area that I published in, and I knew the research and work done by those professors prior to applying. I also had a research plan, ideas on papers to improve on, etc. I contacted the professors 6 months before applying, so I didn't just ""propose to random people in the street""."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I need to correct the distortion. I did try image inpainting and it's working fine to fill the distorted area but in image inpainting, I'm needed to selected the area to be filled, this makes the process manual so I'm looking for some automated ways."
MachineLearning,"&gt;How do you know Europe is less competitive than US?

After my last post, many Professors from ETH, EPFL, ... who served on the admission committees kindly reached out to me and discussed admissions in Europe. While the top applicants in both Europe and US are similar, the *average* applicant in European schools had slightly weaker profiles according to them. The process is also a lot more meritocratic as far as I understood; some schools even anonymise LOR writer name.."
MachineLearning,Tech opportunities and salaries in US are leagues ahead compared to Europe
MachineLearning,I also did a ton of volunteer work. I think a lot of programs really want students who intend to make the world a better place.
MachineLearning,"PhD Student at Berkeley here.

You need to remember you’re getting a PhD from a great school in a hot field and you’ll be able to do great things. I’m sure you’re also very aware you can get very far in ML independent of the university you’re in.

I’ll tell you why I think I was able to get into Berkeley. I worked after undegrad in the field alongside my research, and a lot of my classmates did too. Alongside publications, I had real world experience and was coming from a position where going to grad school was a conscious choice for me, and this was something heavily reflected in my statement. You really can’t underestimate how talented the rest of the applicant pool is."
MachineLearning,looking into this now.
MachineLearning,I waited about 10 days
MachineLearning,How practical is background matting. Could this be used in zoom calls for example?
MachineLearning,Between. As an example  I would like to get Google scholar's articles translated as a brief to another language. Social sciences can be accessible to other languages speakers
MachineLearning,"Currently a PhD with 10 years in AI industry research in respected labs so this ends well. In my 20s I applied to several schools for graduate school in one field. Completely skunked; not even one interview. I took the year, worked different jobs, realized what I really wanted to do. Applied in that different field, and didn't get into my ""dream"" school but got into a good program with an advisor that (especially in retrospect) was a really good mentor. Four years later that ""dream"" advisor approaches me at a conference, likes my idea, and offers to collaborate. Some of my best work with him. Never mentioned to him that he had previously rejected me without an interview, but that's how the field goes. Because really he hadn't; so many applications get filtered out without even being seen by the PI. Eventually I am meeting and working with so many people I had only read their work and never thought I would meet. 

TL/DR: Do your thing. Eventually it happens."
MachineLearning,"I didn't mean to imply that you were, just speculating about another comment upthread which mentioned it. Also not willing to die on this hill considering a) grad school applications suck and b) you've put in way more than I did when applying :)"
MachineLearning,You need a really good reason not to just use cloud hardware--I'd start with articulating that.
MachineLearning,"Yes, as long as your GCN utilizes edge weights in some manner.  GraphSAGE implementations usually do this with weighted sampling."
MachineLearning,Did you tried transducer based?
MachineLearning,I think it's a residual thing from MACC being a fused operation in some architectures. It's somewhat odd because researchers will sometimes refer to it as a single FLOP while marketing will often refer to it as two.
MachineLearning,are you looking for image inpainting? or correcting the distortion
MachineLearning,"Well, in that case, the encodings generated from the original data provide a similar utility, right?"
MachineLearning,Thank you. Very nice talk. I learned a lot about GNN.
MachineLearning,"I think it would be wise to advertise high academia as a trade. It's root is set up like any other classical apprenticeship (think carpentry, blacksmith, etc.).Typically its something like:

apprenticeship -&gt; journeyman -&gt; master progression.

It is rather intellectually dishonest to think otherwise. Now of course, some academics love new ideas, but we all know that it is a gated community and the top brass, if they see a good idea, will just run with it themselves."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Title:Real-Time High-Resolution Background Matting  

Authors:[Shanchuan Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin%2C+S), [Andrey Ryabtsev](https://arxiv.org/search/cs?searchtype=author&amp;query=Ryabtsev%2C+A), [Soumyadip Sengupta](https://arxiv.org/search/cs?searchtype=author&amp;query=Sengupta%2C+S), [Brian Curless](https://arxiv.org/search/cs?searchtype=author&amp;query=Curless%2C+B), [Steve Seitz](https://arxiv.org/search/cs?searchtype=author&amp;query=Seitz%2C+S), [Ira Kemelmacher- Shlizerman](https://arxiv.org/search/cs?searchtype=author&amp;query=Kemelmacher- Shlizerman%2C+I)  

&gt; Abstract: We introduce a real-time, high-resolution background replacement technique which operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU. Our technique is based on background matting, where an additional frame of the background is captured and used in recovering the alpha matte and the foreground layer. The main challenge is to compute a high-quality alpha matte, preserving strand-level hair details, while processing high-resolution images in real-time. To achieve this goal, we employ two neural networks; a base network computes a low-resolution result which is refined by a second network operating at high-resolution on selective patches. We introduce two largescale video and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our approach yields higher quality results compared to the previous state-of-the-art in background matting, while simultaneously yielding a dramatic boost in both speed and resolution.  

[PDF Link](https://arxiv.org/pdf/2012.07810v1) | [Landing Page](https://arxiv.org/abs/2012.07810v1) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2012.07810v1/)"
MachineLearning,Don’t see why an AI can’t learn off its own questioning. We have online learning ingestion mechanisms already. Why not have the AI request a particular prompt after encountering a knowledge gap?
MachineLearning,Thank you!
MachineLearning,"Considering your example, we do not train our model to map STFT parameters to silence/non-silence regions. Instead, we optimize a sparsity measure, which decides what it is the optimal allocation of windows across the audio in order to minimize the sparsity in the frequency domain. (in the silence/chirp example, it would place long windows where there is silence, and would then place shorter windows where there is a chirp)"
MachineLearning,"Feel like the other answers are a little too readily dismissive. There ARE valid applications for AIs being able to question - self-improvement after encountering failure, human-interaction for “training humans”, more realistic simulation of interactions that need the AI to feel human (think nursing care). Or even it’s a necessary pattern for scenarios wherein data pipelines are not directly manageable and the AI has to manage its own ingestion. 

However, I haven’t encountered research in this space. Could be interesting to look at for the NLP folks"
MachineLearning,People have already used the prediction error of a model-based reinforcement learning algorithm as its reward signal. This lead to a curious agent. So it's not too far off to ask this question.
MachineLearning,"I don't know if you have a limitation for cloud based solutions due to confidentiality matter?  
You will have better ROI using cloud services compared to buying and setting-up in house GPU machines. In 2-3 years the GPUs/machines you buy now will be worthless and that's assuming you don't have any issues during that period (quite impossible).  
Think about it this way: with a cloud setup you might end up paying more per GPU, but will you really use your local setup 100% of the time? If not you will never be able to make them cost effective. They will be setting idling and depreciating.  
I personally went through this debate a couple years ago and setup an expensive machine with multiple GPU. The multiple issues I had with the setup and failing hardware through the time convinced me to push for cloud solution and I've never been more happy :)  
And even if in an extreme case where it might cost a little more, it is money well spent and it ends up saving time (no failing physical hardware).  
Ah also think about scaling, with a cloud solution it is a couple clicks and you are all set.  Hardware? Good luck buying GPUs right now lol!"
MachineLearning,"I find it weird that you applied to a school without having an idea of what you want to do or what the professors want to do.

When I applied to my PhD program, I had an outline of a research plan. I had a few options of what specific sub-fields of a sub-field I was interested in, I had rough titles/topics of papers, I had a rough idea of what coursework I'd need to do/what books to read, which universities/companies I'd like to collaborate with/do an internship with/just fly over and introduce myself and so on.

Most schools didn't have a professor interested in the same things as I did or the ability to supervise. I contacted the professor and had a few long talks about what they are interested in and what I am interested in and share ideas MONTHS before I applied.

PhD is like marriage. You don't propose random people in the street, you need to do some dating first. It looks like you did it exactly the wrong way.

I know that SOME schools still do the old-school thing where you apply for a PhD program and then during the first 1-2 years you do the whole mating dance thing, but nowadays I think it's much more popular to do it BEFORE applying because of the large amount of PhD students that drop out because they never figure it out.

When I actually applied for the PhD position, I had 3 professors of that department that recommended me since I've had numerous discussion with them and were eager to start work so the whole thing was a rubber stamp. A lot of it has to do with funding too, those professors that take you on as a student will need a suitable project to get the $$$ from."
MachineLearning,So cool!
MachineLearning,"Right now you give me anything in web development, I can do. I always projected myself to be ""Go To Expert"" when it comes to Web Development

If I can't do the same with ML , I don't feel comfortable. 

Thanks for answering my question."
MachineLearning,My problem with Kinetics and the rest of these datasets is that you don't need all this fancy machinery to detect someone playing tennis or making pizza or opening a wine bottle. You pretty much just need to be able to detect a tennis court or a pizza or a wine bottle.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm not understanding how the dynamically changing parameters actually work when you go to make a prediction. I understand that it can be optimized during train time.

But, that doesn't necessarily mean that the parameters it finds are going to work for you at test time.

For example, you could optimize for a bunch of chirp sounds, then silence during training. Then when you go to make a prediction, the silence is first, then chirp sounds happen. Now your window is all messed up."
MachineLearning,oh wow.
MachineLearning,Alright thank you for your opinion!
MachineLearning,"Wouldn't hurt to try. Based off my limited understanding, I believe so."
MachineLearning,"So does this mean that if i train the model long enough, the same model can perform significantly better although the updating of weights will take a very long time because of the super small learning rate? So there is nothing wrong with the model architecture?"
MachineLearning,Are the slides accessible anywhere?
MachineLearning,It sounds like the learning rate is too high then
MachineLearning,"Nope. The problem is that for the data, the latitude and longitude values across all of the points are very similar. I am thinking that this might be a vanishing gradient problem, but I am already using a LSTM which is supposed to solve this problem due to the additive property of the errors rather than multiplicative like a normal RNN. Then this might mean that the individual derivative is so small such that even if I have the additive property, the end result will still be too small. Hence, the next direction to look into is probably to see if there are ways to ‘enlarge’ the derivatives but online doesnt shed any light on this, so I am wondering if you have any opinion based on the points that I mentioned here."
MachineLearning,hi yuval! thanks for sharing. any update on the code release?
MachineLearning,are you  using random inputs to test the data? idk too much about this stuff yet but that came to mind
MachineLearning,Digital humanities can have some very interesting ML problems. Out of curiosity do you work in a connected area? Would love to hear what you are working on.
MachineLearning,Well if these QA system would work you wouldn't need to RTFM any more at least not for every question :)
MachineLearning,"I am not hitting up Sergey Levine to ask about autonomous vehicles. I'm mostly talking to much less high-status folks about much less flashy topics; semi-supervised learning, mostly."
MachineLearning,Does this produce the animation?
MachineLearning,"Interesting. Nobody I know personally seems to have gone through this, so perhaps it is a regional or domestic/international thing. /u/eatyo made a good point about timing and labs having vacancies upthread. It may be that well-known profs working on the most mainstream ML domains at top schools are drowning in applicants and prefer vetting through admissions first to save themselves some work."
MachineLearning,And I'm on Windows. I'll try and see if I can reinstall pytorch and give it another go later in the week and let you know if I do.
MachineLearning,It immediately returned oom message in my case. Probably related to the environment settings? I'm testing it on ubuntu fyi
MachineLearning,"&gt; it's strange that it takes more than an hour on 3080 since it only takes a fraction of a second on 3090.

I agree. When you run out of memory on your 2080ti does it happen after a few second or does it take a significant amount of time? 

I suspect my pytorch/CUDA to be responsible for the delay when omitting the CPU flag."
MachineLearning,"Thanks for the suggestion. I added the device flag in readme. Btw, it's strange that it takes more than an hour on 3080 since it only takes a fraction of a second on 3090."
MachineLearning,"I'm not applying yet, and I don't plan to for another two years or so, just starting to send out some initial feelers, as I have a decent idea of the specific sub-fields that I would like to pursue during a PhD.

My process, so far, has been to - by various methods - accumulate leads on professors that I think would make good advisors, and then shoot them emails introducing myself, a brief overview of my work thus far, my research interests (such that they align with the person I am contacting), and then asking if it would be possible to chat with them. It is uncommon for them to respond at all, and when they do, if I brought up doctoral study, frequently they'll just say that I have to apply and then discuss with them once admitted.

I don't think this is unreasonable. Otherwise, they'd probably do nothing all day besides respond to emails from interested students, only a minority of which they could possibly admit and fund. But that doesn't mean it's not disappointing to see somebody that I think I could do some really cool work with, and then to essentially be told to talk to the hand and/or graduate admissions."
MachineLearning,Code or it didn’t happen. ;)
MachineLearning,"&gt;  My advice to people applying for ML positions is to consider applying to interdisciplinary CS areas instead

Or even areas outside of CS. They may not be as attractive for those hell-bent on ML theory or getting a FAANG job, but the work is often interesting and has a more tangible impact. Plus it skirts around the problem of competing with thousands of applicants for those scarce ""pure"" ML positions at top universities."
MachineLearning,"awesome, its okay, i need a reference for distillation since I have been having tough time to distill networks. Pruning, and quant is much easier."
MachineLearning,"I uploaded the code, but keep in mind that it is not well tested

[https://github.com/bryandlee/naver-webtoon-faces/tree/distill](https://github.com/bryandlee/naver-webtoon-faces/tree/distill)"
MachineLearning,"This was fantastic. If you're looking to make more content related to this, the second half of the talk where you're using all these DFT identities went a bit over my head, I'd love to see some more exposition there."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for the talk Petar. I came in knowing next to nothing about GNNs, but by the end of the talk I felt like I had a solid grasp of the foundations. I especially liked the the way you built up the the different representations and the abstractions of the different methods/models/etc.

The talk was engaging and it was an awesome result to show that transformers are just message passing GNNs. Mind blown!

Really looking forward to a write up on this at some point.

Cheers!"
MachineLearning,"Thank you for the feedback, the process only takes a couple of second with this setting and more than one hour without so I would suggest making this a default option or maybe adding it to the main readme!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I just tested the code on 2080ti, and got an OOM too. However, you can always use \[--device cpu\] flag to use cpu instead of gpu! It only takes few seconds to process 1280x853 image on i7-9700K."
MachineLearning,I also want to know
MachineLearning,Another nail in the CNN coffin
MachineLearning,"To he best of my knowledge, Bayesian networks are very attractive because of what they promise:   a theoretically coherent unification of symbolic and probabilistic AI.

The way you combine the two is by \_manually\_ specifying your prior knowledge with a probabilistic dependency graph.   Then, given observations, you run an inference algorithm, and get the exact posterior (or an approximate) distribution over the variables you care about.  Researchers imagined that one would hard code e.g., medical knowledge in this way, and be able to query the system to provide you with probabilistic answers.

However, manually specifying a dependency graph is not the most scalable approach, so it became important to figure out how to train these graphs from data.  This approach would've been quite successful, except that training requires that you run the above mentioned inference algorithms at each training step.  These inference algorithms are expensive, however, which in turn makes training expensive.

In the end, deep learning seems to offer nearly all the advantages proponents of Bayesian networks were advocating for, while being far more compute efficient and therefore practical.   I can easily imagine some future deep learning-based approach that may borrow an idea or two from Bayesian networks, and I also expect Bayesian networks to shine anytime we have extremely strong prior knowledge over our stochastic variables -- so strong that we can just write down the graph.  But otherwise, I see Bayesian networks as yet another family of methods that were made irrelevant by deep learning."
MachineLearning,"On the user side there's custom hardware to acquire real-world image data, so a lot of variability. In fact, dealing with that variability is one of the most challenging aspects of this pipeline."
MachineLearning,[deleted]
MachineLearning,"Ah, sorry, that sucks... How long did you wait before sending an email? Thinking about doing the same thing. Looks like no one on this thread has been accepted lol."
MachineLearning,Thank you for putting a description and explaining how it stands out. I wish more people on this forum did that.
MachineLearning,What does the series look like? Are there any obvious patterns/cycles?
MachineLearning,I heard back yesterday and got rejected ! I sent him an email tho
MachineLearning,how do I use this
MachineLearning,The only answer I ever get when I ask a question about a manual is to RTFM
MachineLearning,like a boss. thanks
MachineLearning,"The ""woman-child-group"" model gets 85% accuracy on Titanic, see [https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688) .  It is also possible to get 91% accuracy using genetic programming, see [https://www.kaggle.com/scirpus/my-first-gp-in-r](https://www.kaggle.com/scirpus/my-first-gp-in-r) , but that is a very different type of method than what you tried so far."
MachineLearning,"Great reply, thanks so much for the information."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Sure, but make sure to also report performance on the standard split - so that others may compare your method to existing literature."
MachineLearning,"Wow, great talk. Probably the best introduction to GNN. I’m in."
MachineLearning,You can use GraphSAGE to generate embeddings and apply your clustering algorithm of choice.
MachineLearning,Thanks dude
MachineLearning,"For the tasks you mention, the typical would be to basically ignore the sequence and just apply bag-of-words, n-gram or similar models."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Congrats on getting a position!

Adding some thoughts on this from a slightly outside perspective. For reference I'm at a later career stage (1st academic post) and not in an ML department (physicist with an interest in ML)... but chances are that everything is true for your situation as well

Whether you like it or not these connections are a big part of life as a researcher and can actually be very beneficial for all parties. Firstly it's very easy to give a glowing student recommendation to another prof you don't know, they may never contact you again so who cares? But recommendations to people you do know are usually a lot more sincere and honest, if the student does badly it can damage reputations and may harm future collaborations. Also bare in mind that the reference (if there was one) is more likely to be from an informal chat than a copy-pasted letter.

Secondly, it becomes super important as you move onto later career stages. A big part of research (and I mean a big part) is getting your work out there for people to see and being recognised for it. You'll find that if you apply for research positions after your PhD it'll be a lot easier with people you know beforehand. This isn't about ""doing favours"", it's about knowing what someone contributed to their papers etc so you know what kind of researcher they will be like working for you. You'll also see that it becomes important for identifying like-minded researchers, some fields can be quite small and you usually want to work with someone who complements your own knowledge and skills (this is actually true for larger fields). This also works both ways, when I think about the universities I'm applying for positions at atm I'm specifically choosing ones where I know some of the people in the department already, not because it means the department is any better/worse, but because I know their research interests align with mine"
MachineLearning,What’s the best way to do so.
MachineLearning,"Totally, I don't want to over-generalize—I also talked with a PI at MIT who was super nice and helpful, so I'm sure there's some of everything. Either way, thank you so much for your feedback, it's really reassuring to hear that you chose based on advisor and ended up happy with your decision!"
MachineLearning,lol how would you even objectively/quantitatively classify whether a question qualified as curious or inquisitive enough so that you could provide a training signal to learn from or benchmark the level of success you achieved with the model?
MachineLearning,How much variability in the input? You could cache common inputs and periodically refresh the cache when the model state changes.
MachineLearning,"I saw this: [https://ieeexplore.ieee.org/document/6215223](https://ieeexplore.ieee.org/document/6215223)

But of course any method can be made ineffective if you know the method."
MachineLearning,"I'm dealing with the same issue in a hobby project on AWS.

Similarly, container images still need 3 minutes to start or so, so the next thing I'm planning to try is full instance images (via AMI).

I haven't gotten around to it yet, but simply having a full machine image should be a lot faster than first loading an image and then running docker in it.

Let me know if it works for you, I'll update this comment if I get around to trying it."
MachineLearning,"On the one hand it is a bit disappointing for you that you weren’t able to get an offer own your own. But on the other hand, this is kind of how the real world works, and not just in academia - there are so many opportunities that are only possible because you know the right people and have the right connections. You can legitimately argue that this is an unfair obstacle to people without those connections and by prioritising the human factor labs are missing out on a lot of good people, and you’d be right. But from the other side, an interview/application process gives a very, very incomplete picture of a candidate, and if you have a recommendation or other evidence from someone you trust that knows the candidate better, the risk of the person not working out is much lower. For better or for worse, the trick is to just play the game and build up your professional network. And who knows, one day you might be in the position of having to choose between two candidates, one of whom has say an extra paper on their resume, and the other who has a glowing recommendation from a former colleague that you know has high standards, and then it’ll be your choice who would be more likely to get the project done"
MachineLearning,"I've been making some initial cracks at this, and almost everybody I email basically says they cannot discuss work with applicants before they're accepted. Which kinda puts the kibosh on this idea."
MachineLearning,You're welcome! I'm glad the spectral discussion was interesting -- it took me quite a while to decide on the best way to expose that topic.
MachineLearning,Very kind words! Thanks so much :)
MachineLearning,"Thanks! I hope you will enjoy it :)   


Unfortunately I'm not super knowledgeable outside of the scope of GNNs -- but as far as GNNs are concerned, I believe that Part 7 of my talk should be more than enough to describe the graph representation learning perspectives that have been developed over the years.  


We don't have a paper out at this time. But we do plan to put something out there -- keep an eye out over the next few months :)"
MachineLearning,"I’m going to be blunt. 

If you don’t have math skills or aptitude or else don’t like it, I don’t think data anything or ML anything is a good choice. 

To work really with data, you really need to understand or at least “like” maths much better than you are describing. Algebra? Dude, c’mon.

You would only be setting yourself up for a lot of pain. And definitely limiting your career prospects. Assuming you could get your foot in the door to begin with. 

Sure, there are many many examples of people in jobs that don’t really understand what they’re doing. Pushing buttons and getting answers. Or just accepting any answer. 

Is that what you want? It doesn’t sound very satisfying."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Bayesian theory is a very powerful and general tool. In overly simple terms, a Bayesian approach attempts to model the entirety of a distribution while a traditional approach only models the expected value (mean) of the distribution. This has the potential to make Bayesian models far more powerful and general.

The problem is that in order to make Bayesian models practically calculable you need to narrow down the utility with a bunch of very strict assumptions around Gaussian distributions. Those assumptions can do ok with simple data and simple models but tend to break down with complex data and complex models. So even though something like a generalized Bayesian NN should be better than a standard NN, in practice it isn't. One day, someone will crack the restrictions that limit practical Bayesian modeling and that will revolutionize the field, but until then it'll remain a small niche that occasionally achieves some interesting use cases."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Google handled this exceptionally well. You have no clue what you're saying.
MachineLearning,btw? would you explain more?
MachineLearning,Moridian Forests is interesting [https://www.gatsby.ucl.ac.uk/\~balaji/mfregression\_aistats16.pdf](https://www.gatsby.ucl.ac.uk/~balaji/mfregression_aistats16.pdf)
MachineLearning,"Find a supervisor first, university second. If you're lucky your supervisor has enough power to basically appoint you directly, solving the whole application issue."
MachineLearning,"That sounds like the worst! It's kind of a mixed bag - at places like MIT where it can be a little publish or perish people sometimes can be like that... But I also used to work with a PI at MIT a long time ago who was awesome and brought their puppy in to the lab (dogs are totally not allowed, this person was just goofy and tenured) and bought us pizza every Friday. 

So I'd definitely recommend going to the place that makes you feel wanted! You're going to spend every waking moment of your life basically doing work or thinking about work with this person for potentially the next 5 - 10 years, you want it to be a good fit!!!"
MachineLearning,"Yeah you just have to keep going! I feel for you though, I've worked in a lot of lab-nightmare situations and it can be really depressing when you spend years of your life but only squeak out one terrible paper (or no papers at all!) because the PI didn't really have it together. 

If I can offer you any advice - trying to have attainable side research projects that you know will lead to a publication can help balance this. Like if your advisor keeps dragging their feet on a bigger pub, finding a smaller project you know you can accomplish and push out to a few conferences can at least make the time more palatable. Like being the statistician for a local public health group or hospital group or animal science group or public policy group or education group (Really whatever you happen to enjoy) at your school can squeeze out some additional throughput."
MachineLearning,Noice Man Congratulation! :D
MachineLearning,"NLP involves just as much math as any other deep learning. It's currently at the forefront of a lot of research efforts, and keeping up with it all is even more complicated. Every time a new model structure becomes the new SOTA, youll need to rush to understand it, and if you don't know the math, you'll be waiting around for someone else to do an open source implementation before you can use it.


That being said, I know a lot of data scientists in corporate jobs that don't really understand any of the math or algorithms they apply. They type the right lines, follow simple instructions, and get a result. But they get stuck when the results aren't what they expected, and can only work with simple applications that are a cookie cutter match for work that's been done before. They can't innovate and solve new problems. And they often oversell their results because they don't understand the weaknesses of their trained models. Simply put, their careers have a ceiling, and the field will always be a black box to anyone who doesn't learn the math.


Maybe you'd be better suited as a data engineer? It is a vital part of ML/AI teams, pays very well, and is in high demand."
MachineLearning,"Thank you for the follow-up, this did put a smile on my face. Best of luck with your PhD! It will be hard but judging by your story, you have enough grit to pull it through."
MachineLearning,How do you know Europe is less competitive than US?
MachineLearning,"Not old! Also considering, but pending some verification regarding network latency. 

The data the workflow runs over is persisted on Azure (and it's a few dozen MBs), so questions remain on whether the Azure -&gt; on-prem communication might introduce unwanted delays (versus the current setup, in which data and the VM are in the same cloud region/data center). 

Not off the table, just pending some further testing."
MachineLearning,"Same as YodaML, I have a similar device (boox ink tablet 10"") and it has been a joy to use (I imagine remarkable would be similar experience). No more printing for papers that I realize are not relevant after two pages and no more huge stack of papers!"
MachineLearning,Here we go again
MachineLearning,"There are kinks about everything, I guess. Surely someone out there gets off on the infamous crypto volatility."
MachineLearning,"Thanks Petar, you should cross post this in /r/learnmachinelearning as well if you haven't already."
MachineLearning,"Ah yes, the cryptoporn hierarchy.

Some theorize that crypto = porn, which would cause the hierarchy to collapse."
MachineLearning,"&gt; Wouldn't it be the only ethical course of action left if the company policy was unethical according to her framework and that those documents proved it but she found no other recourse because internally she wasn't able to change anything?

No, this is a very solved problem: if she believed something illegal took place with Gebru's firing, then she should work with Gebru to file suit. Once the lawsuit was brought, Google would be notified to keep all relevant emails, and they would, because spoliation could land them in much hotter water.

Starting to download the files ""just in case"" is an attempt to have ones cake and eat it too (ie. to try to gather data without antagonizing Google).. and for very good reasons, including protecting user data and employee privacy, a company should respond to this with termination.

Just for one example of why: due process will govern what documents are revealed to the public. Normally, a judge will decide that. Bypassing that measure removes the employees' due process on whether their emails get shared publicly."
MachineLearning,"A project I would like to work on (and I would recommend) is ""universal word embeddings"".

The idea to give a representation of words that would be coherent between languages. Like the word ""dog"" in english would be close to the word ""dog"" in german, in french, in chinese etc. Plus the model that comes with it.

Hopefully this would make processing languages much more practical because you wouldn't need 1 model per language.

One model to understand everything.

I though about this while working on Universal Dependencies some years ago. I think it's doable. I partly worked on it for very close languages, some work has been done on that but I don't think it's used in production for now"
MachineLearning,I'm curious to know how this is going!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I might be old, but have you considered on premises hardware?"
MachineLearning,"This is super helpful, thank you.  The googlable term is exactly what i needed!"
MachineLearning,"For a normal distribution, log p(θ) = - 0.5 * θ^T A^-1 θ + constants = -0.5 * λ * θ^T θ, where A is your covariance matrix, so there is already a negative sign in front of λ. This makes intuitive sense, as if we look at a normal distribution, values far from 0 have very low likelihood."
MachineLearning,You will not get any chance to even become a half-assed ML expert because you will be replaced by AWS/GCP/Azure AutoML products in &lt;3 years.
MachineLearning,"[Tim Dettmers'](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/) rules-of-thumb are always my default to-go advice for Deep Learning. Start with the ""When is it better to use the cloud vs a dedicated GPU desktop/server?"" and TLDR sessions, and read everything else when you got the time."
MachineLearning,Text summarizer btw languages
MachineLearning,Thank you so much!
MachineLearning,Not yet! But we do plan to put something out there. Keep an eye out over the next few months :)
MachineLearning,Congrats! I followed you since your last post and  this just made my day. I’m happy for you!
MachineLearning,Thanks for correcting me.
MachineLearning,"I appreciate that you are trying to help me!

I've known tensorflow.js for a while, but could never figure out how to ""install"" it on github.pages. Luckily the tutorials in your link gave me the term I was looking for: ""CORS"", which prevents a browser (client) from installing external packages (e.g. tensorflow.js) when browsing my website. I figured that I ""just"" have to provide ALL the dependencies on my github.pages repository. Oddly enough is, that I haven't found a single example of anyone ever doing this! Either because it's not possible, or because it makes no sense. I thought this would be a very common use case for people who want to share their models for free (without asking the user to clone a repository or running code cells on google colab)"
MachineLearning,"The introduction of 'spectral GNN' with circulant matrices was very interesting. Also I didn't know PGM were used for graphs as well. Thanks for sharing, this is a very good resource"
MachineLearning,"You do need some math, but not a lot.  You may find that you can ""power through"" the deep learning math, and then you're golden.  You don't need to prove any theorems in deep learning, only to have a good intuitive understanding of derivatives, averages, etc.  Still takes work but much, much easier than a math degree."
MachineLearning,Same here. Got an interview almost 2 weeks ago and no news yet...
MachineLearning,"I am really enjoying your talk. You are a really  good presenter, something sadly not very common in academic settings."
MachineLearning,"You can try to do ML/DL without liking maths, but I think you'll either be bad at it or quickly stuck in your work.

But if you find solving problems which involve statistics / probabilities ""irritating"", never do ML/DL. Even NLP.

I mean it's easy, take a course on NLP or ML, in 2 hours you should see that it's not for you if you don't like maths and statistics."
MachineLearning,Very encouraging words! I will try my best :)
MachineLearning,"Thanks a lot, very glad to hear! :)"
MachineLearning,"QM is supposed to be good for computer stuff right? But it is in London so you have to deal with that (might be a perk for you!). Tbh it probably makes less difference than you think, so you may as well go with the one of the syllabus that you prefer.

I've never seen St Andrews but the QM campus is pretty nice and there's are some nice parks very near by."
MachineLearning,"Oh, understood you.

There you go: TensorFlow can run on a browser (for training and inference workloads, with GPU access via WebGL) via [TensorFlow.js](https://www.tensorflow.org/js). Very fun set of demos [here](https://www.tensorflow.org/js/demos). 

Probably PyTorch and other workflows have similar tools."
MachineLearning,Security cams with video analytics on-device
MachineLearning,Thanks so much! Very kind words :)
MachineLearning,but now it’s up to ¥16000
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,catboost is great but doesn't have RF mode and no plans for it.
MachineLearning,"&gt; I believe the term you should be googling is something along the lines of ""edge inference"".

Thank you so much! It doesn't *seem* to be what I'm looking for (or at least I can't find an example that comes close to what I mean, i.e. you open a website and do some AI task solely in your browser using solely your common hardware = any cpu or cuda gpu that's available (=&gt;no special ""edge devices"" or ""edge engine"" required that the client would need to manually install in an extra step). 

But maybe I'm misunderstanding the term. I will continue searching, thanks!"
MachineLearning,"* NLP

 I saw you consistently write that. Its Natural Language processing"
MachineLearning,"Function calls are typical. For example, for logistic regression, ```p = logistic(s)``` is better dealt with forward-mode whereas overall reverse-mode is more efficient."
MachineLearning,"[Misinformation] The data is google searches, not dbs in use."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Nope :( I usually pause and take some time to review the figures in-between paragraphs/sections.
MachineLearning,I don't know of one off hand. My best guess would be to combine a feature similarity metric (cosine for example) with the edge weight between nodes into a new composite edge weight and use that.
MachineLearning,"Hi. Working steadily towards 40x more users! :)

Not unhelpful at all, some of the suggested heuristics make a lot of sense. And the hybrid mode as well - that would probably requires us to setup a Kubernetes cluster, which would be something very useful to have in place as we scale. 

*I think you have a fundamental tradeoff between savings on machine time and average response time.* Exactly, and bounded by current technological limitations as /u/onyx-zero-software below mentions. This was my impression after testing the several solutions, but came here on Reddit to get some unbiased views from the hive mind."
MachineLearning,"If I get stuck in pre-trained models then there are more chances of I will not have a capability to do whatever it takes to solve particular problem. 

Then do I end up becoming half-ass ML expert ?"
MachineLearning,"No. Anyone who tells you otherwise is fooling you. Sometimes by trying to be nice one becomes condescending.

Having said that, you dont need to be Terence Tao to practice ML. With some smart choices you can learn 80/20 of the ""basic"" math that will allow you to become productive as an engineer.

Start with linear algebra, there is plenty of free and paid books and go from there.

You dont need to solve every single proof or problem, focus on understanding the concepts and see how they translate to algorithms and implementations."
MachineLearning,"Yes, something akin the lines of Virtual GPUs seems to be exactly the missing link! The results of this research pointed exactly towards a fundamental technology piece missing.   


LocalExistence's solution is indeed a nice compromise for the time being."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"If you really don't want to do maths i think you will pretty much be stuck to using pretrained keras models
The maths to understand Deep Learning basics are not hard, you should get into it in my opinion"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"In my use case it's not pratical because hardware on the user-side is somewhat limited and adding accelerators (like NVIDIA's Jetson Nano, Google's Coral) is an extra cost that does not make sense. 

However, that is a very common use case (and you can make it so your models are protected). I believe the term you should be googling is something along the lines of ""edge inference""."
MachineLearning,"Wow this extension seems very cool thanks for sharing !
Although it doesn't solve the problem of looking at a screen for way too long periods of time, i will still try it 
Does it handle well equations and other things you can find in papers?"
MachineLearning,"Definitely. They also only used ViT B-32 which has the worst performance among all the ViT models. I think something like ViT H-14 or even L-16 with a dataset like the one you mentioned, on the order of tens of millions of images and text could definitely further boost the performance while making minimal changes to the architecture."
MachineLearning,"Hi! Thanks, I was also not aware of that. Any idea of the timings involved (if you have any past experience)?"
MachineLearning,"Hi, thanks! From what I understand, while KFServing would drastically simplify the management and scaling-to-zero part, it would be ultimately limited by the underlying cloud's (assuming one is running on Azure, GCP, AWS) ability to provision the GPU resources, in which case I could expect loading times similar to my AKS test. Or did I missed any advanced integrations KFServing might include?"
MachineLearning,"I like this answer. Practically there are constraints on the cloud side when dynamically provisioning GPU instances vs. CPU instances. Up until Ampere, there was no true implementation of hardware GPU virtualization outside of theoretical academic projects and software-based multiplexing solutions (slow, buggy).

This basically means that to get a GPU instance, you have to have some interface that accesses the physical hardware (once someone has a GPU, you can't split work between multiple users except in some very specific use cases). That's why it takes forever to spin up a GPU instance dynamically, and why you haven't been able to find serverless support for GPUs.

That said, Ampere does support GPU virtualization at the hardware level, meaning down the road, cloud providers will likely be able to offer the feature you describe because GPUs will be able to be treated like CPUs in terms of being able to be split up amongst a group of dynamic users (whereas at the moment GPUs are bound at the hardware level to a specific accessory).

AWS has apparently already started using this type of tech as of this year (see lost below). They mention virtual gpus but this particular solution probably won't help OP unfortunately. 
https://aws.amazon.com/blogs/opensource/virtual-gpu-device-plugin-for-inference-workload-in-kubernetes/

As an alternative to GPUs that doesn't share the hardware limitation, OP might check out GKE with TPUs for your workload.
https://cloud.google.com/tpu/docs/kubernetes-engine-setup"
MachineLearning,"Not really but it depends on what kind of work you’re doing. Most current nlp leverages deep learning, so you’ll need some background in linear algebra to be an researcher. However I’ve seen people make the transition from linguist to DL NLP researcher. Again there are varying levels of knowledge depending on what you want to do, but machine learning engineer requires a pretty minimal amount of deep math knowledge"
MachineLearning,"ElasticInference billing, afaik, is like EC2 rather than like lambda"
MachineLearning,Was about to say Kubeflow / KFServing as well.
MachineLearning,"I had never heard of this (well, how can one keep track of all of Amazon's cloud releases, to be honest), but this seems more like it. Will look into into, thanks a lot! Hoping Azure copies this sometime soon so testing is easier..."
MachineLearning,"I believe with Elastic Inference, you still need an always running instance, configured with the accelerator. It's just cheaper because you're using a fractional GPU."
MachineLearning,"Sorry if this is a stupid question (I'm clueless about cloud services + web development, but hopefully it fits to the topic:

Is it possible to do the inference on the client's hardware (given that it's strong enough)?

Like, could you publish your model (and I guess all dependencies) on something like github.pages and let the client run the tensorflow javascript in their browser?

I guess it would run very slow, not everyone could execute it and it would make all your source code and model weights public. But it would also mean it's completely free, right? No server costs at all (or is there still a server required?)

I haven't found anyone ever do this and I don't know why to be honest. Is it just ridiculous to even consider it (because servers are so cheap?)? Or am i not searching correctly (again I'm not familiar with web development and maybe I'm just looking in the wrong direction)..."
MachineLearning,How about NPL ? Does it need less amount of maths like Calcus and algebra ?
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yes, the use case is really inference. Although I wasn't aware of all this new tools under AML's belt, they have improved! (yet they ultimately are defined by the Azure resources you have available/want to provision)."
MachineLearning,OP is referring to inference specifically
MachineLearning,"I am sorry. I didn't get your answer. Could you please eloborate for me ?

I also have follow up questions that should I focus on NPL Instead ..."
MachineLearning,Not at all.
MachineLearning,Thanks for sharing!
MachineLearning,Thanks for sharing!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I have been working with GNNs for some time now and I am very much fond of them as I am a visual person. Talks like these gives me a better understanding of the underlying neural architectures. Thank you very much !
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for sharing this Peter! I am currently working in this area and it’s application to catalysis and very appreciate the open source code, follow up blogposts and application discussion from you and Michael! Please continue this efforts it’s been very helpful"
MachineLearning,"I don't know how to name this ;d

I have in mind section 2.1 in paper."
MachineLearning,"For what it's worth, two years ago there was no way to do this in AWS or GCP. In our case inference time wasn't too critical, and CPU inference was fine. However, since then, AWS came out with Elastic Inference, which seems like the tool for the job, but I haven't tried it out yet."
MachineLearning,Mixture density networks could be worth looking into. E g [https://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/](https://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/)
MachineLearning,If you can run on Kubernetes then [KFServing](https://github.com/kubeflow/kfserving) is an open source solution that allows for GPU inference and is built upon Knative to allow scale to zero for GPU based inference. From release 0.5 it also has capabilities for multi-model serving as a alpha feature to allow multiple models to share the same server (and via NVIDIA Triton the same GPU).
MachineLearning,"Thank you, I really appreciate your work."
MachineLearning,Amazing! Your presentation should be the go-to tutorial for anyone wanting to get started with GNNs.
MachineLearning,Is there a paper for this work?
MachineLearning,"Monte Carlo Counterfactual Regret, specifically the tree search algorithm.

&amp;#x200B;

Game theory. Nash. Trees. Truly, it has it all."
MachineLearning,"Nice, I attended this talk and was looking for the recording - you did a great job explaining it! :)"
MachineLearning,?
MachineLearning,"Very cool. I will check it out. Recently I asked a related question so I hope you can help me. Can you give me some pointers to other groups building mathematical frameworks to describe NN?

I'm trying to do some sort of survey of all the approaches being used to construct mathematical frameworks to describe NN (of all kinds)."
MachineLearning,"You can look at Deep Graph Infomax, there is nice implementation in pytorch-geometric."
MachineLearning,"I'm literally reading the twitter threads for the eighth time today, and I'm still not seeing this.  Timnit was incredibly polite, did not hurl insults.  At worse, she expressed frustration with Yann not understanding the point being made and refusing to engage with the material on the issue people were trying to share with him."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"As i mentioned, it improves your capacity to affect change."
MachineLearning,"This is network science not graph theory. Graph theory is more concerned with math proofs about graphs while network science is more concerned with real world applications. The field arose from methods in physics and social science more than it did from graph theory. I just wanted to clarify that in case you wanted to investigate further as the term ""network science"" will get you more relevant hits on Google.

Community structure can be defined in many ways and it depends upon your particular application. But generally it involves grouping nodes together based on their connectivity in the network. The most common definitions describe a community as a group of nodes that are more strongly connected with each other than with external nodes.

So it is for clustering nodes not graphs.

I am not sure if community detection is going to help with anomaly or pattern detection. That said. The field of network science is a broad one and it does have algorithms associated with anomaly detection on networks (I have seen them before but I don't personally work with them)."
MachineLearning,"This is network science not graph theory. Graph theory is more concerned with math proofs about graphs while network science is more concerned with real world applications. The field arose from methods in physics and social science more than it did from graph theory. I just wanted to clarify that in case you wanted to investigate further as the term ""network science"" will get you more relevant hits on Google.

Community structure can be defined in many ways and it depends upon your particular application. But generally it involves grouping nodes together based on their connectivity in the network. The most common definitions describe a community as a group of nodes that are more strongly connected with each other than with external nodes.

So it is for clustering nodes not graphs.

I am not sure if community detection is going to help with anomaly or pattern detection. That said. The field of network science is a broad one and it does have algorithms associated with anomaly detection on networks (I have seen them before but I don't personally work with them)."
MachineLearning,"It was the opposite of what you describe. Yann replied without getting personal, and Timnit, seemingly unable to take criticism, got very personal if not abusive. Certainly wasn't respectful.

She has complained about not being heard, then when someone engages in dialogue they get abuse for their efforts. How does that promote healthy discussion around the topic?"
MachineLearning,"The lengths are used to reduce the batch size when otherwise there will be a OOM error on the GPU. You can make an informed guess on these to fit as much data as you can on the GPU before OOM.

Ideally you shouldn't have utterances that are too long (say 30 seconds). ESPnet has scripts to subset your dataset so that you don't end up with those long utterances."
MachineLearning,"I don't believe these algorithms will detect ""rogue"" nodes explicitly. However, I am not sure if that is a problem. If the data is bad and this node is a result of bad data then you probably need other means of detecting bad data. However, many real world networks have heavy tail degree distributions where a small number of nodes have a huge number of connections. This is perfectly normal and most community detection algorithms are vetted on such networks. There shouldn't be any issue with the clustering algorithm getting confused by very high degree nodes."
MachineLearning,"Is the user providing any input and is it acceptable if the solution is a little stale?

If the answers are no and yes, respectively, you could periodically do the computation and store the result and just serve it up as requesting."
MachineLearning,How does that make it ethical? Ethics aren't defined by how positive the outcome is for one party.
MachineLearning,"Might be an unhelpful answer, but the easiest fix might be getting about 40x more users so that the machine is only idle 20% of the time. :) A more serious answer might be trying to predict the request timing - if the users will e.g. only use it during working hours, you can slash the idle time in 3 or so by keeping the machine live during working hours, and outside working hours only start it up if there is an incoming request and if so keep it on for 5 min or something. You can also consider hybrid solutions like having a cheap CPU inference node which can always handle incoming requests, and some logic to determine when it's getting enough requests to make turning on a GPU machine worth it - the value of having a CPU inference node active here is that it alleviates the response time penalty when you mistime a boot/shutdown.

I would be interested in hearing a better solution, but it seems to me that if provisioning a sufficiently fast machine takes time and request timings are random, I think you have a fundamental tradeoff between savings on machine time and average response time."
MachineLearning,"Okay thank you, I will look into that"
MachineLearning,"Yes wrong words, skipped word, wrong pronunciation or wrong verse

Note: The verses are in sequence 
Thank you"
MachineLearning,"I can reduce the size further, but I still don't understand why I am reaching the memory limit of my 3080 for doing basic inference."
MachineLearning,"Thank you for the response!

For this current example I have a 1 Hour portion of 100 Hours dataset. I wanted to test on this dataset to overfit it and then go to the big one so I don't waste time experimenting. I managed to overfit using the following configuration:

    # minibatch related
    batch-size: 32
    maxlen-in: 300  # if input length  &gt; maxlen_in, batchsize is automatically reduced
    maxlen-out: 150 # if output length &gt; maxlen_out, batchsize is automatically reduced
    # optimization related
    sortagrad: 0 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
    opt: adadelta
    epochs: 200
    patience: 0
    
    # scheduled sampling option
    sampling-probability: 0.0
    
    # encoder related
    etype: vgglstm     # encoder architecture type
    elayers: 2
    eunits: 128
    eprojs: 128
    subsample: ""1_2_2_1_1"" # skip every n frame from input to nth layers
    # decoder related
    dlayers: 1
    dunits: 128
    # attention related
    atype: location
    adim: 128
    aconv-chans: 5
    aconv-filts: 32
    
    # hybrid CTC/attention
    mtlalpha: 0.5
    
    grad-clip: 1

With the following [results](https://imgur.com/a/oknCuj4)

I can see that from a point in time the model begins to overfit and I should stop there (\~step 4k). So now I should move on to a bigger dataset without LM.

I have one question, what do maxlen-in maxlen-out refer to? From what I understand maxlen-in refers to the number of mfcc time-steps that come in, so it should depend on the mean sample length. Maxlen-out should depend on the mean length of the samples transcriptions right?

Thank you so much for the advices! I am going to post on the github also."
MachineLearning,"[https://paperswithcode.com/lib/detectron2](https://paperswithcode.com/lib/detectron2) Our library pages now have efficiency metrics for you to plot (see Results). You can choose what to plot: Parameters, FLOPs, inference time, training time..."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Sounds like you're trying to implement guided backpropagation? 

Checkout https://captum.ai/. It has most of the common saliency map based visualisation techniques."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Im not sure if colab uses an ssd or not. But one way to increase data loading speed is to **copy the data files to the colab vm** instead of the **network mounted google drive**. You can do this using cp command in linux. If your dataset is not big you can even load the entire dataset in memory. From my experience, for textual data it is usually possible to load entire dataset in memory. For image data it is usually better to compress the images to **224x224**  **jpeg** rather than **224x224x3 tensor/nparray** and read from disk **as reading compressed jpeg and converting to tensor** is faster than **reading large tensor** from disk. You should always time your dataloading by looping the entire dataset without passing it through the neural network first though to see the time."
MachineLearning,"Not op but this problem is usually called detection and has a rich history in statistics and signal processing. In practice, it basically boils down to what u/hgruss said, I.e., changing the threshold of a likelihood ratio test. There are a few important concepts but the ROC curve should be enough to get you going: https://en.m.wikipedia.org/wiki/Receiver_operating_characteristic"
MachineLearning,"I see Karate Club i upvote. 

I’m a simple man of simple pleasures."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I think perhaps you might be interpreting it a bit differently from what he meant. For example in mathematics there are many proofs and explanation why one method is better than the other. However in machine learning field, people just state that their neural network outperforms the previous state of the art hence it is better then they look at a possible explanation why it is in fact better."
MachineLearning,"I've used ESPnet. Those loss/acc curves don't look good at all.
The max acc the model achieved seems to be just 30% (or 70% error). This is the token level accuracy (I assume characters of Romanian in this case) in ESPnet, so the WER is usually 3x the CER.
What's your audio dataset size in terms of duration (hours)?
The hybrid CTC/Attention model should give you good results even without an external LM if you have enough data.

To answer your questions

1. Can't say anything about the model without more details about the dataset and model architecture

2. Yes. Use the early stopping option and put an upper limit on the number of epochs. 

3. The features look alright. Although 80 bins and 25ms frame length is more commonly used.

You should open an issue on the github page. The team is very good at resolving issues and helping with best practices."
MachineLearning,"I have these three in an electronic format though, but I refer to them a lot during my master studies because they contain a lot of basics and foundations.   


1. Foundations of Machine Learning ([https://mitpress.mit.edu/books/foundations-machine-learning](https://mitpress.mit.edu/books/foundations-machine-learning))
2. Elements of Statistical learning ([https://web.stanford.edu/\~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/))
3. Pattern Recognition and Machine Learning ([https://www.springer.com/gp/book/9780387310732](https://www.springer.com/gp/book/9780387310732))

Of course there are a lot of great books out there that I have definitely missed. But these three books I have been studying from quite a lot recently and they cover a lot of necessary foundations and principles in Machine Learning and Statistics."
MachineLearning,lol that's not small size
MachineLearning,Why is it NSFW?
MachineLearning,"yes, it absolutely is hate speech to single someone out and lie about them being racist.  

(1) you're committing a civil offense against them.  there are very few areas of speech that are banned by criminal or civil law.  defamation, including both libel and slander, is illegal.  the covington kids made MILLIONS off the hate speech that you extremists spouted about them.

(2) when you lie about someone being racist who isn't, you're shitting on all minorities who have suffered actual racial injustices.  you're equating innocuous non-racist behavior with awful injustices.  YOU did that.  that's YOU being racist.

(3) when you slander someone racist who isn't, it's often because you're imposing YOUR racist biases against them.  it's like when certain politicians said something about crime reform, and immediately you woke bigots jumped to equating that immediately with minorities, that's YOU being racist.  when YOU interchangeably use ""minority"" with ""criminal"", that's YOU being racist.

so cut it with your hate speech, bigot."
MachineLearning,"For those of you watching at home, I want to clarify I have nothing against Yann LeCun who seems like a pretty smart hit and a leader in his field.  I do think he’s a big out of his depth when trying to tackle racism and social media.  For the record, I criticized his comments but spoke against anyone sending him threats or attacking his character."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"No, that’s not what hate speech means.  Sorry.  Try again later."
MachineLearning,slandering someone as racist is absolutely hate speech you bigot
MachineLearning,"I am also working on a similar problem of on-demand training

I am using the API's from the following document.

[https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines)

 Wouldn't this suffice your purpose ?"
MachineLearning,"&gt;recognize a mistake made by the user

What kind of mistakes are you looking for? wrong words, wrong verse or pronunciation?"
MachineLearning,"As I mentioned my image is 1mb, I started with a small size image to specifically avoid memory issues. 

The pic is 1376 x 774."
MachineLearning,"[SwiftRead browser extension](https://chrome.google.com/webstore/detail/swiftread-read-faster-lea/ipikiaejjblmdopojhpejjmbedhlibno?hl=en) shows you one word at a time. You set an average reading speed in WPM which it distributes proportionally to word length and punctiation. Can read websites and PDFs. Before, I would always daze off and loose my place, this allows me to focus. I set the reading speed based on how much retention I need, for example at 200 WPM I retain close to all the paper, if, on the other hand, I just need the general idea, I go for 300 WPM (~50% retention for me)."
MachineLearning,"If you have a exponential family you can divide the exponent by a temperature.
In general, raising the pdf to some power &lt; 1.

If you want to train to get this, add some well chosen noise to your targets."
MachineLearning,Calling someone a racist is not “hate speech” you dumb disingenuous fuck.  I’m sorry the English language or probably any language is too difficult for you to understand; even GPT-3 is clearly better at it than you are.  It was truly my bad for insulting incels by suggesting they would put up with your brand of stupidity and ignorance.
MachineLearning,"I saw somewhere that prophet performs quite poorly compared to simple classical competitors. Sorry, I don’t have a link now."
MachineLearning,"I print them out but take notes in a searchable format on my computer, with tags etc so I can find relevant papers"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you for your explanation! I agree; I'm not sure why this is referred to as a type of meta-learning; due to the reason you implied.
MachineLearning,I think you are referring to adjusting my predicted Y so it classified not on a 0.5 threshold. That would give me more False Negatives basically.
MachineLearning,"there's no ""reverse racism"" ... it's just ""racism"".  and when timnit and her mob singled out yann lecun and slandered him with ridiculously baseless and false accusations of racism and sexism, that's just hate speech.  you come in here defending hate speech? expect to get called out on it.  you're just wildly unprofessional.

and you know you're full of shit because instead of proving me wrong, you start projecting all that incel nonsense.  get the fuck out."
MachineLearning,"there's no ""reverse racism"" ... you come in here defending bigotry and hate speech, expect to get called out on it"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You could look into defect detection or anomaly detection.
MachineLearning,"In some WoW expansion, there was a game where you have to untangle nodes.

Here is another version of the game which I play when to chill, when I read or listen to a podcast.

It's not the same thing, but it reminded me of it anyway:

https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/untangle.html"
MachineLearning,"Thanks for the quick answer. Can you please provide more details, perhaps a link to an example?"
MachineLearning,"Hi guys, I currently training an RNN using MSE as the loss function. The loss value is very low but when I visualise the results (my problem deals with trajectories), the predicted points are not ideal. This is probably because the data points are all very close to one another (the latitude and longitude values are very close). Any idea on how to make my model learn better?"
MachineLearning,You could predict the class probabilities and make a custom split at another threshold than 0.5
MachineLearning,No because ML is not advanced enough for that.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Oh, so you’re one of those people.  I thought I was dealing with a rational human being arguing in good faith.  Go back to r/reverseracism or whatever shithole you crawled out of and chill with your incel pals.  XD

If you can’t even comprehend the meaning of basic concepts like hate speech and bigotry, you’re probably not up for handling the complexity of a field like AI."
MachineLearning,Do you mind pointing me to the paper? Tried to google but no luck.
MachineLearning,CLIP shows great promise.
MachineLearning,"Good job!

It would be nice that [communities](https://github.com/shobrook/communities/tree/master/communities) natively supports both [networkx](https://networkx.org/) and [igraph](https://igraph.org/python/) data structures."
MachineLearning,"As OP, I found it helpful, but like the other person said, it's a very bad idea to get upset about a downvote. Really, even acknowledging downvotes is a bad idea. The only reason I can think of that would cause a reasonable person make an edit as you have is if they thought OP downvoted them after giving them a useful comment.

Rest assured, I found it helpful and you got an upvote."
MachineLearning,Wouldn’t the “universal transformer” handle sequence length growth given controlled training of RNN. Please educate me if I am wrong on this.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,roughly 1TB
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Lol this reminds me of CVPR ~'15 where people were advertising that their papers did**n't** contain any ML :)
MachineLearning,"I dont work at Google. But, it is by far the best place to do ethics research. I really care about ethics research and thats why I say so. Look at the fortune 500 companies, and see how much any of them spend on ethics research. Lets say facebook publishes some research on fake posts etc, and to some extent twitter. So, for the rest of them - banks, oil companies, auto manufacturers, telekom etc, ethics doesn't matter. So i am all for Google in this, hell, nobody shouts at these other companies, guess why, they wouldn't entertain it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"For what purpose? Is it just to create a more realistic dialogue? The AI wouldn't actually learn learn concepts from the replies, and doesn't actually have an understanding of the ""inquisitive"" questions it would ask. It's just learning how to replicate human conversations so well that it seems real."
MachineLearning,"I think you're anthropomorphising a bit too much there. I get the feeling that you're indirectly/subconsciously asking ""Why isn't there research into asking questions that humans like asking?""

Also, I've only ever seen seen research into generating _answers_ from text. Never seen anything about generating questions."
MachineLearning,me too
MachineLearning,I agree and believe a lot of products and solutions are going to emerge in the coming years for on device AI
MachineLearning,"Might be possible to automate posting to twitter. If the posts with the grid image have only one image as opposed to the collage post, then it would be a simple if-branch right?

Looks like there are some services that do reposting as a service too. This one is free [https://medium.com/@marckohlbrugge/a-better-way-to-post-your-instagram-photos-to-twitter-7f3a04a37d89](https://medium.com/@marckohlbrugge/a-better-way-to-post-your-instagram-photos-to-twitter-7f3a04a37d89)."
MachineLearning,"How about twitter?

Just post the grid and maybe a link to the collage instagram post, it'll be helpful since I get a lot of ML news from twitter and mostly nothing from instagram!"
MachineLearning,"For example VR/AR tech on phones. Not sure if smart home devices like amazon echo or facebook portal uses on device AI. On device AI offers better privacy and faster response, at the cost of smaller/less accurate models."
MachineLearning,What is the standard Bible for neural networks? I have learned everything I know from Andrew Ng on coursera
MachineLearning,Even
MachineLearning,Awesome work. Do these algoeithms take adjacency list too?
MachineLearning,"Yes, that's a cool app as well! It seems to be limited to zero shot classification though. 
With  cliplayground.co you also have the freedom to try any sort of prompt as input, as well as uploading multiple images and ranking them. :D"
MachineLearning,I need a robot to tell me that I am pretty.
MachineLearning,You'd prob be better off with gephi.
MachineLearning,"You said a bunch of things about actions and stuff I didn’t really understand, but I think you are saying your classification forecasts are very confident? 

That’s typical behavior: https://arxiv.org/abs/1706.04599"
MachineLearning,"Thank you :).

I am aware of [another similar web app](https://clip.kiri.ai/) from Kiri."
MachineLearning,Glad you find it useful! :)
MachineLearning,"I was reading about Bayesian Inference in the Deep Learning Book. When trying to find the MAP estimate of a parameter, the equation is 

θ MAP = argmax θ p(θ | x) = arg max θ log p(x | θ) + log p(θ).

If we assume a linear reg model and let the prior be a normal dist with mu = 0 and covariance matrix of 1/λ\* **I**^(2), the log prior term becomes λ \* θ ^(T)θ . 

The text says the log-prior term corresponds to weight decay. I do not see how. When maximizing the log probability, for any set of parameters with similar values of log p(x | θ), larger θ would be favored would they not? Sure λ could be made negative to push the parameters lower, but then we have a covariance matrix with negative values which does not make sense. I am not sure what I am missing. Any insights would be appreciated."
MachineLearning,"MLPP by Kevin Murphy, GPML by Rasmussen, Numerical optimization by Nocedal"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,That answers a lot of my questions! I will take it for a spin - really cool stuff. Thanks!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I simply used the architecture of AnimeGANv2 since I had it in my hand. The code is a huge mess but I will try to clean up and share it.
MachineLearning,check https://github.com/bryandlee/naver-webtoon-faces
MachineLearning,You don't have to worry about Google stealing your models: you're so dumb your models are probably shit.
MachineLearning,"Tech giants don't sell your data, they use it to train their models. This is why models are publicly available but If the sold it: everyone would have as good models as them. This is one of the most common misconceptions.

If you are storing something illegal on their servers, and set it as public. They will remove it, that's a no brainer. Try storing porn, on gdrive and share the link on public forums, if it gets popular it'll get removed. 

   Google employers are literally hearing to and laughing about your personal, private conversations with your wife in your bed at night, or the kind of porn that you watch if you ever get to submit an application to work there. Don't you think stealing a good machine learning model is a no brainer for them?

Hey, if you can prove that they do any of these things, you have a multi-billion dollar lawsuit on your hands. If you don't: then please go and stand in the conspiracy theorist corner with flat earthers."
MachineLearning,Checkpoint your model at regular intervals to Google drive
MachineLearning,"GitHub repo: [https://github.com/Rishit-dagli/Gradient-Centralization-TensorFlow](https://github.com/Rishit-dagli/Gradient-Centralization-TensorFlow)  


Hope some of you find this useful!"
MachineLearning,could you provide the code for distillation? and how did you decide the architecture for the distilled network
MachineLearning,"No, but it helps a lot. You need to show proof of ability somehow."
MachineLearning,You can start experimenting right now.
MachineLearning,That's already a basic skill. Any ML engineer should be able to implement an arbitrary architecture.
MachineLearning,Great work! Where can I find the code of SwappingAE ?
MachineLearning,I print them out and take notes in the margins.
MachineLearning,"Quality content, ya hurr me."
MachineLearning,"It is not exactly what you are looking for; however, Open-domain QA can be solved with a retrieval component considering large text corpus at once. For example, [REALM](https://arxiv.org/abs/2002.08909v1) attends of the entire Wikipedia.

Moreover, the processing of long sequences is currently considered in the context of Transformer-based language models. Promising solutions rely on sparse attention with a global receptive field such as [Routing Transformer](https://arxiv.org/abs/2003.05997) or [Reformer](https://arxiv.org/abs/2001.04451). They are able to consider much larger sequences and it would be my starting point if complex relationships are required to solve the task.

For classification of the book, it may be enough to process it chunk-by-chunk and average representations before the classification layer (similarly to what was done in [Sentence-BERT](https://arxiv.org/abs/1908.10084)).

Finally, there were several attempts to locate crucial parts of long documents before going further. It can be done in an end-to-end manner as done recently in the context of [summarization of long documents](https://arxiv.org/abs/2009.05169)."
MachineLearning,Thanks for sharing!
MachineLearning,Thanks for sharing!
MachineLearning,We actual tried implementing both ELA and Splicing. Both work up to a certain extent only as the tampering varies in terms of several image attributes
MachineLearning,&gt;it seems more akin to sorting and thereafter grouping them.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Kindle DX. I own one
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Sure:

1) First paper on time series discords: Rejected from SIGKDD, best paper ICDM.
2) First paper on time series chains: Rejected from SIGKDD, best student paper ICDM.
3)  Clustering of Time Series Subsequences is Meaningless. 5 or 6 rejects, now 800 citations from ICDM publication. 
4) First Matrix Profile paper, Rejected from SIGKDD 2016, now 300 citations, and we wrote 30 spinoff papers in just 4 years. 

To be clear, I am not the only person that could write such a list. The first paper on SIFT [5] was rejected twice, it now has 20,000+ citations. 

As Steely Dan says... *Well, you wouldn't even know a diamond if you held it in your hand
The things you think are precious I can't understand*

[1] https://www.cs.ucr.edu/~eamonn/DiskawareDiscords.pdf
[2] https://www.cs.ucr.edu/~eamonn/chains_ICDM.pdf
[3] https://www.cs.ucr.edu/~eamonn/meaningless.pdf
[4] https://www.cs.ucr.edu/~eamonn/PID4481997_extend_Matrix%20Profile_I.pdf
[5] https://en.wikipedia.org/wiki/Scale-invariant_feature_transform"
MachineLearning,"Step 1 Cry

Step 2 Consider quitting your PhD program

Step 3 Cry more

Step 4 Resubmit your paper


What you should know is that reviews are basically arbitrary. Completely garbage papers full of spelling errors get in, top papers with a bunch of awards and a thousand citations that transform the industry get rejected.

Quantity &gt; Quality. Nobody has the time to go through your CV and actually look at what you've published lol. They put your name in google scholar and look at your most cited papers and then sort by most recent and look at that.

I'd be surprised if anyone actually ever looked at your CV except when you're trying to get tenure and even then it matters a lot less than you think. 

A solid publication record (several papers per year), exploring interesting things etc. is a lot more important than aiming for ""the top conference"". Nobody gives a shit as long as it's not a trash journal/conference.

It's better to publish 3 good papers per year in a non-top conference/journal than publish 1 paper in a top conference/journal. There is a certain elitist circlejerk around only publishing at ""top journals/conferences"". 100% of them wash out of academia because it's inevitable to have a few papers rejected and that dry period of no publications on your CV will ruin your chances of advancing in your tenure track.

The difference between a ""top conference"" and a ""good conference"" is small in quality of papers but the amount of effort and stress to get publish is much, much smaller.

I personally went through publishing in top conferences twice successfully and swore to never do it again. Academic life got so much better when I got to publish in actually relevant venues, started winning ""best paper"" awards and didn't need to stress whether my paper would get accepted.

The difference between a ""top"" conference and a ""good"" conference is usually that the ""good"" conference is more specific so fewer people have relevant papers for it -&gt; it's easier to get in, feedback is actually useful etc. Quality of papers is usually identical."
MachineLearning,"During training, yes, you'll need to crop same sized images for training batches. Otherwise, no. As long as no part of your CNN architecture requires a specific size, fully convolutional architectures can scale to any size. That's one of their key benefits."
MachineLearning,"Hi I’m trying to get grounded in text2speech. I was curious if anyone knows any good papers or more specifically surveys on modern text2speech implementations.

Sorry if it is text-to-speech. I come from the other areas of the nlp world."
MachineLearning,"I imagine you can just come up with a resizing scheme that works for all/most images without corrupting the signal too much. You'll probably need domain knowledge e.g. Try a resizing scheme, view a representative sample of images under the resizing scheme, see if a human can still classify it. If not, rinse and repeat"
MachineLearning,The complete works of stack exchange.
MachineLearning,"Unfortunately yes, you can get published without a test set. Look at literally any metric learning paper in the last 5 years for example."
MachineLearning,"My comment here is getting a little further away from the original topic on ""bringing more diverse views into the AI ethics conversation"", and getting a little closer to the ""soapbox HR drama""... but I fully agree with your last paragraph.

Timnit et al. argue that requiring them to be civilized is [punishing them for externalizing the trauma that they are victims of.](https://mobile.twitter.com/timnitGebru/status/1363573298328657925) As a female engineer who's participated in Lean In circles before, I thought that was the whole point of semi-private support groups. Everyone has negative emotions, but I used to think people generally agreed that there are situations where you can rant, vent, be uncharitable, and share advice with your friends, and there are also situations where you need to be in advocacy/PR mode, where the things you say can be read and judged by your whole company and/or the entire world, supporters and critics alike.

It really confuses me how Timnit et al. came to decide that public twitter, easily accessible company mailing lists, and (previously) public Facebook is where the dirty laundry belonged. I don't know why they just don't create a private chat group and put the rude/angry stuff in there, especially considering how traumatized they feel about perceived online harrassers. You're broadcasting anger and hate to the world via public twitter, how can you not expect to attract anger and hate in kind.
 (Maybe they do have a private chat group... but instead of getting it out of their system they're just egging each other on.  But I digress)

On a self-reflective note: my father used to say that people should never, ever talk politics in the workplace. (first-gen low-income college student, grew up working class, before someone comes in and says he's speaking from privilege.) 
I'm much more casual about this than he is. He might consider my attitude towards acceptable behavior almost as horrifyingly inappropriate as how I consider Timnit et al.'s actions..."
MachineLearning,What's the size of your image? How about downsizing it?
MachineLearning,Thank you for your valuable service and contribution to the community.
MachineLearning,"You cannot function in life if you are going to get paralyzed or angry by every single instance of criticism (it denotes an unhealthy personality too). Go to youtube, even the most wonderful videos will have 1% or so of downvotes . What are you going to do now? To check constantly your comment and add an edit every time the number of likes decreases because that means somewhere, somehow,somebody didnt like your comment? 

I just downvoted your second comment just to give you practice and chagrin."
MachineLearning,"I actually take the opposite view that it is less elitist than suggesting that someone is only good enough to submit to mid-tier conferences due to financial reasons or arbitrary ranking of schools. I'd rather think that anyone can submit to top-tier conferences and there is no reason to self-limit.

It seems, though, that you are confusing tier, specificity, and audience. These need not be separate. COLT is a very high-tier conference that is also highly specific and has a narrow audience. Additionally if you are submitting to a conference because it is ""top-tier"" but the wrong audience (as you suggested that you did) then of course you are going to receive poor reviews. If I submit my ML paper to the best social psychology journal, I'm not going to be surprised if the reviewers get caught up trying to figure out how this relates to social psychology.

I define mid-tier as being on-audience, but lower competition and less rigorous review process. This would be a bad call and would look bad on a CV. As an extreme case, consider how an Arxiv paper looks on a CV."
MachineLearning,That *was* a really good talk — thank you!
MachineLearning,"I'm not asking the community in general. I know that at least one anonymous downvoter exists (I'm aware of reddit's vote fuzzing algorithm and am fairly certain they don't generally fuzz new content to zero because of the harm to the user experience), and I'm talking to them directly. 

I don't need more data, I addressed that question to a specific individual. I get that your comment was tongue-in-cheek, but you're re-interpreting my complaint as if it's one about the overall score my comment received, which it's not."
MachineLearning,I've found it behaves a lot like other implementations if you set the same max depth for both and then possibly set max leaves to 2**max_depth so that it doesn't really grow the leaf first uneven trees its known for. I've heard good things about catboost well and categorical is right in the name.
MachineLearning,"https://www.nature.com/articles/s41598-019-41695-z
Read the abstract"
MachineLearning,"Maybe you should wait more than 6 min to see how people react to your comment. You should wait for more data points, you are in the ML sub after all."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Cracking the coding interview
MachineLearning,"To an independent party? Sure. To one of your friends, not as much so."
MachineLearning,"

If that were really the case, the ethical move would be recusing yourself entirely"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Even though I never went with a VM, I straight up always installed an OS directly on my computer. I would still highly recommend a VM."
MachineLearning,Is there a way to convert html svg tags to png/jpg images? Lets say I scrap an svg tag using BeautifulSoup. Can I convert it to an image so that I can download it?
MachineLearning,"I computed the mean style vector from the original conditional model (swap ae), then used it with celeb-a images to generate source-target pairs. Then trained the lite model with the image pairs using L2 + VGG loss for 50k steps. Finally fine-tuned the model with GAN loss for additional 2k steps."
MachineLearning,"I literally needed this this month. Saved, will be experimenting!"
MachineLearning,"Hi, it seems like the model is loaded but the image files are skipped. Is your file extension included in the list in line 39? If not, you can add the extension to the list.

btw, use \[test\_faces.ipynb\] script if you want to try the face model. Paprika style model is converted from the tensorflow checkpoint and it is best to use the original tensorflow version."
MachineLearning,I use my iPad 12.9. The pen works and feels great and IOS has great compatibility. There is an infinite number of apps for all needs. I also like that is very independent. Moving documents in and out is extremely easy regardless of what you use.
MachineLearning,"I always end up just printing papers if they are actually important to my work. Nothing really beats that. Generally speaking that's what I see senior researchers do, they look at the results (tables I mean), if they are worth their salt and do not look suspect, then they print it out and read it over a coffee break. That's how you end up filtering most papers. I'm pretty sure there was a Hinton quote where he was implying that reading too many papers ends up polluting your brain with other people's ideas and I sort of agree, though I think it's less relevant if you read from a wider array of subfields, instead of just focusing on Transformers or something. I know the rush of reading a ton of interesting looking papers, but chances are you'll just waste your time (this is a guilty grad student in me speaking) that you could spend running experiments instead. I guess one aspect I'm overlooking is that they also filter papers by using their students and other collaborators bringing interesting papers to them. Older researchers err on the side of being to quick to dismiss, while younger students tend to be like puppies with new papers. So there are a ton of heuristics you could you use to adjust your belief (God I hate Bayes speak). That one aspect of having a good lab with multiple diverse researchers in it that I miss a lot.

Hopefully, this wasn't unhelpful or too off topic, but other suggestions, such as using ereaders are good as well."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Wow dude, I gave you a coherent and internally consistent rationale for eating other animals being okay, and then you fly off the handle and say I don't deserve to live.

Fuck you."
MachineLearning,"Not sure, kinda new to reddit but seems they thought it was spam. Idk I just thought it could be interesting for people who wanted a quick summary of the situation. Whatever it's fine :P"
MachineLearning,"This is exactly what I was looking for. I really hope you can help answer this potentially stupid question, but are these clustering algorithms able to identify a rogue node(s) that has connections with everyone and potentially confuses the clustering?"
MachineLearning,"&gt; Yann Lecun did not get run off Twitter.

they defamed him, libel and slander, baselessly accusing him of racism and sexism.  if they weren't unemployable losers who aren't worth suing, he would sue the shit out of all of them and win just like the covington kids did.  it's just hate speech and it's fucking appalling that you defend it.  

the fact that you think there's nothing wrong with that hate speech shows just how bigoted and hateful you really are.  you're just objectively wrong, and your bigoted hateful opinion matters for zero.  cut it with that shit.

and again, your position on margaret is just objectively wrong.  the link on it is already posted in this thread.  and if you still don't want to buy that, look at any of the docs from the damore case, where these same exact people openly espouse anti-white and anti-male bigotry and think it's perfectly okay... or the sexism lawsuits that found not only were the 3 female plaintiffs not discriminated against, google got in hot water because the case proved they're unlawfully discriminating against white and asian males."
MachineLearning,When I bought it they had an offer that I could return it with full refund within 30 days if I did not like it. It didn't happen because it solved my paper reading problem plus it is a fantastic notepad and e-book reader.
MachineLearning,"I have also found reading papers a difficulty with my dyslexia and eye strain. I have tried screen readers but they typically have trouble with papers because of the citations and equations within the test, it can get it wrong and ruin the flow of the sentence."
MachineLearning,Mendeley on the iPad has the option to invert colours so it will be white text on black paper! A nice compromise. Mendeley can also import directly from arxiv.
MachineLearning,"I've heard of it, they are quite expensive tho but it's probably worth the price"
MachineLearning,Wow.
MachineLearning,"This video changed my world view of Gaussian Processes: [https://www.youtube.com/watch?v=aICqoAG5BXQ](https://www.youtube.com/watch?v=aICqoAG5BXQ)

[gpytorch](https://gpytorch.ai/) makes it even more fun to play with GP's. But see the video if you want to ground the concept of GP's and how the kernel trick is the coolest thing you will learn in Machine learning :)"
MachineLearning,"Ages ago, I bought the biggest kindle available and used to read ARM documentation on it. Pluses were that you could easily search, but the other big plus was it was really handy to have it down near your keyboard when you were then coding something related to it on the screen.  It was really big, I forget the model, but most of the kindles now are far too small to this"
MachineLearning,"I read my papers on a reMarkable tablet. It is easy on the eyes, I can take notes and highlight passages. I can't read on a computer screen for long periods of time because it hurts my eyes and printing papers to read got a bit out of control (huge pile of papers on my desk was apparently a fire hazard!)"
MachineLearning,"Not going to waste my time with someone arguing in bad faith, I don't enjoy debate for the sake of it with someone who is making no effort to arrive at a meaningful guide for living life. If you genuinely believe morality shouldn't extend beyond self serving survival mechanisms and have no care for the life of other humans beyond their preservation preventing your own death, you're a truly terrible member of society and would be better off dead than roaming the earth with this view."
MachineLearning,"Thank you very much!
I will come back to that offer later. Currently I have a lot on my mind.
Although with more and more people interested in that the actual betting offices should start using Machine Learning too in the near future. Should it be fun for me than I will do it just for the fun and not with any hope to earn something, I think."
MachineLearning,I don’t know what just happened but I wanna come together like that.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It fixes some issues with broker node iirc 

https://www.nature.com/articles/s41598-019-41695-z/"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'd say you're thinking about this backwards. Put your project on github, and then when you want to work in colab, just clone the repo. Any changes you want to persist from colab, just push commits to the github repo."
MachineLearning,"Try using PyTorch without any .backward or autograd.grad calls. Mainly focus on MLPs and try manually computing the gradients by accessing the .grad values. Will help you understand how the gradients are “chained backwards”. 

Also take a look (again in PyTorch), about how it’s possible to PASS BACK manually specified gradients into the .backwards function. This will lead you down a path into Jacobians work and how the gradients of “later” layers affect the gradients of prior layers in the backprop step. 

It takes a while to “get” it, but console yourself in the knowledge it’s not necessary to fully understand until you’re doing serious algorithmic/systems design."
MachineLearning,"This might be a decent intro, specifically discussing GP's in the context of hyperparameter optimization: https://distill.pub/2020/bayesian-optimization/"
MachineLearning,"Hi! I'm a biologist looking to find a way to determine a specific frame in a set of videos. I thought that machine learning could be a good way to approach this problem, because I have a pretty big set of videos where the looked-for frame has already been localized, so I thought that makes up my ""training set"".  The question is basically what framework or library'd be good to use for this project, and how I could move forward in attempting to do this."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thank you so much for the time you took to answer me, i really appreciate it  
\[edit\] It looks very promising btw"
MachineLearning,"Thanks for these suggestions. I'm going to give that a shot. In order to be able to use a CNN like you describe, wouldn't the images all have to be of the same size?"
MachineLearning,Why was that deleted?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Can you recommend any good introductory papers or articles? TIA
MachineLearning,"EigenGame: PCA as a Nash Equilibrium
https://arxiv.org/abs/2010.00554

Oral at ICLR 2021. Probably one of the most novel and important papers for a while. They reinterpret PCA, a century old problem which is ubiquitous throughout all of ML, stats and engineering, using game theory and come up with new algorithms that can scale to 10’s of millions of dimensions."
MachineLearning,Not familiar with Leiden clustering. Can you send me a link about why it supersedes Louvain?
MachineLearning,"No, the one in the animation is Louvain’s algorithm. But the library does offer a spectral clustering algorithm which does use k-means."
MachineLearning,No but for some you can specify how many communities you want to partition the graph into.
MachineLearning,Learn the math. Do it all by hand first. Make sure you really understand every single thing that’s happening. It’s the only way.
MachineLearning,"Being further left is not being more ethical.  Those on the left see others as being less ethical but others don't see it the same way.  For example, people on the right see people who approve of abortion as being unethical.  Neither the far left or far right is more ethical than any other part of the spectrum.

In fact, the constant implication that others are bad is what prevents an actual discussion of the issues.  Why would anyone try to engage in discussion with a figure like Timnit Gebru about how her ethical viewpoints are flawed if her fans are just going to accuse you of being evil and try to get you fired?"
MachineLearning,"There has been a growth in the number of papers using Gaussian processes over the past year.

I find GP's very cool. I started loving GP's once I understood the kernel trick and how they can make ML so much more interpretable."
MachineLearning,"I'm a few weeks into a Deep Learning course now and I pretty much ""get"" back-propagation conceptually. My main issue now is the math -- Why and when do we transpose? When do I sum up rows? How does matrix multiplication generalized to batches and tensors? etc"
MachineLearning,I love when I'm working on a problem and something that can be very useful to solving it just pops up in my feed :) Looks very neat!
MachineLearning,"Shouldn't the ethicists be (and expected to be) more fair and ethical than mainstream society?

(That said I also don't like the endless 0-informational dogwhistling of ""capitalism bad"" and how everything is always due to American imperialism, meddling, CIA, etc. But \*ethically\* there is a huge fucking mountain of past exploitation that is yet to be meaningfully addressed, right? Or is that not something folks agree on? No, I have no idea how to measure it fairly. However simply enacting a lot of classic redistributive policies to help the bottom quintile/tercile would go a long-long way.)"
MachineLearning,"I felt the same way and left the course at that point and did everything else machine learning related, use nn's for almost a year and a half, till I saw it again after doing the deep learning course from Andrew.
I finally confronted it using the site [neural networks and deep learning](http://neuralnetworksanddeeplearning.com), took a pen and paper and started writing.
I also tried to code it, couldn't do it by myself but in the process found many great sites.
Here's the GitHub [GitHub link](https://github.com/veb-101/Neural-Networks-from-scratch).
It's scary at first and many times after that, but once you start thinking about it; it's quite intuitive on it's own."
MachineLearning,Any particular reason you're avoiding NNs? Just curious.
MachineLearning,"Happy Sunday people!

Video 7 / 52 - Episode #009 of Research Papers Summary

Neural networks are good but they lack interpretability. This week's video covers how we can use instance-based learning to improve the interpretability of NER models! The methods also applies to other similar NLP tasks!

Please check it out and any feedbacks are welcomed! :)

Ryan"
MachineLearning,"The last part is more a reference to how the NN field in the 90s was stuck on e.g. using tanh as non-linearity or energy based modeling, because that kept them closer to theoretic results from SVMs and regression.

Also the story of Sutskever and Krizhevsky versus Hinton comes to mind.

That is a bit why I am of the opinion that theory has"
MachineLearning,"It's an algorithm that tries to arrange vertices into a community structure based on their shared edges.   

In other words it tries to arrange the nodes into groups so that each of the groups share very few connections between one another but share many connections with other nodes in the same group.  

[Here is the Wikipedia page for community structure if you wanted a more in depth explaination.](https://en.wikipedia.org/wiki/Community_structure)"
MachineLearning,"I have some questions about training and sampling from (restricted) Boltzmann machines: 

I've trained a RBM on (labeled) handwritten digits from the MNIST dataset. 

Specs are: 784 + 10 visible units (28x28 pixels plus 10 for label encoding), anywhere from 30-100 hidden units, 1-10k training images, 2-10 training epochs

Classification works pretty well, and I can sample to obtain legible handwritten digits. The problem is that I now try to clamp the label nodes to specify which digit I want the RBM to generate. The results seem almost completely unrelated to the target label, and depend only on the distribution used to initialize the sampling process. (For instance, if I initialize the pixel units on/off with equal probability, then the network will always dream up an 8. I suspect this is simply because 8 usually contains the most pixels of all the digits and so is closest to the initialization.)

Somehow, the machine is not learning that the nodes corresponding to label are very important. This is also supported by the fact that the classification signals tend to be very weak, i.e. even when the machine is classifying correctly it is not very confident. Why doesn't the machine rely so much on the label nodes? And is there a systematic way to obtain a greater variety of samples? I have found that initializing my samples with a gaussian distribution yields good variety, but this is kind of ad hoc."
MachineLearning,This stuff is so fun and neat :)  thanks!
MachineLearning,"Ok, I get it.

Well, may be you should consider one very typical and practical problem - the data compression (also related to Edge Computing and Edge Analytics in IIoT scenarios). 

A usual application scenario is a factory floor with hundreds of sensors that send data every 100ms. A naive data architecture would be to encode this data in JSON and store it somewhere in the cloud and then analyse them using the usual tools like Hadoop or Athena. This is very expensive because you pay per Gb of data, and the most data is not so valuable and doesn't change much every 100ms. 

So the question is, how to compress the data in a rational way. I mean, not just apply LZ or Brotli on the data, and not just aggregate it using some stupid fixed rule (like store only a one-minute average of the readings), but doing something more smart. 

Something that would keep more signal in times the data carries more information, and then reduce the bitrate in times the data is not carrying information. Can be something with adaptive averaging windows, anomaly detection, fingerprinting etc.

You store your readings every second and it feels like ""too often"". The applications about urban noise metering I know operate with 1 minute to 15 minute windows. So the smart compression might be a worthy problem for your use case."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Usually yes, or sometimes a bird instead of a dog. The 2nd and later images are usually much more closely related to the text description. You're not doing anything wrong :)."
MachineLearning,"I understand that.  In my case, 100% of the time the first output image is a grey dog-like on a green grassy background.  Is that specific detail what you usually see as well?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Yes. The first output image (using the default values for other parameters) is usually not  closely related to the user's text description.
MachineLearning,"I'm weak in graph theory.  Is this largely for clustering nodes within a single graph based on some set of adjacency similarities, or is there a way to cluster graphs based on similarity to each other?  If either of those questions make sense?

I'm exploring the use of graphs to model enterprise infrastructure configuration and it seems like this could be extremely helpful for pattern and anomaly detection, but I don't know if I'm thinking about it correctly."
MachineLearning,"It has been a long time and I don't remember, but I think it took me at least a few days to get comfortable with the concept and apply it to arbitrary combinations of activation and loss functions. Once it clicks, it is essentially ""just"" applying the chain rule and gradient descent. Personally, it helps me when drawing out the computation on paper. That's also how I am teaching it now. If helpful, I have videos for this semester online (the one for MLPs follows in the next couple of weeks):

L5.6 Understanding Gradient Descent: [https://www.youtube.com/watch?v=L4xzybIa-bo&amp;feature=youtu.be](https://www.youtube.com/watch?v=L4xzybIa-bo&amp;feature=youtu.be)

L6.2 Understanding Automatic Differentiation via Computation Graphs: [https://www.youtube.com/watch?v=oY6-i2Ybin4&amp;feature=youtu.be](https://www.youtube.com/watch?v=oY6-i2Ybin4&amp;feature=youtu.be)"
MachineLearning,try working it all out with a pen and paper. it took me a few days of repetition and thinking about it (and headaches lol) but i got it in the end
MachineLearning,Is it normal for the first image to always look like a grey dog in front a grassy background?  Because that is what I am seeing with a wide range of inputs
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Sure, whatever"
MachineLearning,"There are a lot of great suggestions already. 

If you do end up with a website, you could try something that has the same look as Instagram as you like. Take a look at the mediumish theme for Jekyll/Hugo"
MachineLearning,"Wow this is literally exactly what I needed, thank you so much."
MachineLearning,"I'm just a poor phd-student on a less than triple A tier university, but to me this sounds very elitist, doesn't it?

In my experience getting a paper accepted in a mid-tier conference that's more focused towards your work (and thus has a better review process) is just as good as some broad A-tier conference.

Submitting to an A-tier conference gave me the worst reviews I've ever seen because the reviewers were solely focused on the idea and didn't appreciate the domain-specific effort (this might not be applicable everywhere)."
MachineLearning,Looking forward to trying this! Thanks for sharing and your hard work!
MachineLearning,"Nicely done!

If you haven’t already, check out cdlib, which similarly uses a common syntax to wrap multiple community detection algorithms. 

[http://cdlib.readthedocs.io](http://cdlib.readthedocs.io)

I particularly like their visualization of different graph and community metrics based on different algorithms applied against the same graphs."
MachineLearning,Im glad that ML is finding more and more uses in bringing humanities greatest historic achievements to modern era!
MachineLearning,What algorithm did you use?
MachineLearning,This is absolutely beautiful. Can you share how big of a dataset this can handle (I.e. number of nodes)?
MachineLearning,"Just google backpropogation and look at the various articles people have written to build likr a general idea or intuition for it. Then try to make a small NN from scratch train it, test it and this should give you a pretty good idea of backprop.
If you know linear algebra and calculus you should be able to understand the math but coding it would really help you."
MachineLearning,Amazing job!
MachineLearning,Around 7000 employees. My team is 7 growing to 10 in the next few months. The main product we’ve worked on so far is estimated to see $1bn in sales over 2 years.
MachineLearning,Is this one k-means?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Is it possible to specify the iterations to perform (like early stopping) ?
MachineLearning,"Great library, thanks for sharing, just wanted to ask if you tried it with large graphs (more than 10 000 nodes)?!"
MachineLearning,"Yann Lecun did not get run off Twitter.  He just didn’t like being criticized.  Every woman and POC even well below his level gets five times or more the crap he got and somehow they survive.  Whereas one criticism of him was too much for his fragile ego.

And I am not wrong about Mitchell.  Unless you card to link to a source I’m gonna go with what she and every article on the situation has said which is that she used some automation to get communications that may have held evidence of discrimination. Nowhere have I seen her accused of stealing trade secrets or product data.  Only transferring copies of company communications between various employees."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,This is one of the most ignorant opinions I have seen lately
MachineLearning,Isn't Louvain superseded by the the Leiden clustering? Just wondering your rationale for not supporting Leiden...
MachineLearning,"Hey r/MachineLearning! 

A couple of weeks ago I was a bit frustrated because I could not easily try out OpenAI's CLIP model, so I decided to create a simple UI for it myself! 

You can try it out at [clipplayground.co](https://clipplayground.co) . 

Let me know what you think!"
MachineLearning,"Some comment say something like: its morally good to try to expose documents that show wrong doing (e.g. Snowden). I agree if it were the case, but she is not trying to uncover a conspiracy here or she would have already leaked something, she is only trying to favor her political group.

Given the current cultural hegemony Google is just stabbing itself over and over, these premium ""unprivileged"" (apparently you have to feel pity for a leader working at one of the best companies in the world with a high paying salary) will win the cultural battle no matter what because:

* Capitalism: bad
* Historically unprivileged: good
* Free speech: who gives a fuck?

Companies and society have to realize that progressives are not just ""people with good intentions"", they have a clear agenda and will try to bend everyone that doesn't 100% agree with them to their knees, and they are succeeding because people are very late starting to realize that ""positive"" bias can be hacked / abused. 

Bias in a model: root of all evil.
Promote people based on race / gender: solution to all problems in society."
MachineLearning," Topaz Labs Video Enhance AI

\[App\] [https://topazlabs.com/video-enhance-ai/](https://topazlabs.com/video-enhance-ai/)

&amp;#x200B;

iSeeBetter

\[GitHub\] [https://github.com/amanchadha/iSeeBetter](https://github.com/amanchadha/iSeeBetter)

\[Paper\] [https://arxiv.org/abs/2006.11161](https://arxiv.org/abs/2006.11161)

\[Paperswithcode\] [https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling](https://paperswithcode.com/sota/video-super-resolution-on-vid4-4x-upscaling)

&amp;#x200B;

TecoGAN

\[GitHub\] [https://github.com/thunil/TecoGAN](https://github.com/thunil/TecoGAN)

\[Paper\] [https://arxiv.org/pdf/1811.09393.pdf](https://arxiv.org/pdf/1811.09393.pdf)

\[Video from Two Minute Papers\] [https://youtu.be/MwCgvYtOLS0](https://youtu.be/MwCgvYtOLS0)"
MachineLearning,"no, your paper is garbage just like OP's project."
MachineLearning,"Murder of humans is bad because if murder were okay, somebody could murder me (kantian categorical imperative). Killing and eating animals is okay because that doesn't enable somebody to kill and eat me."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Did you realized that you are crying unbaseless?

Even if that was the case, I invite you to continue researching, knowledge should always be updated

Greetings"
MachineLearning,Sorry for my ignorance but is this like a sorting algorithm or is more going on?
MachineLearning,Gg mate
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Title:CD-split and HPD-split: efficient conformal regions in high dimensions  

Authors:[Rafael Izbicki](https://arxiv.org/search/stat?searchtype=author&amp;query=Izbicki%2C+R), [Gilson Shimizu](https://arxiv.org/search/stat?searchtype=author&amp;query=Shimizu%2C+G), [Rafael B. Stern](https://arxiv.org/search/stat?searchtype=author&amp;query=Stern%2C+R+B)  

&gt; Abstract: Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Although the literature has mostly focused on prediction intervals, more general regions can often better represent uncertainty. For instance, a bimodal target is better represented by the union of two intervals. Such prediction regions are obtained by CD-split , which combines the split method and a data-driven partition of the feature space which scales to high dimensions. CD-split however contains many tuning parameters, and their role is not clear. In this paper, we provide new insights on CD-split by exploring its theoretical properties. In particular, we show that CD-split converges asymptotically to the oracle highest predictive density set and satisfies local and asymptotic conditional validity. We also present simulations which show how to tune CD-split. Finally, we introduce HPD-split, a variation of CD-split that requires less tuning, and show that it shares the same theoretical guarantees as CD-split. In a wide variety of our simulations, CD-split and HPD-split have a better conditional coverage and yield smaller prediction regions than other methods.  

[PDF Link](https://arxiv.org/pdf/2007.12778) | [Landing Page](https://arxiv.org/abs/2007.12778) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2007.12778/)"
MachineLearning,"oh, i just saw your other post. so you work on this useless topic as well, makes sense."
MachineLearning,I truly love this
MachineLearning,"Well then M1 would be more than enough! (I'd personally wait for the new release of the 2021 upgrades). When it's time to expand it's also good to learn how to use online services/ backends to train your models.

Good luck with the degree 😄"
MachineLearning,"You may want to try running linux on a virtual machine first to see if it is better for your project: [https://blog.storagecraft.com/the-dead-simple-guide-to-installing-a-linux-virtual-machine-on-windows/](https://blog.storagecraft.com/the-dead-simple-guide-to-installing-a-linux-virtual-machine-on-windows/)

&amp;#x200B;

I've found that virtual machines are lot easier to set up than a proper dual-boot. It'll run slower, but it should be fine for testing the environment out. It's also much easier to clean up if you decide you don't need it!"
MachineLearning,"Use the @ operator in numpy. 

So lets say you have the polynomial features (including the column of ones in the first column) in a matrix X. 

Then you multiply by a column vector B which has the coefficients for intercept, x,x^2 ,etc in that order.

Y = X @ B #can also do np.matmul(X,B) 

You need to end up with Y having dimension n x 1 in the end."
MachineLearning,"Really cool! Would the visualization still look and run well with thousands of nodes? I may try running this on some of my bioinformatics single cell data, which has anywhere from 1k to 10k cells/nodes."
MachineLearning,"Anything other FP32 is already gimped for geforce cards, you have this backwards."
MachineLearning,"This is great, I will install it and give it a try! Thanks for sharing."
MachineLearning,Beautiful
MachineLearning,"Hey r/MachineLearning! 

A couple of weeks ago I was a bit frustrated because I could not easily try out OpenAI's CLIP model, so I decided to create a simple UI for it myself! 

You can try it out at [clipplayground.co](https://clipplayground.co) 

Let me know what you think!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,This is awesome.
MachineLearning,"Is everything all right at home bud?, are you getting enough attention?"
MachineLearning,"no, it's really not. these crap applications make the whole field like dumb.

again, I'm saying that the context is stupid, that's on topic"
MachineLearning,"Stay on context -&gt; common tampering techniques

Better than nothing bud"
MachineLearning,"yeah my point is who gives a fuck if you can detect such stupid basic crap, when for a couple hundred $ you can churn out fakes identical to the real thing."
MachineLearning,"&gt;'t recall it either :-( I think it was discussed on Hacker News, so you may want to look there. Here are some URL I found along the way (but none is the 

I think it is Error Level Analysis (ELA). However, I don't know whether this method is a legit way (it is still debated as far as I know) to know whether an image was tampered or not. The name of the above tampering example is splicing (CMIIW)."
MachineLearning,"I think you are out of context, here we are talking about common tampering techniques, not advanced falsification... Even those can be determined if they are real or not haha

Greetings"
MachineLearning,"In my opinion, this data can be used for cases when you are designing software for noise cancellation in headphones. The headphones can be trained using urban noise data in order to understand how to generate white noise to cancel it."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,you're a moron. you think you can tell whether an ID came off the official printer or one I bought off ebay?
MachineLearning,Github repo: https://github.com/shobrook/communities
MachineLearning,Anyone think it’s silly worrying about power consumption?
MachineLearning,"a late addition (after benchmarking the algorithm myself) the competition does only look at certain suproblems. For example in this case multi-modal functions with nice conditioning. The algorithm presented here is for example bad at solving badly conditioned quadratic problems, because the underlying function model does not really take those into account.

The next thing to consider is that this benchmark is a ranking of all submitted algorithms (+some baselines). If none of the algorithms are good on badly conditioned functions, there would be no way to even figure out that one of the functions is indeed ill-conditioned (because none of the algorithms would really work). In my experience, there are no BO methods that work well on ill-conditioned functions and the BO and ES community (who are good at solving ill-conditioned stuff but bad at multi-modal functions) do not really talk."
MachineLearning,"I think it's unlikely that numpy code written today will still run in 30 years as-is. That's just the nature of things nowadays - dependencies will change, Python will change, etc etc.

Funnily enough, the other day I downloaded some Matlab code from 1997 and ran it in GNU Octave - worked flawlessly."
MachineLearning,Normalize the pixel values and apply benfords law to find discrepancies
MachineLearning,"I think you’d have more luck with a contextual word embedding, like Bert or elmo."
MachineLearning,"&gt; Most people don’t bail on a lucrative position at the first sign of harassment, and most don’t speak about it publicly, either. You have no idea if her claims of racism or sexism are accurate because you didn’t sit next to her every second to observe.

you absolutely do.  look what they did with yann lecun.  they were screeching about how facial transformation algos are racist because they don't work with black people.  yann pointed out that it's because they did all their training with a dataset where 90something % of the people were white.  he mentioned you'd get the same bias if you did it on white people with a mostly black training dataset, or any other cross racial situation.  then he mentioned it's important to include a lot of people from a lot of races in these.  and for showing that their bullshit complaints were already long known about and solved, they dogpiled him, defaming him as racist and sexist for accurately saying this non-white female is just objectively wrong.  they don't want solutions... they just want outrage.

it's why they can't actually identify anything jeff or anyone else did as racist.

and your impression on margaret's situation is just wrong.  she wasn't stealing chat/email messages that'd prove racism/sexism (that shit would be easily discoverable in any lawsuit).  she got locked out of the system by an automated algorithm that flags and locks google employee accounts that appear to be compromised.  it detected her account was being used to take thousands of proprietary dataset files over a very short period of time, transferring those files outside the google network.  her account was automatically locked.  same/next day, she admitted it wasn't compromised and she did it herself.  further investigation found she transmitted those files to at least 4 external sources.  it's just pure data theft.  no racism/sexism involved."
MachineLearning,"Nah, it depends what circles you roam in. Twitter is like a garden, if left untended it will degenerate quickly."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yes it can be determined haha

You should update your SOTA bud, ML ain't magic, but deep learning is pretty close to"
MachineLearning,Yeah I don’t have Instagram but would love thid
MachineLearning,"Happy to help! 

Hope all information you need is on paper

You can visit toc.cl or tocbiometrics.com to check our products and contact website :)"
MachineLearning,how are you performing distillation?
MachineLearning,Awesome! Happy to know that there’s people working in a similar domain ;)
MachineLearning,"You could contact sebastian gonzalez or juan tapia, they are direct authors, I'm (andres valenzuela) just a co-author and I don't know how much info I can reveal :("
MachineLearning,"Great results! Would you be willing to share the ""source"" Google Slides file for one of the papers, e.g., [PaperReadingGroup010](https://www.instagram.com/explore/tags/paperreadinggroup010/), for inspiration? I really like the format and would like to try to make some slides myself, using yours as a template/starting point."
MachineLearning,"This looks promising! From the abstract, it mentioned a use of deep learning method. Can you tell us more about this?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hello there, we already solved this problem on TOC :)

Check [Hybrid Two-Stage Architecture for Tampering Detection of Chipless ID Cards ](https://ieeexplore.ieee.org/document/9197632)

Greetings!"
MachineLearning,"I'm not sure whether it's the best idea to have two posts, just from an engagement POV. While the grids look nice, I'm not sure how to include in one post, I would keep the title as the cover of the post (it's clear and recognisable - would make for a better feed ;) )"
MachineLearning,"I was never arguing about the utility of ML theory, I was simply arguing that it exists, differs from what most people think it is, and is widely unknown (which can be due to its lackluster results, or simply due to the amount of background it requires, which is an issue with theoretical topics).

&amp;#x200B;

I do agree with you that theoretical underdevelopment is not necessarily an issue. And I also agree that the pendulum will not swing back to pure theory, but I disagree with your last statement about holding back progress. Theory of applied topics is almost never about driving progress but more about understanding past progress through a more calm and careful lens."
MachineLearning,"Are there any resources that give gender and/or number for English words.  I'm thinking of a lookup but a corpus with these annotations would work as well, or possibly even a pretrained classifer on GitHub.  This is for a coreference task where these and a few other features are important for a simple classifier."
MachineLearning,"Because not doing so makes you complicit in the murder of a sentient being, a nearly universally agreed upon morally bad action. The time will come when people will look upon the murder of animals as they now look upon the murder of each other."
MachineLearning,"Yes, thanks, on my setup the amp actually runs slower :( The latest pytorch version only comes with cuDNN 8.0.5 which doesnt support my RTX3070 card. I added a configuration option for it anyways."
MachineLearning,99% luck
MachineLearning,"I think that is due to the best theoretical bounds being extremely loose, and usually being very detached from the performance we observe in practise.

And that needn't even be the problem, if it weren't for the philosophy behind ML. We have a methodology where we test our models against unseen data and treat that as the gold standard, *not* theoretical results. In my opinion, it is also the scientific thing to do, because what you ultimately care about is utility. Math has always been the weird one in the sciences, being not based on observation but being purely focused on building a castle in the clouds.

I do not see the pendulum swing back to pure theory, like in the works you mention, as in the bigger picture it seems like it has held back progress rather than strengthen it."
MachineLearning,"There are many different models and loss functions used for ranking ([Tensorflow Ranking](https://github.com/tensorflow/ranking) offers a bunch, probably also available for Jax / Pytorch / etc., or easily convertible).

In my experience the harder part though is actually interpreting correctly your training data, specially if it comes from users -- and is biased in a particular way due to the UI. For that I highly recommend reading through [Thorsten Joacquim's publications](https://www.cs.cornell.edu/people/tj/) \-- since his early works in 2007 studying the biasing of ranking data (user generated).

Good luck and have fun, it's a fascinating subject."
MachineLearning,"I tried the lightweight version but after running a couple minutes the process stopped and I got nothing as output. 

    (animeganv2) PS path\to\animegan2-pytorch&gt; python test.py --input_dir .\pictures_lowrees\
    path\to\.conda\envs\animeganv2\lib\site-packages\torch\cuda\__init__.py:104: UserWarning:
    GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.
    The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.
    If you want to use the GeForce RTX 3080 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/
    
      warnings.warn(incompatible_device_warn.format(device_name, capability, "" "".join(arch_list), device_name))
    model loaded: ./pytorch_generator_Paprika.pt
    (animeganv2) PS path\to\\animegan2-pytorch&gt;

Do you have any idea of what went wrong?"
MachineLearning,"He asked ""why is eating vegan unquestionably the ethical right choice?"""
MachineLearning,"Thank you! This is probably the most helpful advice I was looking for. I have been unsure what a reviewers view on these things of mine would be. 

In that case, I will focus on being as robust as possible in my comparison because a big part of my contribution is indeed efficiency. Thanks!"
MachineLearning,"This is good to know. Indeed, my current aim is to train/benchmark all these models again in my own training environment for a fair comparison. I hope that by doing that and making sure that GPU utilization is at 100% for each of them, I can present a trustable comparison."
MachineLearning,"here's a hint. if something is generated by a digital process (ID card software), even filtered through an analog process (printing and photograph), you can't possibly determine if it's real just by looking at a photograph.

ML AIN'T MAGIC"
MachineLearning,"I'd argue saying ""ML is not pure maths"" is not incorrect but it conveys the wrong message. While it is not pure maths in the classical sense, neither is statistics, as many mathematicians will tell you. However, there is statistical learning theory (e.g. A Probabilistic Theory of Pattern Recognition by Devroye, Gyorfi and Lugosi, or Foundations of Machine Learning by Mohri, Rostamizadeh and Talwalkar, are some examples of lesser known books which do provide theoretical results for machine learning algorithms). And statistical learning theory (which includes a major part of ML theory) has remained sort of neglected when compared to the development of new methods. And that is fine, it is also true for Physics. Practical results tend to emerge prior to theoretical results. Performance first, justification later. Also, I'd highly recomend reading the two books I just referenced as they showcase what ML theory looks like from two different perspectives."
MachineLearning,"I released a [dataset of stories ](https://storium.cs.umass.edu)that are 19K tokens on average, but the longest are over a million. Our human evaluations show that relevance is the biggest factor in whether authors decide to use model generated text in their story, making this a good platform for assessing long document understanding and generation."
MachineLearning,It's not constructive to tell you to stop wasting time?
MachineLearning,Not going to answer this. Only constructive feedbacks are welcomed :)
MachineLearning,"Thank you! Echoing my previous reply: ""I do everything in Google Slides, but I should clarify that most of the cool visuals you see are lifted from the original papers, and I just do my best to organize them into neat little squares."""
MachineLearning,I like a website idea! Esp if there is a way for the ppl who read the papers to discuss ☺️
MachineLearning,"That seems to me as a ""no true scotsman"" argument. ML is no *pure* maths, but it is no qualitative science either."
MachineLearning,"This is a dumb project. You can't possibly detect sophisticated fakes, so what's the point of catching high school counterfeiters."
MachineLearning,"Seems doable. Since the signals are rather long you could try using a CNN on the waterfall plots. Convert them from false color to 0-1 grayscale, put them through a handful of convolutional layers (probably at least some of the layers should have a kernel that spans the entire frequency axis since that seems fixed), eventually take the mean across the time axis, finish off with a couple dense layers and a binary classification output.

Alternately, you could try a transformer (again with waterfall plot input) but you'll likely need to downsample the signal across time to reduce the number of input tokens."
MachineLearning,I probably will just do some toy datasets and maybe if I do more I can use collab or other services like that. I just finished college and plan and going back to do masters in a year or two in the area of AI/data science.
MachineLearning,"This argument circumvents my essential point, which is that eating meat is a form of murder of sentient beings, which we almost universally dream amoral. You must be one of the people who loads their entire dataset into memory before training (just kidding).

You can make reasons to justify murder, we do it all the time, but utopiah asked why it was considered default as amoral, so my explanation stands unless you want to try to refute MY point directly."
MachineLearning,"In past I used [this](https://29a.ch/photo-forensics/) free online tool and [this](https://getghiro.org) opensource offline tool with some success. 

They had bunch of options: jpeg compression difference, clone detection, noise levels, luminance difference, thumbnail differences. 

Maybe something will work for your problem :)"
MachineLearning,"OK well thanks for providing some links. 

From the first one:

&gt;The study, which did not draw any conclusions about causation, 

So they didn't show that being vegan/vegetarian *caused* bad mental health, just that they were correlated somewhat. Meaning it is probably related to either social stigma of being different to the norm, or being more easily upset by ethical issues, or a combination of the two.

From the second one:

&gt;We have made the case for vitamin B3 – nicotinamide – as this is the key vitamin that is missing in cases of over-reliance on a single plant, usually maize, and little or no meat 

So this article is saying: meat provided an abundant source of B3 for ancestors who were over-reliant on a restricted set of plant foods. You can get B3 prefectly well from green vegetables so there is no need to eat meat these days unless for some reason you can only eat a very restricted set of plants. 

From the third:

&gt;But dining with dogs was worth it. Meat is packed with lots of calories and fat. Our brain — which uses about 20 times as much energy as the equivalent amount of muscle — piped up and said, ""Please, sir, I want some more.""

Yep. Calories. That's what I was saying. Now we have an abundance of food to get calories from, we don't need to resort to meat. 

From the fourth:

&gt;Being an herbivore was easy—fruits and vegetables don’t run away, after all. But they’re also not terribly calorie-dense. A better alternative were so-called underground storage organs (USOs)—root foods like beets and yams and potatoes. They pack a bigger nutritional wallop, but they’re not terribly tasty—at least not raw—and they’re very hard to chew. According to Harvard University evolutionary biologists Katherine Zink and Daniel Lieberman, the authors of the Nature paper, proto-humans eating enough root food to stay alive would have had to go through up to 15 million “chewing cycles” a year. 
&gt;This is where meat stepped—and ran and scurried—in to save the day. Prey that has been killed and then prepared either by slicing, pounding or flaking provides a much more calorie-rich meal with much less chewing than root foods do, boosting nutrient levels overall. (Cooking, which would have made things easier still, did not come into vogue until 500,000 years ago.) 

Again. Calories."
MachineLearning,Could you share your papers that you mentioned here? Very interested to read them
MachineLearning,"Yeah I get that. This was my bachelors thesis. I had to develop a system to study how different architectural and material choices impact noise levels. I uesd decibels for that and it worked nicely. 

But I also have all this fft data and I wanted to see if I can do anything with that. My mentor (a data science and ml expert) kindof bailed on me. He provided the idea and material to make 20 sensors and then lost interest and now I have to scrape something together."
MachineLearning,"In photography there is a procedure called culling where you try do deduplicate photos from the shoot by picking the best from each angle / viewpoint. To automate this you have to cluster and rank, the problem can be solved by e.g. doing hierarchical clustering on SimCLR embeddings and then filtering each cluster with a pre-trained aesthetics classifier / ranker in a zero-shot fashion.

The use of embeddings + an algorithm on top (e.g. a linear classifier) is considered a type of meta-learning by it feels like you are just doing regular search over an embedding space. I've never seen the more juicy / crazy adaptive meta-learning techniques (e.g. a NN learns to output the weights of another NN that can solve the new instance of the problem) in action."
MachineLearning,"GAN then probably, trained on a large corpus of real IDs. Then use the trained discriminator network as the fake ID test."
MachineLearning,"In essence, yes"
MachineLearning,So you want to be able to scan an ID and then perform a test if it has been physically modified?
MachineLearning,"This is a very typical situation I've seen when participating in Big Data adoption programmes in various industries: unfortunately they are driven by the people who are fascinated about the opportunity to handle huge data or fiddle about low-level system performance, so that they invest some money (mostly much more than you did) and collect some data and then invite a data scientist to magically do something with that pipe of junk. 

Not **any** data is the new oil, dude.

Successful big data projects I know look usually like that:

1) Find out an unsolved problem that a) **many** people have, b) **many** people want to pay for a solution and c) probably can be solved.

2) Create first version of a causual inference model 

3) Collect the data needed for that model. Only at this step, you start thinking about databases, sensors, protocoll and other CS stuff

4) Use the data to create and evaluate the model.

5) Iterate (go to step 2 if model is promising, or to step 1 if the problem is intractable)"
MachineLearning,"Again, we are not dealing with digital manipulation. We are facing attackers that allegedly and physically modified the image using scissors. The question domain here is verifying the authenticity of the input image."
MachineLearning,"Yeah, it basically boils down to something like this:
     l_channel = np.zeros_like(img)
     l_channel[landmarks] = 1

You have to adapt it to your needs of course (this one for example works on a single channel image."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Seems like you are way over-engineering this. What's wrong with simply storing a SHA-256 hash for each image?
MachineLearning,"Sadly, I can't recall it either :-( I think it was discussed on Hacker News, so you may want to look there. Here are some URL I found along the way (but none is the one I have in mind, though):
http://coding-experiments.blogspot.com/2009/03/detecting-copy-move-forgery-in-images.html  
https://www.fxguide.com/fxfeatured/adobe-photoshop-ai-detector/  
https://peterwang512.github.io/FALdetector/"
MachineLearning,"This is not quite at the level of entire books, but the Long Range Arena goes up to 16K tokens: [https://arxiv.org/pdf/2011.04006.pdf](https://arxiv.org/pdf/2011.04006.pdf)"
MachineLearning,That’s for files that have been modified digitally right.
MachineLearning,Maybe we can use MD5 checksum for this?
MachineLearning,"Thank you for your advice, I will try it out."
MachineLearning,"Tips from Raquel Urtasun about her impostor syndrome:
https://www.rsipvision.com/ComputerVisionNews-2018January/28/
I hope this helps."
MachineLearning,"This page gives off some decent information: [https://itsfoss.com/guide-install-linux-mint-16-dual-boot-windows/](https://itsfoss.com/guide-install-linux-mint-16-dual-boot-windows/)

&amp;#x200B;

Since I am also using windows, what I used to do is to install the other OS on another partition where I am not using windows and then dual boot my machine."
MachineLearning,"Sounds cool, but this data could be useful for local governments. If you can determine what the level of sounds/noise is in what district. You might be able to adjust policy based on noise. Of course one days point in not enough haha. Me thinking 10 years ahead ^^"
MachineLearning,"My bad, I did not notice the skranger package in your list. Nice to know that it exists!

There are a couple of ways to use Shapley values for explanations in R. One way is to use [DALEX](https://github.com/ModelOriented/DALEX), which also contains a lot of other methods besides SHAP. Another one is [iml](https://cran.r-project.org/web/packages/iml/vignettes/intro.html). I am sure there are several other implementations of SHAP as well."
MachineLearning,"https://github.com/vacancy/PreciseRoIPooling

features from the different levels are used to generate proposals"
MachineLearning,"From my personal experience, and also as a current AI master's student. This highly depends on what you do, and what you're looking forward to do.   


1 TB SSD sounds quite enough (for me at least),  also given performance and specs of M1 for getting into ML it would be way more than enough. But also depends on what will you do. 

For example are you just learning ML from scratch and dealing with simple/toy datasets to understand core algorithms or are you going into developing for instance Deep learning models with large datasets and will require heavy processing (which I wouldn't personally do on a laptop, rather use a backend). 

For just getting into ML from scratch, M1 would be more than enough, you'll probably end up using Google Colab to train larger networks or other resources from University."
MachineLearning,"&gt; 

I'm not disagreeing that hunting animals didn't give an edge: I've already explained this, it provided an alternative calorie source, and because it is typically denser in calories than plants, that's what would've made our ancestors go out of their way to get it. But as I've also explained, there's no unique type of nutrient in meat that can't also be found in plants. 

The problem with this argument is it doesn't account for the obvious negatives of eating meat. ""Being more caloried dense"" isn't enough to justify your eyes getting scratched out by something as simple as the rat you're hunting; **when we evolved somewhere with plentiful plants.**

No, i didn't read the study. Because i'm not a scientist and i'm not arguing on the level of a fucking thesis! I'm a guy on the internet giving you my vague arguments. Accept it or go home. The title led me to believe it was about the psychological health, because i've seen articles like this. https://www.psypost.org/2020/05/meat-eaters-tend-to-have-better-psychological-health-than-vegetarians-56698

Why you expect me to be a fucking nutritional expert, i don't know. But since you insist:

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5417583/

https://www.npr.org/2010/08/02/128849908/food-for-thought-meat-based-diet-made-us-smarter

https://time.com/4252373/meat-eating-veganism-evolution/

And no, i haven't read it; but i will respond if you pick it apart."
MachineLearning,"thank you for your response! I have no doubt that few-shot learning in principle can be useful for many industrial applications. to get a better understanding of the current situation and open issues of these approaches, I'd like to see examples of actually deployed few-shot models rather than hypothetical scenarios."
MachineLearning,"that experiment looks indeed very realistic, thank you!"
MachineLearning,"that's very interesting, thank you!"
MachineLearning,"thanks for your response! there are two things I try to understand:

1) regarding meta-learning, I want to understand the properties of task distributions in the real world and how meta-learning can help capturing them or if common approaches still fail. I feel like the task distributions you find in the experimental sections of meta-learning papers are very narrow, e.g. the Omniglot dataset. but perhaps I'm wrong

2) regarding few-shot learning, what are examples of real world situations where new labels/classes arise over time, such that a few-shot approach naturally makes sense?"
MachineLearning,I think looking into WordNet is a good start.
MachineLearning,Cuz anonymous
MachineLearning,Very pleasant format. What software do you use to create the images?
MachineLearning,"So to be clear then, you don't have an answer for what type of nutrient that meat provides that we can't get from plants.

 I'm not disagreeing that hunting animals didn't give an *edge*: I've already explained this, it provided an alternative calorie source, and because it is typically denser in calories than plants, that's what would've made our ancestors go out of their way to get it. But as I've also explained, there's no unique type of nutrient in meat that can't also be found in plants. 

OK, I just read the conclusion of that study you have highlighted, and it seems you have not read it because it does not support your point. It's talking about the psychology behind why people eat meat and why changing dietary habits is hard from a psychological perspective. It gives suggestions as to how to approach changing someone's diet *away from eating meat* taking into account human psychology. 

So to summarise, your entire argument revolves around:
- you believing that there *must* be some reason why you eat meat
- then coming up with various ideas as to support that
- then not substantiating those ideas and not properly engaging when I provide evidence to the contrary

It's okay to admit that you eat meat purely because you like the taste of it and are used to eating it - that's why I used to eat it. You don't have to reverse engineer arguments that provide a deeper reason for eating meat when there aren't any."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Their area of Twitter is so toxic if you disagree with them you’re racist, hate strong women, etc. 

I still remember the time I was told I hate strong women for pointing out posting that you’re looking to hire a woman CTO is sexual discrimination. Honestly if you invert the gender in 99% of what these woke people post you would get called a sexist."
MachineLearning,Cool thanks for the information :)
MachineLearning,Thank you!
MachineLearning,Phenomenal idea. Need to start reading more papers myself.
MachineLearning,"Copying my response from another thread:

""In terms of selection, I started this mostly for myself to get an idea of what lines of interesting work exist, so I hope to cover a wide breadth, guided more by exploration than exploitation. I'm intentionally leaving this open-ended so that I can develop my own tastes. :)""

But to add to the How, I maintain a long queue of papers that I append to daily through input from subreddits like this one, email newsletters (e.g. Import AI, The Batch), and from following researchers on Twitter. When it's time to review a new paper, I look through the queue and select whichever one I feel like doing that week."
MachineLearning,"You won't be doing that alone, I suppose. Especially when you've never created a server. Please make a post when you will need any kind of help in that. It's intuitive tho. Nothing to be scared about."
MachineLearning,"Your opinion seems to stem from thinking that if math is being used, then it's theory. That is not, at all, the case. For example, most ML papers provide very little, if any, theory. Again, papers like Autoencoding Variational Bayes (which contains math errors precisely due to excess engineering) might seem like theory, but they are very far from it. ML theory equates more to information geometry, PAC learning, distributionally robust optimization, etc.

Being math != being theory (example: using probability distributions is bot theory, but specifying sigma algebras and borel measurability is)"
MachineLearning,"Hey, thanks so much! For now, I am just hoping to get some discussions going in the comments, but as others have pointed out, Instagram may not be the best place for this, so I'll update when I think of ways to let people be more engaged. :)"
MachineLearning,"Hey! If you check the IG page, every post is actually double-posted - once in the grid overview format and also in the slide-by-slide view which should not require zooming in. 

Does that cover what you want, or did I misunderstand you?"
MachineLearning,"Shut up. What are you talking about? We live in 2021, and there shouldn't be any consequences of action these days. People should be allowed to decide if they want to face any repercussions. 

Get out of that archaic concept of action -&gt; reaction, decision -&gt; consequences, cause -&gt; effect equations.

^/s"
MachineLearning,"Thanks! I do everything in Google Slides, but I should clarify that most of the cool visuals you see are lifted from the original papers, and I just do my best to organize them into neat little squares."
MachineLearning,"Hmm I would say anyone who has spent about a year working on or reading ML stuff should be able to get something out of it - but of course your familiarity will depend on the particular paper and problem field. The level of detail in the post tries to give an elevator pitch of the paper rather than perfect detail, so that should help in being more accessible to beginners.

In terms of selection, I started this mostly for myself to get an idea of what lines of interesting work exist, so I hope to cover a wide breadth, guided more by exploration than exploitation. I'm intentionally leaving this open-ended so that I can develop my own tastes. :)"
MachineLearning,Which ROI Pooling do you mean? How is a FPN is integrated into Faster-RCNN?
MachineLearning,CenterNet/ CenterMask is an efficient alternative that is much simpler than Mask-RCNN and work pretty well.
MachineLearning,"Interesting.. Similar to your example, optimization (especially RL) can be used for finding process parameters I think. I  need to read more to understand the differences between these diferent methods..
Thanks for sharing."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;double jpeg compression detection

I will look into it. Thanks a lot!"
MachineLearning,"So the field is called ""double jpeg compression detection"". I don't think the content matters but I'm not expert. Hope this helps."
MachineLearning,"Medium is fine, it's just a place for writing articles. It's easy to use and to interface with websites.

As long as the content comes with a code and reproducible results, I don't care if it comes from Medium, Arxiv or a big conference"
MachineLearning,"&gt;:)

:)"
MachineLearning,Would it be able to neglect the differences in the contents and only consider the global properties? Would appreciate if you could recall and revert back with the name :)
MachineLearning,"Idk who is claiming this but the person might know little about the machine learning in the research world. In it, it is almot 90% math, and if we really segregate a 10% engineering portion, it is also done with mathematical methods. The resason ppl think its not math related is bcuz they are closely exposed to deployment part of the pipeline, which obviously gains more attention."
MachineLearning,"I like you, thinking ahead! I'm not a big discord user myself, and I want to be careful not to bite off more than I can chew. Let me think about this!"
MachineLearning,I remember a tool that specifically looks for JPEG artifacts and identifies regions where the quality level between different regions doesn't match. I can't recall the name but might that solve part of your problem?
MachineLearning,"Cool idea! I'm not a big discord user myself, do you think that would be significantly more useful than if I were to just post them here in the subreddit each week?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Sure, I'm open to that! Any suggestions? I'm quite open to any other place where enough people want it.

I'm thinking I could stick them up on a webpage (for viewing past entries), and simultaneously post here in this subreddit whenever I have a new post. Would that work?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Do you put every landmark point as single point in L channel with specific value?
MachineLearning,"Yeah they've really got the Twitter narrative surrounding this incident on lock.  And the press narrative too, it seems--press reads twitter but not this community specific subreddit."
MachineLearning,She's tweeted some toxic stuff: [https://twitter.com/mmitchell\_ai/status/1347279099635458048](https://twitter.com/mmitchell_ai/status/1347279099635458048)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yeah same these look great but I'm not stepping foot on Instagram :/
You could post them here as self posts maybe since that's possible these days"
MachineLearning,"Do i have to be a scientist to just tell you the logic behind my assumptions? Look man, is eating meat fucking easy? Hunting things which can easily tear bits of you off, with a low hit rate? You don't do it for the lols in a bountiful environment unless the one fucker that did got an edge.

https://imgur.com/V9w6Tx6.jpg Are you blind?"
MachineLearning,"To be an individual contributor/writer, getting distribution is hard. There are exceptions clearly, but this is a fat-tailed distribution with a small number of contributors receiving most views/reads/cites (or whatever proxy we use for impact). By some game theoretical logic therefore, brands evolve as signals. 

Is a paper from Stanford or Deepmind necessarily better than Louisiana State or Reddit Randos Ramblings &amp; Reflections? Not necessarily, but probably. By creating a barrier to entry, and nurturing their public brand, the former are in themselves signals of impact, while the latter are less so, not because God ruled it thus, but because short-hand signals are fit in the social ecosystem. The top level nodes in the Decision Tree of Life have been trained on these signals of social proof (actual ethicists assemble!).

I therefore think you’re correct in one sense, but wrong in another. Given that a contributor has successfully competed and attained The Signal, impact is increased by just putting more stuff out there for everyone. We love these people. But if you’re an occasional contributor, then putting up your intensely researched blog post about a cool application of X on your run-of-the-mill GitHub blog or kitschy 90s Internet style blog, post about it on Twitter/Reddit and see the ten-ish people even click in to read it... well, it’s a touch dispiriting. And no, Google isnt good enough to put match a search query to your high quality stuff. That’s where renting The Signal that someone else has built becomes useful. Towards Data Science has built a brand of sorts that in effect allows lower volume contributors to get readers. It’s superficial and marketing, but a world of all content and merit, no surface and fluff, just hasn’t been invented yet &lt;insert utopian manifesto&gt;.

Now I agree that the paywall stuff on medium is a bit shitty (especially when some articles are outside the paywall but you don’t know until you click in), or that TDS is accepting more low quality stuff, thus diluting their brand. I wish there was a good solid reliable place to put stuff as the occasional contributor that has a decent shot at attaining impact. Suggestions are welcome."
MachineLearning,"The results on Google searches are driving the popularity. You can also find many explanations that would otherwise require actually reading papers.
In my experience, medium is more misleading and full of false statements than other available sources.
If it keeps going, it looks like the engineer of the future will be a medium student."
MachineLearning,"Papers, yes. But at times you don't want to read a whole paper. Instead you just want some simple explanation about a concept or some tech issue. You Google/Bing/Duck and half of the results are on Medium which is disappointing."
MachineLearning,"&gt; Care to elaborate on your last sentence? Sounds interesting.

Okay, so you're talking about homophobia, misogyny, human rights violations as things that are obviously intrinsically wrong and must therefore be overcome. That is true, they are, *to you*. If you went to Egypt, it would be equally obvious and widely understood that is intrinsically good to stop young women from wearing revealing clothes. Neither worldview is *better*, even though you instinctively prefer one over the other, they are both internally consistent, you can only say that one is stronger/more robust/growing faster.

The most basic associations, context, historical traumas are different. 

You're talking about the internet and media dominating discourse and erasing differences, and that is absolutely correct. I am living in a small third world country and probably have more in common with you than with my neighbours. So that is the first group, everyone refers to it as ""the West"", there is the Arab/Muslim world, and there is China. The latter two have sufficiently strong barriers to endure the constant propaganda from every screen.

Anyway, that is just my worldview. What you said here, I completely agree:

&gt;just like nuclear weapons, randomly brandishing it at things I don't like can result in horrifying consequences."
MachineLearning,"I wrote a summary for this paper: [https://wandb.ai/ayush-thakur/taming-transformer/reports/-Overview-Taming-Transformers-for-High-Resolution-Image-Synthesis---Vmlldzo0NjEyMTY](https://wandb.ai/ayush-thakur/taming-transformer/reports/-Overview-Taming-Transformers-for-High-Resolution-Image-Synthesis---Vmlldzo0NjEyMTY)

&amp;#x200B;

Hope it will be useful."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Without a test set you cannot know that your models have good performance.

You should have set a fraction of the training images for use as a test set and not have trained on them."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"This is an inside joke in the Polandball community. Most flags are drawn normally except for Poland, where ""everything is upside-down""."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"1. This https://www.auai.org/uai2021/ 

2. I just came to know the overlords trash COLT."
MachineLearning,"I want a single fit curve.  I can not send you a photo of what i want in reddit(because of the limitations reddit have ) but i think you know what I mean of a single fit curve. 
So how can I multiply the polynomial matrix by the coefficients of the polynomial and get a 1D array? 
Can you help me with this ?"
MachineLearning,"Precise RoIPooling, SOTA backbones + FPN or alternatives, decoupling classification and localization, SOTA optimizers"
MachineLearning,Use bibliogram!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"What are you trying to do? The PolynomialFeatures() creates a design matrix of polynomial features so your y here is a matrix which doesn’t seem like what you are trying to do. Y is typically denoting the response vector not a feature matrix

I am guessing you are trying to generate some polynomial but you would have to multiply the polynomial matrix by the coefficients of the polynomial and get a 1D y."
MachineLearning,You share them and face consequences.
MachineLearning,Everything worth reading is or will be on arxiv anyway. If it's not on arxiv one can live without it.
MachineLearning,Love this
MachineLearning,Thank you so much for taking your time to answer.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,If she does she would leave all those documents untouched. If she considered becoming a whistleblower then it would be the most pragmatic way/
MachineLearning,But that would be leaving whatever she considers incriminating materials untouched.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,What is your first choice?
MachineLearning,"I am using a windows system on my laptop, but I think it will be more convenient to use linux system for my project, is there a way to run dual system on a laptop?"
MachineLearning,"Mac will not be my first choice when getting  a laptop for ml, but 1tb of ssd should be enough."
MachineLearning,If you don't mind could you explain how do you select those papers?
MachineLearning,"I'm kinda curious, can you set up a deep environment in a sandbox?
My nvidia drivers got seriously messed up last time it  updated, can't even see the gpu showing up in task manager, it took me a lot of work to get it working again, I reinstalled every single Nvidia related program. 
I would like to avoid this from happening again, do you guys think sandbox will work?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Hmmm noice :D
MachineLearning,"My only question is why the red is on top? This is an Indonesiaball, Poland has red on the bottom"
MachineLearning,"Totally doable, it just takes effort.  Lots of doable things simply haven't been done."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Oh and in terms of metrics. Other than what I said regarding topics, acceptance rate is one for sure. Keep in mind that conferences with deadlines near the new years or slightly after tend to have lower acceptance rates as a lot of people are doing exactly as I am suggesting. Don't let that shy you away from it if you believe in your work."
MachineLearning,"Well your list in the post did include some to consider, and there are others like aaai, ijcai, cdc even. None of these are specifically mid tier, but I guess they don't get as much hype as NIPs. Also different conferences have different audiences and specializations (control theory vs ai for eg.) Which can hurt or help your paper. 

The real benefit is you don't have to get your paper rejected when you choose to withdraw and reapply for a later conference. As long as your work is still relevant and has not been done before, you might as well polish it if your reviews are borderline. This of course is a tricky decision, and is hence a lot easier when your advisor can help you with insight like this. Hope I was able to pass some of that along, I myself am new to this as well. Best of luck!"
MachineLearning,"The issue I'm having is that it rarely matches the sklearn or ranger performance and requires much more hyperparam searching. This isnt a huge issue.

Perhaps i can use both and switch depending on the number of categoricals i have."
MachineLearning,"These CMP cards are garbage. By crippling the 3060s and coming out with  these weak-sauce CMPs it looks like NVIDIA is just trying to force  people to buy subpar products at a hefty markup.

The  whole introduction of old 1050 TI's and such also stinks of rotten fish  to me. Only this time the folks being screwed are Gamers.

What  bothers me about all this that is going down is how everyone is  pointing fingers at each other: Gamers are blaming Miners, Miners are  mostly sad they can't get the cards either and are reacting to Gamers  misdirected fury... and now even ML folks are badmouthing Miners.

The  fact of the matter is that producers are to blame here -- specifically  NVIDIA. They come out with a product line that is woefully  underproduced, then they conveniently blame a tiny subsect of people  buying the item. And then, showing their greedy corporatism they come up  with schemes to resurrect and sell cheap outdated tech at a ridiculous  markup to Gamers and to simultaneously sell stripped down (aka cheaper)  newer tech to ALL Miners.

Look, in  the end this isn't about miners. If you take away anything from what I  am saying please understand this: the vast majority of people interested  in mining are also starving for these cards. Only a super small portion  of miners got some 30 series cards."
MachineLearning,"Yes, these CMP cards are garbage. By crippling the 3060s and coming out with these weak-sauce CMPs it looks like NVIDIA is just trying to force people to buy subpar products at a hefty markup.  


The whole introduction of old 1050 TI's and such also stinks of rotten fish to me. Only this time the folks being screwed are Gamers.  


What bothers me about all this that is going down is how everyone is pointing fingers at each other: Gamers are blaming Miners, Miners are mostly sad they can't get the cards either and are reacting to Gamers misdirected fury... and now even ML folks are badmouthing Miners.  


The fact of the matter is that producers are to blame here -- specifically NVIDIA. They come out with a product line that is woefully underproduced, then they conveniently blame a tiny subsect of people buying the item. And then, showing their greedy corporatism they come up with schemes to resurrect and sell cheap outdated tech at a ridiculous markup to Gamers and to simultaneously sell stripped down (aka cheaper) newer tech to ALL Miners.   


Look, in the end this isn't about miners. If you take away anything from what I am saying please understand this: the vast majority of people interested in mining are also starving for these cards. Only a super small portion of miners got some 30 series cards."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,hey looks cool! what do u use to produce the visuals for the ig posts?
MachineLearning,"But with freewires you could e.g. insert a 1000-length chain from the start node to end node, which is comparatively hard to do with traditional layers"
MachineLearning,Ah wow how have I not heard of these before?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,https://www.osa-opn.org/home/newsroom/2021/february/shaping_light_pulses_with_deep_learning/
MachineLearning,It’s not “bitcoin people” you can’t even mine bitcoin with these gpus. What I was asking is why would people mining cryptocurrency buy that equipment if existing equipment is already better? That would not cause them to “clamp down” on the mining demand for the gpus allowing others to buy for normal use.
MachineLearning,And if there’s evidence of bigger ethical problems that you believe the public  or another injured party has a moral right to see? And you feel ethically obligated to share them?
MachineLearning,Will transformers replace Cnets?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"After accounting for noise in the review prices, these conferences are all pretty much equal. If anyone says otherwise to you, they are spewing bullshit because the organizers, senior programming committee, and reviewers are all nearly the same (for example I review for *all* of these every year and my standards don't change across conferences and neither do the other reviewers in my experience).

ICML, NeurIPS, ICLR, AAAI, UAI, AISTATS, AAMAS

The only major difference between them is the number of submissions (NeurIPS ~10k, ICML ~5k, ICLR ~2k from a quick Google search). A publication to any of these will look great on a CV and no one worth a damn will pick a different candidate just because they have a NeurIPS submission and you have a AAAI. Also remember, more competitive does not equate to better. Just do a kick-ass job on your paper, listen to your reviewers where appropriate, and treat your top-tier work to a top-tier conference.

Also COLT is in a (higher) league of its own, whoever told you it was mid-tier.....must not be a theory person :)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Don't submit to mid-tier. They are instantly sniffed out on CVs and if your work is good enough for top-tier then stay top-tier.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Loved it. Great UI too.
MachineLearning,"The models that train on 100s of GB of data wouldn’t train on a MacBook anyways. It would take months.

Buy cheaper MacBook and use Collab or your college resources for training."
MachineLearning,+2
MachineLearning,Have same questions
MachineLearning,"Thanks for your nice and detailed comments. This is turning out to be a really healthy conversation about the topic.

It is not about drawing the line to use something or do not use something. Different scenarios work for different projects. Every project has different set of wisdom and limitations. And ""many or most problems"" is not ""every"" problem out there. And the thing which you talked about the cost, is certainly a topic which is also been emphasized in the post specifically. Certainly the ""Return on Investment"" is always a key metric. No matter if a project is ML based or not. The point of cost made in the post is strongly about ""training cost"" of a model, which may significantly increase the additional cost of project.

Overall you have mentioned good points which are universally true for the projects. My focus through the post is mostly based on individual aspects related with Machine Learning. Cheers..."
MachineLearning,UAI is a very respected conference in the Bayesian machine learning community
MachineLearning,I also think it needs to be said that not every paper should be published. How much work you put into it doesn't matter if it's not quality.
MachineLearning,"Well in that case, the rationale is that if Nvidia doesn't stop the Bitcoin people from hoarding all the GPUs then Nvidia loses customers when people decide to buy GPUs elsewhere. 

I also suspect that Nvidia is concerned about attention from regulators. Normally the government wouldn't care at all about scalping, delays, or overly optimistic advertising about availability, but in the case of the RTX 30 series launch it's been so absurdly bad that Nvidia could start finding themselves in hot water if they don't start doing something to clamp down on this quickly."
MachineLearning,"You’re making a lot of assumptions here.  I didn’t say Timnit specifically. I answered your question in general.  In my case, they also tried to fire my partner who works at the same company.  They almost lost their housing, and they had no support to fall back on or savings/etc because their family is horrible.  Like kill yourself rather than live with horrible.  So fuck off."
MachineLearning,"Then she should have quit, no?"
MachineLearning,"Trump’s initiative was just 4 show, Biden is backed with juices."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Google did NOT handle this well
MachineLearning,AMD it is.
MachineLearning,r/coolguides
MachineLearning,We haven't had too much success with meta-learning yet but we do use few shot and zero shot learning a lot. Here's a recent example of zero shot learning for spell correction: https://www.microsoft.com/en-us/research/blog/speller100-zero-shot-spelling-correction-at-scale-for-100-plus-languages/
MachineLearning,Thank you so much! This is exactly what I was looking for
MachineLearning,"There's some cool work on ""corralling"" bandit algorithms, see [https://arxiv.org/abs/1612.06246](https://arxiv.org/abs/1612.06246) and the later papers which cite this."
MachineLearning,"OOTL, first time I hear about this. Why in the world would they do that? Why would they care what their GPUs are used for?"
MachineLearning,I second that. Would be really nice if we had the community as discord server.
MachineLearning,"Hello.

First, don't give up! 

* I have two papers rejected from SIGKDD that became best paper winners in ICDM. 
* I had a paper rejected 6 times, but now has 800 citations 

The review process has a lot of noise.

Spend a day addressing the reviewers comments, and resubmit. Good papers will find a home."
MachineLearning,"You are being downvoted because your ""heroes"" are no references ...

By any fair standard, they are failed academics who love to shit on science for no other seeming reason than spite."
MachineLearning,"&gt;read. Encourage oth

Transformers were proposed for seq2seq tasks mainly, although variants like GPT and BERT have utilized similar structures in transformers for pretraining on tasks that are generally seen as ""generic"" or ""applicable to all text"" tasks. Since your question is about fine-tuning I am assuming you are talking about these pre-trained models.

You are right to some extent. Mostly it is about fine-tuning the models. Basically, the models (BERT and friends) have been pre-trained to generate fluent, contextual, and somewhat coherent sentences (or words) on a huge corpus. Fine-tuning gives the model the ability to look at the tasks standing on the shoulder of giants.  But in some cases that is not too direct. You might have to formulate the objective well. Some examples are stated below:

1. BERT models have been used for building evaluation models - see BERTScore and BLEURT.  
2. Another interesting application is for infilling tasks where the number of words to be filled is not specified upfront (see Infilling Language Model). 
3. Tasks in Dialogue Generation and State tracking \[See SimpleTOD\]
4. Improving decoding in LM - Nucleus sampling, 
5. Training objective: Sparse Text Generation.

Coming to your final question. It is difficult and an open-problem on its own to define a good evaluation metric in generative models about the ""goodness"" of your generation. Evaluation is a very task-specific thing. There are no good work-for-all evaluation metrics in generative models (as opposed to classification models). If you want to build let's say a paraphrasing model, the onus is on you to define what ""good"" samples mean. In most cases, if you are able to justify that to yourself how you measure different samples for ""goodness"", then use those intuitions to look for evaluation metrics. If you want to show fluency - perplexity and its variants might help. If you are looking for diversity perhaps distinct n-gram scores, REP, and WREP are your friends. In most cases, it really helps to formalize the problem statement better than saying something vague like our model generates good paraphrases. That being said, this is a good survey paper on evaluation metric for generative models.: [https://arxiv.org/pdf/2006.14799.pdf](https://arxiv.org/pdf/2006.14799.pdf)"
MachineLearning,"Maybe.

See of all the chips rolling off the line some of them will have defects in areas that don't relate to mining, but do relate to working as a GPU. Right now these chips get dumped into a pile for recycling. With this strategy those can have the GPU exclusive parts disabled and they can sell them on mining cards."
MachineLearning,In theory you could make it differentiate from one another by making it analyze the comic itself and checking for words like poland monaco indonesia. Altough this could be very hard and take a lot of conputing power so maybe a way would be to make it run a text analyzing script if its unsure about some balls while at the same time this could be a lotbof work for something that categorizes balls that look very similair differently
MachineLearning,There's no reason not to submit to neurips again if you think the work is solid and you addresses valid concerns of reviewers.
MachineLearning,Quitting would be more ethical
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"YoloV4, Yolov5, and RetinaNet may interest you. PyTorch models already has RetinaNet support."
MachineLearning,Looks cool! I look forward to trying it out!
MachineLearning,Discord is great for this kind of thing. You could easily explore and store the material that has been discussed. Excellent for creating a community. It can easily switch to semi-conferences/webinars/presentations on voice channels with streaming helping out. Bump!!!!
MachineLearning,"Few-shot via embeddings has tons of industrial applications e.g. search, simple classifiers, ect. Even zero-shot with e.g. CLIP seems pretty useful.

Beyond that, I've never heard of industrial uses of the more esoteric parts of meta-learning."
MachineLearning,:o
MachineLearning,"What the fuck is UAI, and who the fuck is trashing COLT?"
MachineLearning,"I'm almost sure there is no way to speed up data loading. Read / write operations are always the bottleneck in clusters.

The only alternative is to pay for google cloud or any other cloud provider.

Tip (but you probably already done it): make sure you are not doing read / write operations during training and load everything in the RAM."
MachineLearning,"What the fuck is UAI, and who the fuck is trashing COLT?"
MachineLearning,Thats not quite what I meant. But thanks anyway. I was essentially trying to find a multi-armed bandit algorithm based multiple multi-armed bandits. Rather than the conventional sense of 'meta learning' in machine learning
MachineLearning,"I generally like the idea behind the post but mostly disagree with where you draw the line.  In truth, I think many, maybe even most,  problems _can_ be improved with ML.

I would instead lean more on the costs: complexity, LOE, whether you're going 0 to 1 or improving an existing product.  ML pipelines and online predictions add significant complexity to an application.  This in turn increases development costs and devops costs.  If you don't have the people to maintain the systems, it's almost never worth it's usage.

For new products, time to market and product market fit matters are the most important constraints.  If you can deliver faster and test the product thesis quicker, it's absolutely the right choice: most products fail so reduction in time costs is almost always preferable.

Real-time model usage is complicated: problems such as out of domain inputs and concept drift can cause lots of issues if the owners are lax in monitoring.  To what degree the product is tolerant to errors is something else to consider when deciding on how to construct business logic.

Overall, heuristics are simply swapping machine biases for human priors, which also have plenty of bias :)"
MachineLearning,"Definitely an important consideration! From their website, their previous students end up in quite a lot of good places, including FAANG research roles and professors at a number of good universities (including a few of the really top-tier ones—in fact, I first came across this person when I realized that a lot of my prospective PIs at other schools came out of their lab). I also talked with a few of their current students and they had lots of positive things to say. I'm not sure about how many students have dropped out, but those two things at least give me confidence."
MachineLearning,"I'm so glad to hear that this was your experience! Honestly, with maybe one or two exceptions, I've never liked any of the professors I've talked to from the very top-tier places. Almost all of them have come off as elitist snobs whose attitude is just ""how lucky YOU would be, peasant, if I were to give you the privilege of working with me."" Yuck. I've talked with two professors at this prospective school, and their vibes couldn't be more different—super nice, genuinely interested in my work, always happy to talk or answer questions over email, plus their current students have a lot of positive things to say about them as advisors. Not to mention, they've made a strong push to recruit me, and frankly, it's nice to feel wanted! I feel like working under this kind of advisor would surely be better from a research output perspective than a PI whose only goal is to nitpick everything you do and make your life miserable."
MachineLearning,"Not sure if quite what you mean, but there's meta RL, and a multi-armed bandit is a special case of an MDP: [https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html](https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html)"
MachineLearning,"&gt;‘Cause you need a job or you starve.

Seems like a tale of victimization without any foundation or a modicum of truth whatsoever. As someone pointed out in this thread Timnit and co. were getting paid at least half a million dollar a year and they couldn't leave the job because they would be starving. Quite a fairy tale you have concocted. And if they were so worried about starving, why threaten to quit? Why blatantly violate company policy by doing something that potentially can get you fired? Why drag your manager through mud in social media if you are so afriad of losing job and then starving? This seems like a reality TV show script I am reading.

I come from a very poor country. I have seen **children** (I repeat children) doing very demeaning job day after day so they or their family don't starve to death instead of going to school. Please don't use other people's suffering as a tool in your cococted soap opera. And please don't use the word ""starve"" unless you have actually witnessed people starve. Have some basic decency people."
MachineLearning,+1
MachineLearning,"No, it would be incredibly stupid of Nvidia to do anything to their GPUs that interferes with AI.   They didn’t get the market cap they have because gaming.   And crypto mining is simply a nice bonus in their revenue stream.  AI and selling $10,000 GPUs by the truckload to companies developing AI applications is their bread and butter.

&amp;#x200B;

I have no idea what the hardware or software differences in their crypto cards are, but they aren’t going to kill the goose that laid the golden egg.

&amp;#x200B;

Intriduction of a crypto card is an attempt to allow non-crypto users the ability to purchase their product."
MachineLearning,"Try its successor Mask RCNN

https://github.com/matterport/Mask_RCNN

https://arxiv.org/abs/1703.06870"
MachineLearning,"Not trying to self-promote my reddit thread, but yeah I think this is a more pressing topic rather than the HR drama XP would love to hear your thoughts  https://www.reddit.com/r/MachineLearning/comments/lnto47/d_ai_ethics_and_voices_at_the_table"
MachineLearning,Hey great initiative! May I suggest to have each post converted into like a collage format so that people can swipe around to view what you’ve posted instead of zooming in and out? Provides better viewing experience. The current posts are a bit small to see. Thanks!
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Consider some voices are already in the dataset.
Clearly you cannot add a license to those specific voices (although I am not soure what legal restrictions could even apply).

(Of course you don't have to use PD/CC0, that was just my suggestion, but specify some license)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,These resources are really helpful - thank you.
MachineLearning,"Lol it’s awful to who, you? You make pretty definitive statements about things that you don’t seem to understand. It’s profitable to mine on far more than the RTX 30 series or gpus for that matter. Back to ASICS? Do you have any idea of the profitability of ASICS long term based on the few companies who produce them with months of sold out inventory every time the price rises? Wasn’t asking for your opinion, was trying to determine a companies rationale for creating a product with seemingly no value to its customers."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"OK so if I understand correctly, you have a theory that all our ancestors had enough edible plants around all the time that they didn't ever need an alternative calorie source to survive, but when they did try eating animals, that's when they started evolving beyond apes, because they suddenly had an extra type of nutrient that they didn't have before. Right? Well which nutrient was it then?

Oh sure, I can find you studies that show meat contains nutrient x. What I can't find is studies that show: getting nutrient x from meat means you have x physical improvement over getting nutrient x from plants. Can you find me one of those?"
MachineLearning,"They're not full GPUs, they're what you'd get if you took a GPU and stripped out everything unrelated to mining."
MachineLearning,"Yeah, but using GPUs for mining is awful in general. It's currently profitable to mine on GPUs again because no one expected the RTX 30 series to be so powerful and because there's been a surge in the price of BTC, but this is very temporary and it will be back to ASICs soon enough."
MachineLearning,"The difference is classic Twitter/Reddit dynamics.  On Twitter, you read what people want to be seen as thinking.  On Reddit, you read what they actually think."
MachineLearning,same!! love if this could be a discord or something
MachineLearning,"One thing that I still don't understand to this day is why googled staffed it's ethics team exclusively with intersectional feminists. Isn't that quite narrow? Why did they not hire philosophers of science and/or ethics, similar in spirit to lets say Nick Bostrom? Ethics is a vast super interesting field"
MachineLearning,"might also of interest to read this (my own paper) http://proceedings.mlr.press/v124/tabibian20a.html

we used a basic Placket-Luce model."
MachineLearning,"I don't think it is very widely used in deep learning, if you have enough data to put a good test set aside and don't touch it until the end, then that might be more practical."
MachineLearning,Great point. I'll add a point to the page that we plan to use CC0.
MachineLearning,Thanks for the input. So I took a look at nested cross-validation. It looks like I'd need to basically do 25 runs of training/testing. That seems to be...impractical(?)... for a network that takes 45 min or so to train on one iteration right? Is this actually used for CNNs with many layers?
MachineLearning,Read the other posts about jobs in this subreddit. There is a big “FAANG or bust” attitude
MachineLearning,"Care to elaborate on your last sentence? Sounds interesting.

And yes, re: cultural norms such as homophobia in modern Christianity, misogyny in modern Islam, human rights violations in China/Burma/Russia/[insert authoritarian country]... 
That is kind of what I wanted to get to with my final point about transitional justice.

I personally am very torn about this, because while I think ML could be a tool for positive social change, just like nuclear weapons, randomly brandishing it at things I don't like can result in horrifying consequences."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Can you be more specific about the hyperparamater issue you have been having? I've found that setting a max depth can make it behave more like standard cart style tree growth with random forest like robustness.

Also using one hot encoding and sparse matrixes works really well and  doesn't have much of a memory penalty for categorical features."
MachineLearning,"I think your coworker is right here. In order to check if you might be overfitting, you should focus on the test data used for the reported metrics. If any of that data has been used to optimize any part of your model, e.g. learning rate, it can be the case that you are overfitting. In your case, some folds will overlap with the test set of your initial hyperparameter search.

If you need to optimize hyperparameters and also want very accurate test metrics (i.e. like cross-validation), I think you should use nested cross-validation. If you are confident that your test set reflects reality good enough, the approach that your coworker suggests is the right way to do it I believe."
MachineLearning,"To be honest, in my experience most people not only downsample to 224x224 for the spatial resolution, but also usually do something like use only 1 in 5 frames at 30fps for the temporal resolution for the old ConvNet + LSTM models, or at most 25fps for something relatively newer like I3D.  As such, for the practical reasons, I'd lean towards option c, though it may depend on how much longevity you want this dataset to have.  Keep in mind it's also easier to downsample the data later if you need to, so going with a or b isn't necessarily wrong either."
MachineLearning,"... yeah, but my point is it's not like Geforce cards have intentionally gimped performance. They do in the sense of Tensor cores, but using them is not a necessity for DL. The difference in these is that the crypto approach screws hobbyists, while access control doesn't."
MachineLearning,"If you considering GBDT check out [catboost](https://github.com/catboost/catboost), unfortunately RF mode is not available but library implement lots of interesting categorical encoding tricks that boost accuracy."
MachineLearning,"&gt; reduces parameter counts by 200%

Wat"
MachineLearning,Seems most in here think rule following is ethics.
MachineLearning,"Gotcha. I'd be surprised if PyTorch's [automatic mixed precision](https://pytorch.org/docs/stable/amp.html) didn't just work out of the box. The problem with full FP16 is that it can often lead to instability. AMP, as the name implies, does a mix of FP16 and FP32 based on the module. For example, both BatchNorm and GELU default to FP32 in AMP. If you have one of the newer NVIDIA GPUs (e.g., RTX 2000 series) you should get a speed up by using AMP.

AFIAK, PyTorch 1.7+ will use TF32 when possible on cards that support it. It also looks like bfloat16 is built in, but I'm not sure what devices are supported at the moment."
MachineLearning,"Questions:

* What level of reader will the posts be for (Undergrad, Masters, Phd)
* What type of papers will be covered (Trending, Last year's Seminal, Click bait, test-of-time seminal ?)

Followed"
MachineLearning,"There's TensorLy which can by used with pytorch and supports many tensor operations and decompositions including cp decomposition.

Here's the link if you want to check it out: [http://tensorly.org/torch/dev/](http://tensorly.org/torch/dev/)"
MachineLearning,It's a great idea. How can we contribute? I do 5 minute presentations for my lab time to time.
MachineLearning,"You don't mention licensing.

You should ask users to dedicate samples to the public domain and you do the same with the dataset.

Like the Mozilla voices dataset is CC0."
MachineLearning,"I don't visit instagram that often, so I support an alternative like this."
MachineLearning,"There's no shortage of herbivores. They don't exactly pick the jungle clean either. If plants got you everything,  there's minimal incentive to be a carnivore given the risk in the environment we evolved in.

As for the studies... for fucks sake. https://lmgtfy.app/?q=why+eating+meat+is+healthy+scholarly+articles

Look at the psychological study. Yeah, you tried so hard."
MachineLearning,You mean like the linzhi phoenix miner for eth?
MachineLearning,Any chance you could mirror it somewhere other than Instagram/social networks?
MachineLearning,"Bitcoin is built to kind of fill out whatever space people are willing to make for bitcoin.  If it's the wild west, then everyone will make as much hashes as they can to have the chance of being the best miner.  If the hypothetical emperor states that only a certain number of hashes may be used for bitcoin, then suddenly a calculator can farm bitcoin.  It all depends on the restrictions the bitcoin miner has placed upon him."
MachineLearning,You made a throwaway just to say this?
MachineLearning,"&gt;If you believe there to be institutional bias in favor of eating meat to the point of eating it for no particular reason, there are obviously studies in favor of it. You will also obviously discredit them

I used to eat meat because I liked the taste of it and I was brought up eating it. Which is like a lot of people, including athletes. If I didn't care about any of the ethical/sustainability issues, and I was an athlete who didn't have a strong performance-related reason to *stop* eating meat, then I would continue eating meat. 

In addition, you keep re-iterating that you believe there HAS to be evidence that eating meat provides some benefit, but you have not produced a single one of these studies that you believe must exist. I can not find one from searching the web. If you can then please let me know.

&gt;that omnivores went to the effort of eating meat for no reason in a jungle full of plants; good fucking luck to you.

There is a clear reason why our ancestors started hunting animals. Because for many of them, it was an extra calorie source that they needed. Equally, if at any point they couldn't hunt an animal but they found enough plants to live off for a while, they could live fine like that too. Being able to live off different calorie sources is the advantage that hunting animals gave. There isn't a magic intelligence nutrient in meat."
MachineLearning,"I am not really sure yet what exact changes need to be made as I’m not yet familiar with the pytorch typing internals. The thing is that the authors actually utilize bfloat16 (not float16), so I cannot use pytorch’s half() function (or can I?). Moreover, the authors state that they keep their weights in full precision, even though the code says weights.dtype = inputs.dtype and inputs.dtype=bfloat16. Maybe it is something JAX specific.

Then there is a little hardware chaos as well, because bfloat16 and TF32 types are only supported from the Ampere architecture with CUDA11 and onwards (and TPUv2+). With an ampere card installed, pytorch will automatically use TF32 ops, which are the larger brother of bfloat16. This again makes it hard to write general code, applicable for everybody. 

In the end it would probably require some extensive testing on expensive hardware...
If you know more on this topic I’d appreciate your help :D"
MachineLearning,"&gt; With how open/accessible information is on the internet nowadays, unless you're China and you build a firewall, there's not really a notion of country/culture borders.

A very significant (and influential) fraction of the world believes that women should be covered in public and that alcohol should be illegal.

Imho, you are correct to some degree, but there are at least three ideological poles with major, significant differences in cultural utilities."
MachineLearning,"It's an utterly ridiculous amount of energy. I'm not opposed to decentralised currency (although I struggle to see what it's achieving for the vast majority of people, bitcoin has become a novelty investment more than anything), but the implementation of bitcoin is terrible."
MachineLearning,"&gt;waste

Nope, just as one example Twitter is a far worse invention. All bitcoin does is waste some energy. You can argue how well it accomplishes the task but decentralizing the financial system is a worthy goal."
MachineLearning,"If you believe there to be institutional bias in favor of eating meat to the point of eating it for no particular reason, there are obviously studies in favor of it. You will also obviously discredit them. If you really believe such different foods can truly be *meaningless* in  their differences; that omnivores went to the effort of eating meat for no reason in a jungle full of plants; good fucking luck to you."
MachineLearning,I hope nobody is being too quick to judge Google's actions.
MachineLearning,"So maybe I wasn't clear, but nitrates are found in abundance in only a small number of plants (beets are the most common one consumed by athletes). So someone on a plant based diet could be eating a diet that isn't hugely different in nitrate levels than a meat eater, if they aren't eating many of the nitrate rich plants. 

As long the diet has adequate levels of unprocessed foods like veg/fruit/whole grains and it is hitting the micro and macro nutrient targets, whether you have some lean meat in there or not doesn't really make a difference. Drinking beet juice before your marathon would give you an improvement of something like 5% vs not drinking beet juice before your marathon though. That's what I'm trying to say."
MachineLearning,"You are a gentleman and a scholar.

Thank you so much. This is the first I've seen someone describe what maxout-2 might be. 

It's gonna take me some time to read up on this and actually try to implement a version of it. 

I really appreciate it."
MachineLearning,I really don’t understand why people are so positive about this online. It’s not gonna increase the chip output in the fabs. It only reduces usefulness of affordable consumer grade cards. Think this step is a disadvantage for consumers and NVIDIA is very lucky to get away with it
MachineLearning,"I'm reading ""FAANG worship"" as ""not freaking out about FAANG being evil *quite* 100% of the time""."
MachineLearning,"The Curious AI Company did something like this, but precisely what they did is uncertain.

They're website is not online though, so it's probably that they've shut down."
MachineLearning,"What happens in the module, generally stays in the module, so you are probably able to do any *parallel constellation in there. Hyper-parameter sweeps can be parallelized using dask (essentially a model copy per compute unit), see [here](https://skorch.readthedocs.io/en/stable/user/parallelism.html) but this is separate from the torch multiprocessing/distribution.

My guess would be that in some cases the data flow of skorch may get in the way if you focus on raw performance. But then you would not opt for a high-level interface anyway, I suppose.

In any case, if you experience major problems with any of these topics, feel free to open an issue and we can discuss the specific problem. Often times it is very hard to judge these workloads in a generic way."
MachineLearning,"Agreed with this. Even I have observed that it is better to do undersampling (specially random undersampling given how easy it is to implement and works good in most of the cases). 

When doing oversampling we are just trying to replicate or synthetically generate minority class samples (around the existing ones) which usually doesn't bring much to the table but on the other hand given that there are so many instances of majority class, one can easily let go a proportion of majority class. I usually keep a 1:9 ratio of minority to majority samples (but worthwhile to tweak this in your scenario)."
MachineLearning,"This is great, just gave it a follow!"
MachineLearning,"My impression all along is Timnit and company were extremely problematic, toxic employees who drove high levels of drama. If they had been model employees I doubt either would’ve been fired for these most recent infractions.

Google is not stupid and knew the PR this would generate. They already went through it worth Timnit. Clearly they view these people as very problematic employees that are worth the trouble of nuking."
MachineLearning,From an article the driver software detects specific algorithms. I have no idea what that means exactly but if its looking for activity signatures dl calculations should be unaffected.
MachineLearning,"You should wait for the 3080 ti, which should have 20GB of memory."
MachineLearning,"Yep it's all marketing (and extra profit because these new mining cards can't be used for gaming, so when mining crashes again, they won't flood the second hand GPU market"
MachineLearning,The whole proof of work thing is such a waste of energy. It would be cool if proof of work was actually useful like SETI or some other type of deep learning calculations.
MachineLearning,"Only the 3060 is being limited, right? That's a budget gaming card, anyway."
MachineLearning,"yolov4 / yolov5 I think, for the compromise mAP / inference speed (which is usually what people are looking for)

Otherwise I guess you can find the benchmark you want on paperswithcode"
MachineLearning,"If there are no proven benefits to eating meat, and there  is to plants, then by definition vegetarianism is superior. Therefore you are following the logic i atated."
MachineLearning,What are SOTA architecture in object detection at the moment in your opinion? But faster-RCNN is still used a lot in research. But I guess its not the vanilla 2015 version
MachineLearning,"You could read at efficient net, yolov4 tricks, attention blocks, self supervision SOTA etc. but I doubt it'll outperform current SOTA architectures"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"A friend of mine did just that a few years ago! He didn't really market it very well however, and I'm not sure he's still working on it today. Interesting project though with some cool visualizations.

https://www.reddit.com/r/gpumining/comments/9yju3t/raycoin_the_first_ray_tracing_proofofwork_and"
MachineLearning,[This (NSFW) kind of counts as the intersection of crypto currency and porn.](https://bitcointalk.org/index.php?topic=2225096.0)
MachineLearning,Those cards seem awful no? Not even 90Mh/s for over 300 Watts? A 3060 can get 60+Mh/s with about 125W if that
MachineLearning,"Again you don't seem to be engaging with my arguments properly. When did I say there were decades of experiments suggesting vegetarianism is superior? I said there wasn't any benefit to eating meat. So I'm suggesting it doesn't matter whether you eat meat or not. Not that either diet is superior. And because of that, athletes don't have a strong motive to change away from eating meat, even though they don't gain anything from eating it. 

When I talk about nitrates, I don't mean you can only get nitrates as a vegan/vegetarian. You can eat beetroot whether you are vegan/vegetarian/meat eater. I'm merely demonstrating to you that *there are* proven ways to improve performance, and eating meat is not one of them."
MachineLearning,"I think this is against training big nets in datacenters using gtx/rtx cards. Effectively making it much more expensive.

Think George Hotz was talking about it on Lex Fridmans Podcast."
MachineLearning,Amazing. Great summary.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Don't pretend your extra dislike of big tech, enough to raise the topic of FAANG as an acronym when it wasn't even mentioned and immediately criticise it... 

... isn't based in some grudge against software as a field or something similar. I'm just pointing it out."
MachineLearning,"Yes, UAI and COLTS are tier 1.5 conferences. 

Tier 3 is AAAI where most NeurIPS rejects go."
MachineLearning,"Thanks for your input. My use case would be predominantly exotic architectures we are developing.  I see lightening and skorch are similar in goals.. but wonder what the multi-GPU training looks like? Can I wrap a torch dataparallel /distributed parallel module in skorch? If so, would the hyperparameter sweeps like grid-search.. work in parallel?

I personally like the Skorch design philosophy being aligned with sklearn. Lightening code-style seems too much of a leap for me, though I would be interested in knowing your opinions (as a dev and user) on the comparison."
MachineLearning,"&gt; I can find your comment that says you ""aren't comfortable with machine learning math"" and you would call that trolling

I would probably would call it trolling because you probably saw that skimming  all my posts like a creep because it sounds like something I would have quoted from someone else to criticize boot campers who don’t want to learn the math

However again thats just ad hominem attack be either way"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Allow researchers a maximum of one paper published per year across all conferences.
MachineLearning,"Oh sorry, that's not how I meant it.

Nvidia prohibited use of consumer cards in servers, to sell basically the same chip (+ drivers) at a huge markup.

I am aware that Nvidia basically has a monopoly right now, since google/Tesla won't sell their tpus."
MachineLearning,"I can find your comment that says you ""aren't comfortable with machine learning math"" and you would call that trolling.

Anyways don't come around saying that everyone doing ML is some big tech seeking koolaid drinker and everything will be fine"
MachineLearning,"My way is to target conferences from the same tier several times and act according to reviews contradicting each other until I feel hopeless. Then, I target some journals since their formula assumes that the same reviewers will reconsider submission after changes.

The crucial factor for remaining sane is to have considerable output and multiple works awaiting review so that you can easier fool yourself that the success will come soon."
MachineLearning,"&gt; Meanwhile it looks like you don't have the background to interact with the papers, so why even claim you can speak for ""people on this sub""

Thats ad hominem bs . I guess you are just trolling which kind of makes a consistent picture with deleting posts"
MachineLearning,"I actually read this sub back when it was just sharing papers which is years ago, so probably longer than you have.

Meanwhile it looks like you don't have the background to interact with those, so why even claim you can speak for ""people on this sub"""
MachineLearning,"That is wonderful to hear. My ""production"" work is most sharing research codes with colleagues (I work in Physical Sciences + engineering)  so its a worry here. I am still not sure how it compares to Pytorch lightening - if they are competitors or just target different aspects of the ML ecosystem."
MachineLearning,"&gt;it's not to protect gamers as they frame it.

It is, kinda. Not for altruistic reasons, but because minners driving the cost up prices part of the gaming market out. They want to keep selling GPUs at low cost to casual gamers (who won't buy at high cost) while selling them at high cost to miners. 

So they manually segment their products so they can tap into that priced out part of the market while keeping their mining cash cow market too."
MachineLearning,"&gt; This isn't r/programming , as much as you want to think it is.

I am basing it on reading this subreddit for a long ass time. 

You are just going to delete your posts anyhow so I dont see the point. The point was pretty clear from the jump and I elaborated for anyone else."
MachineLearning,"Trivially true in theory by induction.

Empirically true as well (pornhub had a shitcoin, didn't actually research for the first one)"
MachineLearning,"Look for this german technology giant (S) 
https://openreview.net/profile?id=~Ahmed_Frikha1
At least in experimental status..."
MachineLearning,"Just out of curiosity, is there python code to take a 3d tensor and decompose it as a series of rank 1 tensors? I think its called the canonical polyadic decomp? Does tensorflow do this?"
MachineLearning,Marketing
MachineLearning,"If your paper was good enough to submit to NeurIPS and you believe you have a solid idea, then you should address what the reviewers mentioned and resubmit it, either to NeurIPS or another equally good conference. This is not uncommon. Don’t water down your work by submitting to an easier conference. 

If you got poor reviews and you aren’t confident it’s a good idea then well submit it wherever you want. Or just put it on ArXiV."
MachineLearning,"Hi, we are currently working on both of those issues. The domain is crowd surveillance (crowd flow, crowd density, social distancing etc.). What exactly do you want to know?"
MachineLearning,How big is it?
MachineLearning,"&gt; We all know

This isn't r/programming , as much as you want to think it is."
MachineLearning,"Also there is porn of cryptoporn, and crypto of porncrypto. And so on, recursively, all the way to infinity."
MachineLearning,Numpy might be one of the stabler? parts of python. But  unannounced changes are part of the thrill in data science
MachineLearning,Here. Fixed a tiny bit
MachineLearning,"&gt; Yes I know what G stands for. Now back to my question, who else used the FAANG acronym other than you?

Then you know the G stands for google and you know the sub slobbers on the thought of google and slaps G on their face for pleasure so yeah the sub isn’t unbiased in regards to google if they constantly think pre drinking corporate kool aid for FAANGs will make getting a job there easier because we all know its FAANG or bust to a fair amount of folks here and in the CS subreddits"
MachineLearning,I like what you're doing. Could be expanded maybe
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I believe they've stated in the past that segments of the driver are licensed from third parties as well. They couldn't release it if they wanted to in that case.
MachineLearning,"Consequences:

- There's a porn of crypto.
- There's a crypto of porn."
MachineLearning,Nvidia cards support CUDA which is essential for most forms of GPU-based rendering.
MachineLearning,Look up square root velocity functions and see where that leads. That was how I started reading about diffgeo.
MachineLearning,Potentially the most stupid invention of the 21st century. We've invented a way to waste gargantuan amounts of energy for almost no reason whatsoever.
MachineLearning,"Yes I know what G stands for. Now back to my question, who else used the FAANG acronym other than you?

Sounds like you're projecting discourse from other subs onto this one."
MachineLearning,I believe this doesn’t apply to the 3090.
MachineLearning,"CMP cards aren’t particularly RAM heavy, so I would be surprised if they’ll be all that useful. A 6GB CMP card isn’t going to get you very far no matter the speed, except for very specialized scenarios."
MachineLearning,Yeah and most people aren't event aware that it even existed.
MachineLearning,Are you sure you know what the acronym stands for? Especially the last letter of it
MachineLearning,No one mentioned FAANG except you
MachineLearning,"If there's no advantage, nobody would buy them and Nvidia just wasted money for nothing. As you note, there *is* a competitor, so how would this be unfair?"
MachineLearning,Twitter sucks
MachineLearning,"Hello machine teachers!

I recently started a little paper-reading account on Instagram ([https://www.instagram.com/paperreadinggroup/](https://www.instagram.com/paperreadinggroup/)) to motivate myself to read and condense at least one interesting paper a week.

I haven't promoted this to anyone so far (little bit shy!), but I've realized that Paper Reading is only part of the joy of a Paper Reading Group. So if you like what you see, do hit the follow button - I would love us to become a Group!

I try to read on a variety of topics within Machine Learning as I try to find my niche in this vastly exciting and interesting high-dimensional space, but I'm still pretty new to the field, so I'd welcome any comments, feedback or suggestions on new papers to read. :)

Cheers!"
MachineLearning,"It probably would have been better if they said that GeForce is a consumer line.  As far as I know the nerf only affects future Geforce cards.

The data center lines are definitely growing faster though (we’re chewing through them at my work for sure)."
MachineLearning,"Hello machine teachers!

I recently started a little paper-reading account on Instagram ([https://www.instagram.com/paperreadinggroup/](https://www.instagram.com/paperreadinggroup/)) to motivate myself to read and condense at least one interesting paper a week.

I haven't promoted this to anyone so far (little bit shy!), but I've realized that Paper Reading is only part of the joy of a Paper Reading Group. So if you like what you see, do hit the follow button - I would love us to become a Group!

I try to read on a variety of topics within Machine Learning as I try to find my niche in this vastly exciting and interesting high-dimensional space, but I'm still pretty new to the field, so I'd welcome any comments, feedback or suggestions on new papers to read. :)

Cheers!"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;no miners will buy 3060, no good gpu deals after crypto crash, people buy new gpus nvidia profit all while making gamers think nvidia is doing a good thing"
MachineLearning,"Exactly, no miners will buy 3060, no good gpu deals after crypto crash, people buy new gpus nvidia profit all while making gamers think nvidia is doing a good thing"
MachineLearning,Thank you for sharing I will have a look!
MachineLearning,"It might be a stretch (I don't think so), but do you think this Wuhan doctor who violated the policy to warn about COVID unethical?

Policy is not ethics.

https://www.latimes.com/world-nation/story/2020-02-06/coronavirus-china-xi-li-wenliang"
MachineLearning,"Second hand 1080tis are now about $1000.

The entire market across the board is all sorts of fucked up right now and likely will be for months."
MachineLearning,It will 😅 but we will not find cheap used cards.
MachineLearning,"The most optimistic outcome is that those new cards might be excellent workstation GPUs for us.

I don't care if my workstation's GPU can play games at all."
MachineLearning,"Me neither and I rarely game. But I need nvidia cards for cuda. I got a 1070 for cheap after the last crypto crash in 2018. Not gonna happen this time. 
Totally agree it makes business sense. But saying nvidia is consumer brand :D"
MachineLearning,"&gt; at least they are doing something with all that processing

That's awesome, considering [the energy mining bitcoins exceeds the energy used by entire countries](https://www.bbc.com/news/technology-48853230)."
MachineLearning,"&gt; Its a knee jerk tribal reaction.

A great definition of identity politics."
MachineLearning,Oh noes my karma is gonna get low lol
MachineLearning,lol not gonna crash this time.
MachineLearning,"lol -5 within hours. Gotta love Reddit, what a joke"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thank you !

If you have to do simple optimization, you can look for bayesian optimization. It is a method that uses the same gaussian process as the model, but possibly more advanced ways to select next points to try.

Example applications can be optimizing materials properties by testing different material composition in as few trials as possible. You can also look at this video for a visual explanation in a specific application (optimization of a drone controller parameters): [https://www.youtube.com/watch?v=GiqNQdzc5TI](https://www.youtube.com/watch?v=GiqNQdzc5TI)"
MachineLearning,Me for one.. I don’t mine and never will because its a waste of electricity.
MachineLearning,"I think their Twitter exploits are the real reason for the firing, not their gender, or the fact that they work in ethics. Woke army getting push back."
MachineLearning,"&gt; This seems like a CYA move to make sure they don't get sued when someone buys a few hundred of them and then they overheat or otherwise die on the rack.

Has absolutely zero to do with this.  Is simply about price differentiation."
MachineLearning,Crypto goes under gaming in the reports FYI. Miners don't buy teslas.
MachineLearning,"I mostly agree with you, and definitely agree that this balancing job is super tricky, hence why I think we need more people representative of society and not just a small group of engineers and far-left activists making the calls (this actually reminds me of that survey that went out a few years ago about crowdsourcing the trolley problem for self-driving car collisions...)

But just pushing back slightly re: different societies, that's something I really struggle with. With how open/accessible information is on the internet nowadays, unless you're China and you build a firewall, there's not really a notion of country/culture borders. Heck, I have QAnon acquaintances in my home country who don't speak English yet rely on google translate and/or bilingual QAnons to read Breitbart ._.

(As someone who's country has been torn apart by genocide in the past century, I think it would be great if AI/ML tech could help with achieving transitional justice and reducing pain. Thus I probably am in many ways more sympathetic to Timnit et al.'s antics than the average subredditor here 😂 But I think that's more of a personal note, the broader conversation about how we get to wield this tech needs to happen, notwithstanding what my own thoughts on this are.)"
MachineLearning,"This is a great example of edge computing, at least they are doing something with all that processing"
MachineLearning,It depends what you use BERT for. I've been able to fine-tune both BERT and Distill-BERT for text classification on a GTX 1070. VRAM usage maxes out around 6gb.
MachineLearning,"this doesn't make any sense. Why would anyone buy an nvidia card that doesn't mine, while you can get an AMD that does both. They are only doing this because they can. Both cards will use the same chips so this will not change the shortages. The difference is they can charge extra."
MachineLearning,"this doesn't make any sense. Why would anyone buy an nvidia card that doesn't mine, while you can get an AMD that does both. They are only doing this because they can. Both cards will use the same chips so this will not change the shortages. The difference is they can charge extra."
MachineLearning,"Oh, my bad. I was under the impression it is planned to work on GPU in the future."
MachineLearning,Could someone please ELI5 the concept of pruning at initialisation? What are they  doing practically? Sending random signals through synapses and from  those calculate the gradient based scores the have defined? (Trying to wrap my head around the SynFlow paper)
MachineLearning,"To sell more chips, and charge miners more. It's the same silicon, so shortages will persist until the next crypto crash. it's not to protect gamers as they frame it."
MachineLearning,Yea sorry thats skranger in python which I've tried. I like it but would need to figure out how to run SHAP on it. Do you have SHAP or an equivalent in R? Haven't used R in ages.
MachineLearning,Can someone ELI5 the concept of pruning at initialisation? What are they doing practically? Sending random signals through synapses and from those calculate the gradient based scores the have defined?
MachineLearning,I meant within the algorithm how it treats the categoricals (splitting method etc)
MachineLearning,Why would that change anything? You're still running on CPU.
MachineLearning,"I agree with you on pretty much everything, especially about the truth being somewhere in between. This is a terrible look for Google, though I also think that Google is not the only one doing immense damage to bipoc and women. 

From Timnit framing google as a white supremacy org, to her and her supporters constantly emphasizing the fact that ""if you succeed in big tech as a black women, you're either an anomaly or a sellout"", to her supporters in academia saying that they'll no longer encourage their woman/bipoc students to go to Google/facebook... 

Women/bipoc do NOT have the power/influence needed to gain **anything** by boycotting working at high impact companies. They're taking their own negative experience and trying to ruin+influence everyone else for that job. Fwiw I've been at a  FAANG as a non-white woman, and even without sugarcoating it (sexism/racism exists) I think the vast majority of women/poc definitely do not have as traumatic an experience as Timnit et al. goes on about. I know it sounds a bit victim-blamey, but if you go looking for a fight, you'll always find one. If you want to function in a large organization (especially in a managerial role,) you need to not be constantly in guerilla tactics mode and bristling for a fight."
MachineLearning," I'm just staying the harsh reality for corporations, their goal is to increase value and it's you're either with us or get out type of mentally. I value ethics and employee values but people need to understand why these terminations were made from the other point of view. There are terms to live with"
MachineLearning,[Ranger](https://cran.r-project.org/web/packages/ranger/ranger.pdf) for R.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Great idea. How do you find mid tier conferences, and what metrics are you looking at?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Absolutely, CUDA is amazing and helps explain why they have the kind of dominance they do. 

But part of me is dogmatic and wishes they'd cooperate and release open source drivers and such. I don't think they will (they'll have market dominance without doing so) but it'd be nice."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Maybe https://github.com/microsoft/DeepSpeed

I haven't tried it but it allows you to have memory requirements offloaded to the main(CPU) memory. Could work for your case."
MachineLearning,"Fair enough :)

I was just spitballing."
MachineLearning,"I found ordinal encoding to be sufficient for categorical, is there anything else available for it?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Doubtful, should probably just get a 3090.

See here: https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#GPU_Deep_Learning_Performance"
MachineLearning,If Jeff Dean has to put up with this nonsense then how are the regular folks supposed to function?
MachineLearning,"This is all because Google's PR machine is caught in a vicious cycle:

1) BIPOC and women have become elevated to celebrity status in the community and how many of them you can hire has become a metric you can optimize on to prove your virtue (now literally a data point used to evaluate managers at Google).  

2) The company locks down what information is allowed to leave the company in order to control PR.

3) If the people who violate 2) were hired in 1), you now have a huge PR disaster on your hands, nullifying your efforts in both."
MachineLearning,"it's just a rewrite of stuff they've been doing before. I honestly find the new NVIDIA stuff to be better https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/
cause you can just use standard c++ instead of the sycl implementation. tbh though, I like cublas way mroe"
MachineLearning,"So, in your opinion, despite all of decades of experiments suggesting vegetarianism is superior, nutritionists looking for any edge have simply chosen to ignore it?"
MachineLearning,Want don't they make dedicated cards for mining and charge titan prices?
MachineLearning,The hypothesis here  is that she tried all other realistic internal recourse.
MachineLearning,"Yes! Ah, that sucks to hear honestly I haven't used it yet. I just thought the idea neat. Does it seem like it would be bad even in a couple years, or bad with kinks that could be remediated somehow in a couple years."
MachineLearning,"Really good job! 
I’m wondering how this technique can be used for physical sciences. I mean for example it may be used for optimization problems, right?"
MachineLearning,"According to [this article](https://www.wired.com/story/second-ai-researcher-says-fired-google/) from Wired she was downloading material related to Gebru's firing.

&gt; A source familiar with Mitchell’s suspension said she had been using a script to search her email for material related to Gebru’s time at the company."
MachineLearning,"Wall-time or inferences/second might be a better metric. The problem is that it depends on the test system, so you have to benchmark all models yourself for a fair comparison.

Then details like GPU utilization (average % of CUDA cores / SM busy) become also important to identify if the libraries you are using make efficient use of all GPU resources. If not, then either the layer might be memory-bound or it has insufficient parallelism to exploit all GPU resources.  

I have to admit that my research topics are on energy-efficient processor architecture design (not specifically GPUs or deep learning accelerators), so there might be better people to answer these questions."
MachineLearning,Cuda represents years of 'putting in the work' that AMD simply hasn't done. It's not that surprising nvidia is still on top.
MachineLearning,And the restriction is around selling access to the cards. Google has to use t4 and a100 cards but a scrappy research lab rolling their own servers can still buy 3090s to use.
MachineLearning,I think that would largely defeat the purpose of making those modifications.
MachineLearning,"**Without going into it here lets not pretend like this subreddit doesn’t have a FAANG worship issue that will give it a bias also**.

The existence of that acronym is evidence of that worship"
MachineLearning,do you mean oneapi? I'm using it and I'd literally rather use anything else tbh
MachineLearning,"I am not sure about which specific comparison would be better, but I can add insight on the reviewers opinion. 

If your contribution is some increase in computational efficiency and all you have to show for it is empirical analysis (as apposed to theoretical,) then I would try my best to be as robust as possible in my comparison. Ideally you would include both metrics, explain your computer setup (gpu/cpu/ram), and show comparisons with other models running with the same method. 

If on the other hand your contribution is more related to a new solution to a problem, better accuracy, etc. Then I wouldn't worry as much about measuring FLOPs, as your contribution should be strong regardless of this. This is especially relatable to things like adversarial attack/defense where computation time is not as important as robustness/attack potential."
MachineLearning,Also this community has an extreme case of FAANG worship as the existence of that goddamn acronym helps to show
MachineLearning,I really wish one of these 2 researchers was actually a man because for the sake of science I would be interested in the response both here and in Twitter
MachineLearning,Sklearn is quite slow but that doesn't bother me as much as the complete lack of categorical support. I still suck at categoricals but other than the hashing trick I'm at a loss with dealing with large amounts.
MachineLearning,Thank you so much.
MachineLearning,"I tried sklearn, planning to use XGboost, will share my experience once I am done with it. I had issues with scalability with sklearn."
MachineLearning,1080ti's have 11GBs of memory. May be worth looking into.
MachineLearning,"I haven't seen any details, but they said it would work by throttling hashing. ML does not use hashing. Also hashing and crypto in general (not just crypto currency) is based on integer arithmetic, while ML uses floating point. 

So it's extremely very unlikely that it would, except maybe as a weird side-effect."
MachineLearning,"Actually, if your reviews are borderline and you are applying to a prestigious conference, I would withdraw then and there. This is how I've had so/so work accepted in the past:

Applied to high-tier conference, got one good review, one bad, one borderline. In our case the bad review had a good point, so we went ahead and withdrew the paper. Then we addressed the problems that were revealed to us and reapplied to another medium tier conference. The key is this time you don't have to say that your paper got rejected before! This is very crucial imo, especially if the reviews are open! Also this is something my advisor told us to do, and I am sure his experience with this was better than mine."
MachineLearning,"The CEO apologized for the firing and said they would correct the error. I'll believe that when I see it. If his and your attitudes are common among management, no wonder the employees are forming a union."
MachineLearning,"Nice work! Out of curiosity, what changes would need to be made to support FP16? It looks like you're largely relying on PyTorch building blocks, so shouldn't it be supported?"
MachineLearning,The CEO apologized for the firing and said they would correct the error. I'll believe when I see it.
MachineLearning,Distillbert seems like it would.
MachineLearning,Redlining is older than computers.
MachineLearning,Would it work on an 11GB card?
MachineLearning,Not 100% would be bad but depending on how 100% is measured it may still be not accurate.
MachineLearning,"The book written by Goodfellow would be a good fit, it's also available for free:

https://www.deeplearningbook.org/"
MachineLearning,"https://www.levels.fyi/company/Google/salaries/Software-Engineer/L6/San-Francisco-Bay-Area/

Timnit joined 2 years ago (an aside: L4-L6 in 2 years is unheard of) so that stock has almost doubled in value. 

Imagine being in a position where you think you’re so important that you can just throw that away. Those people are not experiencing trauma."
MachineLearning,"Thanks for the update! Yes, so when i run these tests my gpu is at a constant 100% utilization in each case. This is good for the fairness of the benchmark right?"
MachineLearning,One benefit of Skorch over Pytorch lightning is that Skorch didn’t plagiarize Fast.AI’s entire open source library.
MachineLearning,"Why are you going Mac for ML? You don't care about having CUDA support with some Nvidia GPU? For example if you were planning to run pytorch frameworks, I am not sure how your performance would be: https://github.com/pytorch/pytorch/issues/47702

But otherwise 1tb should be enough, considering that you only use 500Gb as or now. The only down side is you can not expand later (although I am not sure about this, not an apple user.)"
MachineLearning,I edited and added how much a GPU is needed which can be a much bigger problem. If you run nvidia-smi is your gpu constant 100%.
MachineLearning,"‘Cause you need a job or you starve.  Most corporate workplaces have a lot of toxicity. Can’t just hop over to competitor B and have happy working environment.


Twitter mob?  You could describe anyone asking for accountability as a mob if you assume the person being criticized is innocent.  That’s also what people call those calling out sexual harassment and abuse, proved racism and sexism.

Most people don’t bail on a lucrative position at the first sign of harassment, and most don’t speak about it publicly, either.  You have no idea if her claims of racism or sexism are accurate because you didn’t sit next to her every second to observe."
MachineLearning,My intention is to get really hands on with deep learning within the next 6 months and I too am weighing up whether to get a rig (I don't game) or use aws/Azure.
MachineLearning,Hmm good points. I could set my GPU to a constant clock and memory rate and like that make sure that the speed stays the same?
MachineLearning,Thanks for the insight. Yeah this seems like the issue. Do you know of any popular alternate or additional way of providing a more trustable performance comparison?
MachineLearning,"No, eating vegan was one of the few things they ranked as moral and also did irl (more than the control group, which included professors of other fields as far as I can recall). Its just one small exception I mentioned to be fair with them."
MachineLearning,"Thanks for that view! That is what I thought. I never really see anything else but FLOPS reports in papers, but I feel like giving a relative runtime comparison additional to this would give a much improved overall comparison. So like you say, seems like it would be best to just do a FLOPS comparison AND a runtime comparison. Thanks."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Do you know that your gpu is running with the same speed always and there is no turbo/throttling involved? Even if you warmup the gpu some task may run hotter. Also some task are bottlenecked in compute while other in memory bandwidth.
MachineLearning,"This seems like a CYA move to make sure they don't get sued when someone buys a few hundred of them and then they overheat or otherwise die on the rack.

Definitely worrying that they have the kind of monopoly they do though."
MachineLearning,"Thank you for the answer :). Yeah that is what I thought. Based on different GPU setups, my runtime benchmarks might vary in favor of different models. So, it is probably best to report this runtime table and ALSO put in the work to be able to report a FLOPS table, right? I guess that way I would be on the safe side and provide the most holistic view of performance, right?"
MachineLearning,"&gt;Wouldn't it be the only ethical course of action left if the company policy was unethical according to her framework and that those documents proved it but she found no other recourse because internally she wasn't able to change anything?

Either you change it from the inside by working hard on it, or you are out. Outcry on Twitter and violating what you signed in your work contract is not the way to do it by any cause."
MachineLearning,"Well, I also play games. It’s the main use for the card these days."
MachineLearning,"gosh, this is beyond disgusting."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Maybe. /s
MachineLearning,"Your question is not easy to answer. It's actually a research question that many in the computer systems and metrics community think about. The safest rule of thumb is to copy the methodology of previous papers in your problem area. If you're uncertain about it, provide additional metrics on top of   it."
MachineLearning,"&gt;she downloaded and moved thousands of documents outside the company clearly violating the company policy

Wouldn't it be the only ethical course of action left if the company policy was unethical according to her framework and that those documents proved it but she found no other course of solution because internally she wasn't able to change anything?

Just to say that it's not because it' against company policy that it's wrong, even from the point of view of the company itself. That's precisely what an oversight board with a meaningful impact should be able to challenge."
MachineLearning,"UAI and COLTS are slightly easier? My man.. you'll learn the truth the hard way.. Whether its AISTATS ICML or NeurIPS, the odds of getting in are not that different. It's all about the quality of your paper and luck."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The real question is if the cheaper CMP cards can DL as well as the GeForce cards. Because if they can, they might be cool choices for DL focused workstations"
MachineLearning,"&gt;twitter's algorithm keeping all the spicy stuff out of \[...\] view

that'd usually be the opposite to get engagement"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"So it's unrelated to being vegan specifically, that was just the context for that experiment?"
MachineLearning,"Twitter users are terrified of provoking the woke mob because most Twitter interactions are tied to real identities and they'll go after anyone who doesn't bend the knee.

[Pedro Domingos](https://quillette.com/2021/01/27/beating-back-cancel-culture-a-case-study-from-the-field-of-artificial-intelligence/) - a Comp Sci professor at the University of Washington - stood up to them and they tried to get him fired."
MachineLearning,"I'm in the same boat with 6 gb. I'm wondering that, given the currently-inflated second hand gpu prices around if it'd be smarter to sell it and just use aws rather than try to upgrade."
MachineLearning,"Consumers will obviously switch to other product. imho why are there pushing for that kind of update, is there any damage it is doing to the financial market"
MachineLearning,You’re actually experiencing the wonderful corrupted world of science. Look up Eric and Bret Weinstein they have a lot to say on the subject
MachineLearning,"CUDA on Nvidia cards is the only truely supported way to do machine learning on the GPU right now, you are severely misinformed https://developer.nvidia.com/deep-learning"
MachineLearning,"This is one of the gripes I have with the online activists. They take someon's suffering and make a carricature out of it. As someone who has faced sever childhood abuse, when I see people claim ""not getting a free pass is trauma"" is so disrespectful to the victims of actual trauma and making a mockery of it."
MachineLearning,"There are just too many people applying, I having forced to do 2-3 interviews a week, which is an out of control of time for me, had I not declined some of them.

Trying other venues would be my suggestion, don't focus on FAANG solely, there are just that many internship position opening every year."
MachineLearning,"As you say, the non technical parts are currently occupied by the far left.  This is a big problem for the field of AI ethics because it has the effect of discrediting the whole field with people who are not the extreme left.

I am a mainstream democrat (Obama style) and I build real AI products that are in use today (business software).  What I hear coming out of the field alienates me.

Unfortunately, it is not socially acceptable to publicly disagree with the far left.  As a result, I never speak up publicly about this.

I would love to have a real AI ethics field that used a moral compass that was more similar to broader society."
MachineLearning,"Good coin miners aren't.

Amateurs still are."
MachineLearning,"College student here, looking to get into ml a little. I am buying an M1 macbook pro and wondering what size ssd to go for: 1tb or 2tb. I am edging towards 1tb because it's 400 euro cheaper and before this I havent used more than 500gb. I don't have a lot of media. What would be good considering I will use it for development and doing ML? Is 1tb good enough?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Right, that is what I am saying. Nvidia is taking gradual steps to normalize its ability to control what they allow to run on the GPUs they sell. It's one of the advantages of a company with an overwhelming monopoly."
MachineLearning,"To sell more mining-dedicated graphics, coming soon"
MachineLearning,This whole saga has nothing to do the facts on the ground. ML people/Silicon Valley types are just as immune to identity politics as they claim these SJWs who are ruining ML are. Its a knee jerk tribal reaction.
MachineLearning,"&gt;Yeah Timnit and Meg were both L6 which means they were easily making 1/2 million + (would have been even more with the recent stock appreciation).

Wow didn't know it was that much. Must be easily global 0.1% incomes, as 420.000€ would be Top 0.26% of incomes in Germany and similar in the UK."
MachineLearning,"Fair. But the thing is studies show that EVERY diet helps with weight loss for example, simply because you start being conscious about your intake again.

It's complicated, obviously. [This](https://www.who.int/nutrition/topics/guideline-development/en/) was my starting point to decide what I want to eat regularly."
MachineLearning,As I said
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; a balanced vegan diet is better than the average western diet 

you're comparing apples to oranges.  yes, the average western diet is awful, calories excessive, etc.  but very few westerners who practice veganism have balanced vegan diets either.  compare like to like.  the average western vegan diet is awful... while you don't die of obesity, the hormone problems on the average western diet are excessive.  you get women whose periods stop without any other birth control, men whose testosterone falls so hard that if they're not on injections, they can't get an erection and they can't produce swimmers.  and children raised on the average western vegan diet are underdeveloped, excessively low weight, or they just outright die.  and that's just the basic indisputable medical stuff... changing someone's hormones at that level has serious effects on mood and basic behavior.  significantly higher rates of depression... the list goes on."
MachineLearning,"To be fair, the company behind this are the developers of Octane render, which is a real deal and probably the best renderer there is (which happened to be a GPU one)"
MachineLearning,"if you are trying to gain a more holistic benchmark, this may work, but it will still suffer some degree of bias similar to using FLOPS. while runtime may seem like a more holistic approach, your runtime is now sensitive to architecture changes. if a newer generation of nvidia gpus come, some operations may be disproportionately faster than others, causing different speed gains across different CV models."
MachineLearning,"They haven't done anything to the cards, the license is there to force companies to buy their enterprise cards

You can still do DL on all kinds of cards, you just can't legally deploy models on Geforce cards. I'm talking about a scenario where they straight up gimp the performance in DL applications, which is not likely to happen in the same way, as DLSS is one of their top products as of right now, but it's concerning nevertheless."
MachineLearning,imho NYU is actually a better course to MIT in terms of Technicality and depth while MIT is like a Intro Class for Non CS Students
MachineLearning,"&gt; I've been wrongfully terminated before for pushing back against bad management practices and employee treatment. I got my job back because of course they had no evidence of misbehavior as there was none, excepting some very choice words I said to my direct supervisor as they attempted to fire me. 

I never understood why anyone would want to go back to a company that engages in such blatant misconduct... even if you won your lawsuit, you can't ever go back to things just being normal at that company.

&gt; Sure Google claims they fired Gebru because of that ultimatum, but in fact we have no way of knowing why she was fired specifically, and how long that plan had been in the works. It certainly gave them a convenient excuse to terminate, but that doesn't actually touch on the ethics of the situation on either side.

except we know with 100% certainty that immediately as it was happening, she weaponized her woke twitter mob to start baselessly defaming all these people at google as racist/sexist.  that's toxic and not just a fireable offense... jeff and yann and others who were defamed could sue her into the ground.  they were the bigger people and just let it go."
MachineLearning,"Rule 35

An addendum of the internet rules stating that if there is no porn of it, it will be made."
MachineLearning,"

&gt;There has, nevertheless, been decades of nutritional studies by people trying to find any additional edge

Yeah! And not one of them found eating meat to have a provable significant advantage over not eating meat. And like I said earlier, the one significant advantage they *have* found comes from consuming nitrates from nitrate rich plant sources such as beets. No other nutrional magic bullet has been found, aside from just obviously meeting your macro and micro nutrient targets which can be done on both a plant based and a meat eating diet. The point you still haven't absorbed from one of my earlier comments is that: because of the bias in favour of meat, it takes a huge amount more convincing to switch to plant based. So even if they are equal, athletes will continue to eat the diet closest to how they were brought up. 

&gt;Bare minimum, that means vegetarianism needs a hell of a lot of effort, on average, to compete.

Source? What is the extra effort they need to put in?"
MachineLearning,"Rule 35 of the internet:

*If it exists, there's a shitcoin about it*"
MachineLearning,"This entire thread says no.

I’m not convinced yet, though I’ll wait judgement till we have empirical benchmarks to form an opinion either way.

In theory hampering integer logic and hash functions shouldn’t affect matrix multiplication as they are mostly utilize (read: bottlenecked) by different silicon real estate and nvidia may very well be able to limit one but not the other. However this is a bit simplified view IMHO due to anywhere from hidden vendor agenda to implementation bugs to limitations in chip architecture and shared buses.

I’m not saying that it will affect ML/DL- I’m only saying my confidence level is not very high that it won’t."
MachineLearning,"That's one narrative that could be argued for, but it's not the only one.  I don't think anyone has contested that Mitchell removed documents from Google.  The argument there is about why she did so.

&amp;#x200B;

As for Gebru, that's a very simplified and speculative version of events.  We haven't seen the internal documents on why any of the decisions towards her were made.  After all, dozens of Google employees disputed the company accounts of the review process.  


I've been wrongfully terminated before for pushing back against bad management practices and employee treatment.  I got my job back because of course they had no evidence of misbehavior as there was none, excepting some very choice words I said to my direct supervisor as they attempted to fire me.  My site manager and local HR person wouldn't even respond to my communications for a month after this ""firing"", but as soon as HQ called them about it, suddenly they were offering me my job back, although they wrote me up for ""disrespect"" for slagging off my manager in the termination meeting after he made it clear he intended to fire me regardless of the lack of justification according to the law or company policy.  Apparently curse words are intolerable while wrongful termination is no biggie.  


Sure Google claims they fired Gebru because of that ultimatum, but in fact we have no way of knowing why she was fired specifically, and how long that plan had been in the works.  It certainly gave them a convenient excuse to terminate, but that doesn't actually touch on the ethics of the situation on either side."
MachineLearning,[deleted]
MachineLearning,"[https://www.datacenterdynamics.com/en/news/nvidia-updates-geforce-eula-to-prohibit-data-center-use/](https://www.datacenterdynamics.com/en/news/nvidia-updates-geforce-eula-to-prohibit-data-center-use/)  


They've already integrated it into their license agreements. The next step is software."
MachineLearning,"timnit wanted to publish an article with google's name on it that failed google's publishing review.  she could still publish it, just not with google's name.  she then proceeded to submit it anyways to a bunch of journals, with google's name on it, against their wishes, in basic violation of google's policies.  when she didn't get a free pass to violate policies that apply to everyone in google, she immediately took to twitter to defame them personally with evidence-free accusations or racism/sexism.  there's no universe where that's okay.

&gt; it was a disastrous PR move for google and did immense damage by sending a horrible signal to women and BIPOC in tech

these researchers are the ones doing it themselves.   they don't get special treatment just because they're women or minorities.  that would be racist/sexist."
MachineLearning,"Yeah Timnit and Meg were both L6 which means they were easily making 1/2 million + (would have been even more with the recent stock appreciation). 

Real privilege is blowing up your tech job in the middle of a pandemic when unemployment levels are through the roof because you think you are unfireable."
MachineLearning,"You said people are biased and people are studying the benefits of plants. Yes. There has, nevertheless, been decades of nutritional studies by people trying to find any additional edge. Bare minimum, that means vegetarianism needs a *hell* of a lot of effort, on average, to compete."
MachineLearning,"So you have not read or properly engaged with my previous comments then, where I have addressed this argument. 

Please re-read them thoroughly and if you still take issue with my stance, come back to me with the exact sentences and studies that you take issue with."
MachineLearning,"mitchell's google account got flagged and locked by an automatic system that detects compromised accounts.  her account was pulling out thousands of files very quickly, and transferring them out to external accounts.  when google approached her about it, she was the one doing it herself.

gebru's firing wasn't unethical either... she refused to abide by google's policy on putting their name on her research article, and tried to get something published that failed google review.  she could still publish it, just not with google's name on it.  she went on repeatedly trying to get it published with google's name on it, in blatant violation of company policy and basic sensibilities.  she immediately shifted to accusing them of racism/sexism (absent any evidence at all), and then told them if they didn't let her publish with google's name on it, she was going to eventually have to find another job.  google said ""well then, we accept your resignation effective immediately""."
MachineLearning,"I thought you had already accepted that the majority of athletes eat meat. If you do, you must concede they follow the advice of thier nutritionists; who not being idiots, investigated the hype of vegeterian diets becuase it dos give great results for *some* people."
MachineLearning,Feel free to judge when you posses relevant information about what actually happened instead of conjecturing what you think happened based on your “gut feeling” about people you’ve never met.
MachineLearning,"It really depends on what you’re training. Trying to load even a educed version of BERT for transfer learning nopes out very quickly on my 6gb. If anyone actually has a way  around that, please let me know."
MachineLearning,"No, Deep Learning performance isn't going to be affected for neither of the GeForce, Titan, Quadro or A100 GPUs."
MachineLearning,"I have linked studies showing ""scientists doing their job"". You have told me your theory and not shown any evidence to back it up."
MachineLearning,"""Scientists do thier job"" isn't a sound argument?"
MachineLearning,"&gt; My gut feeling is she was leaking internal emails, where some of her coworkers or managers probably have said the ""wrong"" thing. So that the Twitter mob and activist media can witch-hunt her ex-coworkers, who are doing the ""wrong"" kind of thinking. If I was her coworker I'd be scared to work with a toxic person like this. Think about it. The person you are sitting beside is plotting to ruin your career and life just because you are not agreeing with her politics. 

which is double nuts because these people absolutely use this stuff to take people out.  even if you're not some hate-filled bigot, these people have made this persecution stuff so much of their identity that they say everything is racist/sexist/whateverist... if they can't find something you said that's actually prejudicial, they're going to mind-bogglingly twist something you said into being hate speech.  and if that doesn't work, they'll just make up sexual assault / harassment accusations that can't be proven or disproven and then you're dragged through the mud anyways.

no one should have to face sexual assault or racism or sexism or whatever at work or anywhere, but these people are a serious overcorrection.  they're utterly unstable.  just being around them is a huge liability in every way."
MachineLearning,"But do you understand that that is not a sound argument? It's just based on a theory you have that you haven't got proof of. Meanwhile, I have provided sources for my statements."
MachineLearning,"I haven't been bothered to find a study. I thought the simple idea that olympian nutritionists would have tried vegetarian diets on thier athletes would suffice. Even if you say they wouldn't take the risk, they *would* take the risk at a lower level."
MachineLearning,"&gt; priviliged

Check your privilege.

***

^^^BEEP ^^^BOOP ^^^I'm ^^^a ^^^bot. ^^^PM ^^^me ^^^to ^^^contact ^^^my ^^^author."
MachineLearning,"Benchmarking is difficult and FLOPs is a very simplistic estimate for runtime given that most of the workload consists of matrix-matrix multiplications and the model is not memory-bound. 

Modern networks, especially those containing dilated or depth-wise convolutions (e.g. mobilenet), are much slower than traditional convolutional networks with the same number of FLOPs."
MachineLearning,"""Firing @timnitGebru created a domino effect of trauma for me and the rest of the team, and I believe we are being increasingly punished for that trauma."" - https://www.twitter.com/mmitchell_ai/status/1362885120005345280  

Classic first world problems.  
As someone who lives in a 3rd world country I cannot stop laughing when I read stuff like this. If this is ""trauma"" then I don't know what to say."
MachineLearning,"I agree with your points, the problem is that it is impossible to disentangle the valid points from the loaded accusations. E.g. was she fired because of her work, or because she was a black woman and it is because Jeff Dean is a sexist, racist, white male. Google itself is portrayed as this extraordinarily horrible  and evil place to work at, where you are constantly being ""gaslit"" and psychologically manipulated going as far that people are ""traumatized from their experience of working at Google"".  

It gets rather to identify yourself with incredibly priviliged people that consider themselfes incredibly oppressed."
MachineLearning,"&gt;And yet, meat remains part of thier diet... it's unrealistic to think the nutrionists haven't considered this.

So:
Many athletes eat meat =&gt; meat provides an advantage over plants in athletic performance

Is not a sound logically argument. There are other factors to consider. Not only is the ""default"" diet seen as one that contains meat, but we grow up bombarded with the idea that meat = strong and manly. Which has no basis in science, but it can affect the psychology of athletes and even nutritionists. We have grown up with a biased view on nutrition. So for an athlete to switch to a plant based diet they don't just need a study showing that meat and plants provide equal benefits, but a study showing *strong negative* effects of meat. There's a higher hurdle to overcome to make someone stop eating meat. 

Additionally, there *are* successful athletes who do not eat meat - others on this thread have linked examples of this. 

Finally, the biggest point here I want you to understand, is that if meat really did provide some significant improvement in athletic performance over a plant based diet you would easily be able to find a study showing that. But there isn't such a study."
MachineLearning,"True, that piece of information was on regard to sustainability not optimality. Nobody knows what an ideal diet is. But we can observe that a balanced vegan diet is better than the average western diet and not worse than a balanced western diet.

Since it's cheaper and environmentally more sustainable as well and minimizes animal suffering, to come back to the OP, it clearly is the more moral and ethical choice."
MachineLearning,"this.  the vegan/vegetarian crowd likes to accept the science that dumps on excessive meat consumption but when you point out the plain science details how some meat consumption is just strictly better than none, they're just going to downvote you and huff away."
MachineLearning,"So I tried finding the original paper but my institution is not allowing me to login at this moment. Anyway, I found some reading that may help you understand the way max out can be implemented to produce outputs with more than one dimension. https://journalofbigdata.springeropen.com/article/10.1186/s40537-019-0233-0

If you look at their figure 1, they are showing a Maxout-2 layer that has two outputs. Maxout-2 in this case means that they preform 2 seperate linear transformations (each with their own set of learnable weights and biases) to the input features, and take the maximum one. They use TWO of these maxout-2 ""cells"" in one layer so that the output is of dimension two.

I am assuming in your paper they are using maxout-2 as well, but are outputting to a dimension of N and hence have N maxout-2 cells in one layer. The same is done in the next layer depending on the output dimensionality that is desired. 

Hope this helps!"
MachineLearning,"&gt; But if you look at asia where over 50% of consumed meals are vegan and at large studies that show that Chinas shift to meat based diets brought with it the same health issues so prominent in the west, you can see a clear connection.

except this is just rationally garbage.  i can have vegan breakfast, vegan lunch, a big fat juicy steak, and taco bell's hot garbage excuse for ""beef"" for fourth meal, and ""50% of my consumed meals are vegan"".  just because a continent has a large amount of people who can't afford animal products in most meals doesn't make veganism healthier."
MachineLearning,I wish I had enough disk space to store The Pile and train word embedding on it
MachineLearning,"Nvidia aren't profiting from the shortages. They haven't raises prices themselves and are still selling near MSRP. Scalpers are picking them up and selling at a higher price. That is why there is no stock anywhere, because they are not selling at free market prices. 
The prices are only this high because of mining. 
There not going to sacrifice their high margin data center cards for gamers either."
MachineLearning,"Pre-trained models are not the solution to every task. They just work as the baseline for the new solution, and still needs to be trained with task-specific data for the downstream task. And yes there are cases when pre-trained model can be leveraged with comparatively less amount of data. The guideline in the blogpost says that if the data is hard to aquire and/or expensive to label, then we should not use ML. If both problems are not there then of course using pre-trained model can be useful. Cheers..."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Most likely because the revenue report is public for everyone to look at and gaming is indeed the biggest part of their income at around 50% of their revenue. It's not like OPs post was based on speculation. It's based on the quarterly report.

The second biggest part of their income is data, but still gaming is much bigger.

Also sometimes it's important on how to say things, especially if you ignore facts."
MachineLearning,"Just out of curiosity, at what point should we be able to judge? She was on the probation since the first week of Jan for her conduct. It's safe to assume whatever documents she had leaked, the third parties have access to the said documents for almost two months. Is there any word of substance on what was in those documents? It's very unlikely that the her activist friends have access to something explosive and they are just sitting on it for two months in the age of outrage culture and scoops.

I see a cult of personality around these ""ethics"" researchers. ""Ethics researchers"" have become the new catholic priests, the new protected class. Anytime an ""ethics"" researcher does something very unethical given the evidence on hand, I see the arguments along the line that they must be the innocent or the victim party or they must be on a mission to expose the next holocaust. (Sorry for the exaggeration. I hope I have got my message across.) Are we giving these people a free pass just because they're ""ethics"" researchers? At point do we hold them resposible for their actions? At what point do we stop giving them preferential treatment just because they are ""ethics"" researchers? This wouldn't be a news to begin with if the fired person was not an ""ethics"" researcher or some sort of new protected class.

I am not necessarily accusing you here. But this is something we all need to think about."
MachineLearning,"A one year online course will be enough to learn a lot of useful Data Science tools.

 A Master's would help if you want to dig deeper into the theoretical side of things. The  advanced math courses in grad school related to ML have helped me a lot, but they definitely were time consuming and not for everyone depending on your goals.

Masters are expensive though so idk...when applying to grad schools I told myself if I'm not getting a funded offer for an MS/PhD, then I'm going to industry and learning these tools on my own time. Heck at that time I didn't even know ML was what I'll get into, but I deff knew that working in industry as a fresh B.S. ECE graduate was something I really want to avoid (atleast from the impressions I got at my internships in a power electronics company.)"
MachineLearning,"No, but when they realise that there isnt uproar about them restricting them for mining they will restrict the rtx series for AI most probably"
MachineLearning,Can’t they? Why don’t they reduce their capacities for pro cards in favour to consumer cards? I wouldn’t either. That’s what I’m saying: the current shortage with immense prices is their sweet spot for the current capacities. NVIDIA is fables they could change that but why should they? All I hear is some gaming fanboys crying why they can’t source an RTX 3080.
MachineLearning,"And yet, meat remains part of thier diet... it's unrealistic to think the nutrionists haven't considered this."
MachineLearning,"&gt;clearly found the best results needed meat.

So to be clear then, you are not basing your statements here on actual research you have seen, just ""well surely that must be the case because a lot of athletes eat meat"". That's not a scientifically sound foundation for your beliefs. 

The most common concern surrounding a vegan diet seems to be protein, however, it has long been shown that an entirely plant based diet containing an adequate number of calories [WILL have](https://www.ahajournals.org/doi/10.1161/01.cir.0000018905.97677.1f?url_ver=Z39.88-2003&amp;rfr_id=ori%3Arid%3Acrossref.org&amp;rfr_dat=cr_pub++0pubmed&amp;) 
*more* than enough of all essential amino acids, no matter which plants you choose to eat. The myth that plant proteins are inferior came from a 50+ year old study on rats, and even the author of that study has since said it was inaccurate.  

The hottest topic surrounding nutrition and athletic performance has actually been [nitrates](https://pubmed.ncbi.nlm.nih.gov/22709704/) from plant sources. These improve oxygen delivery and utilisation and have provably improved athletic performance."
MachineLearning,https://coinmarketcap.com/currencies/render-token/
MachineLearning,"I believe that Google is profoundly unethical, but I also believe that the strain associated with ethical AI research is not opposed to the immoral choices Google is likely to make.

The problems I see in Google are a tendency to want to 'combat misinformation' and other censorious activities where they place themselves arbiters of truth and morality above ordinary people. That's where Google's immorality lies."
MachineLearning,Generally this is so that data-centers will buy the professional level cards that cost 5x more. They still let individuals use the gaming cards to get them to buy into the Nvidia ecosystem though
MachineLearning,"Usually they pay more and get the datacenter cards if they want some sort of service contract through a lease I believe. 

Nothing stopping them just putting the consumer grade stuff of course but usually companies just pony up"
MachineLearning,"It depends on your network. CUDA cores are not only maeketed to DL applications, they're just another name for execution units.

Just get the fastest available card"
MachineLearning,"They were talking mostly about the GEFORCE line in terms of consumer. NVidia CAN’T double their capacity, there is a worldwide shortage in the supply chain for high end chip manufacturing, why do you think basically every company except for Intel (who has their own fabs) are struggling with supply?"
MachineLearning,"3090 would allow huge models without device parallelism. However, I think it's best to wait until this sort of thing is resolved, unless you will get enough training worth of the 3090 to justify the price... 6GB 1060 should be fine enough for now, IMO, you're mostly batch size limited I imagine."
MachineLearning,Is it possible then to invent a new coin that uses video rendering as proof of work?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I have no idea what the documents contain, and I’m not defending her, just pointing out people shouldn’t be so quick to judge other’s actions. She may have had legit reason to take the documents or she may not have, idk, but I doubt anyone on this Reddit thread knows what really happened in a meaningful way to judge either her actions or google’s action."
MachineLearning,"&gt; It’s funny to see Twitter going crazy over this.

They have been posting for like what, three months straight now how Google and their superiors, are evil, sexist white supremacist that don't care about diversity. I don't see how Timnit and Margaret are going to get hired outside of Academia, who wants to have a person with a huge following working for them that is going to trash you non stop on Twitter if they feel wronged?"
MachineLearning,Ty for your comment. For my PhD I applied Machine Learning to optimise semiconductor supply chains. Every quarter I’m sourcing our data centres with NVIDIA products. I EVEN HAVE A PERSONAL KEY ACCOUNT MANAGER AT NVIDIA. But GAMING FANBOYS downvote me ... guess it’s time to stop commenting to BS 😅😅😅
MachineLearning,"No, but nvidia may in the future implement something similar for DL"
MachineLearning,Here's a good site on what miners are doing w/ GPU's  https://whattomine.com/
MachineLearning,"In case someone wants to deploy it in data-centers, do they need to pay more to get the license?"
MachineLearning,"And equally, teams of nutrionists work for these olympians, and they clearly found the best results needed meat."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Numpy and scipy are pretty stable, far more than most because of how long they have been around and how well established they are. I have numpy code from a decade ago that still works just fine even with the most up-to-date versions today."
MachineLearning,"No idea why you are getting downvoted, people who call Nvidia a consumer brand in an ML-Forum are pretty clueless."
MachineLearning,"&gt; The world record holders are not vegans.

I'm surprised that someone on a machine learning sub doesn't appear to have a good grasp of statistics. Clearly the sample size of vegan athletes is far too small to prove statistical significance."
MachineLearning," Paper reference: Rematas, K., Martin-Brualla, R., and Ferrari, V., ""ShaRF: Shape-conditioned Radiance Fields from a Single View"", (2021), [http://www.krematas.com/sharf/index.html](http://www.krematas.com/sharf/index.html)"
MachineLearning,"this reddit was like ""gebru bad for shitting on her employer"", did you really expect anyone to back margaret?"
MachineLearning,"have you read the work on sinkhorn networks? fascinating stuff.  https://arxiv.org/abs/1802.08665

it's probably not a full solution for your problem but could play a part as a building block. essentially it's a way to learn a permutation matrix, where any row and column only has one 1 and zeros everywhere else."
MachineLearning,Here to hoping intel's api thing works out. Never thought I'd say that.
MachineLearning,"This is a common argument I see on her behalf. And has anything come out of her leaked documents? Has she said anything substantial other than the typical protrayal of victimization? Or is there anything substantial anywhere other than the regular crying wolf from people like this?

My gut feeling is she was leaking internal emails, where some of her coworkers or managers probably have said the ""wrong"" thing. So that the Twitter mob and activist media can witch-hunt her ex-coworkers, who are doing the ""wrong"" kind of thinking. If I was her coworker I'd be scared to work with a toxic person like this. Think about it. The person you are sitting beside is plotting to ruin your career and life just because you are not agreeing with her politics."
MachineLearning,"What do you think is the lowest ranked university that is still decent (in terms of career prospect) for a PhD in ML?

I know it depends on what you achieved during the phd but let's say you are just an average student."
MachineLearning,"Most cryptocurrencies nowadays use special-purpose ASIC chips, which have significantly higher hash rate than GPUs. Some currencies, including Ethereum, design ASIC-resistant hash function. Until someone invents an ASIC for their complex hash, GPUs are the second best choice."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"With pre-trained models, is size of dataset still required?"
MachineLearning,"Also veganism just isn't very common in athletes yet, and again, individual differences play huge roles. [Searches like this can be interesting starting points to dig deeper though.](https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=vegan+athletes&amp;btnG=)"
MachineLearning,"That's just it though. Vegan is sustainable for everyone and for some athletes it can be ideal. The Game Changers interviewmovie showed many remarkable examples of athletes that switched, to increase their performance. Taking a fanatic that trains like hell to prove vegans van do it too won't help, but there is many, Arnold Schwarzenegger as well fir example, that did the switch to *enhance* their performance.

The myth that protein must be from meat is just weird, as animals get their protein from greens. You don't need that middle man.


I'm not here to convert anyone to a religion, but the amount of misconceptions on nutrition out there is staggering considering that it is a core source of pleasure and wellbeing. I myself used to eat meat and animal produce 3 times a day and reduced it down to 2x per week on average. I must say nothing changed besides me saving about 500 bucks on food and my recent blood screening showed no changes(perfectly fine as last years).

But I cook as a hobby and learned lot's of indian cooking like currys masalas, ramen, wok and rice dishes etc. It takes about 50% more time to make a tasty and affordable vegan meal as compared to western meat based dishes. So yeah, there's pros and cons. I just like to save a buck and maybe splurge on a supreme cut of wagyu every few months instead of consuming cheap cuts daily."
MachineLearning,"At this point I don’t even know anymore what specs I should look out for when shopping for a consumer ML/DL card. GPU Memory seems good, but the 3080 doesn’t exactly have a lot of it compared to the 3090. Also CUDA cores. As for clock speeds I have no idea if that makes any serious difference. 

Am currently running a 6GB 1060 and wouldn’t mind cutting training times in half. If only the cards were available in the first place."
MachineLearning,"They're releasing [special crypto mining cards](https://www.nvidia.com/en-us/cmp/) next to their consumer and data center models. seems to me they're trying to get every customer in their respective lane, buying the cards that were deigned for them."
MachineLearning,"Hey! Thanks for your reply. Based on your answer I feel like I didn't express my point clearly enough. 

&gt;If you threaten your company with  resignation, they won't give in to your demands, they'll fire you. No  need to even make the demand something outrageous, like ""name the people  who rejected my paper so I can have them canceled""  
&gt;  
&gt;And  if you  move private company documents outside the company when you  were specifically asked not to, and a lengthy internal investigation  confirms this, then you will be fired. What do you want them to do,  positively reinforce this kind of behavior.

I am trying to argue the notion of ""right"" on two dimensions here. Was Google legally in the right to do what they did? I am not lawyer, but I am pretty certain that they are, and I think that this is what we can definitely agree on.

What it ""right"" from a moral standpoint? This is a much harder question to settle from my perspective. 

Generally, and I don't mean to connect this to their case, something like whistleblowing is often a legal violation, but feels like a moral imperative to a lot of people. History is littered with laws that were right at the time but appear morally wrong today. The point I am trying to make that these two dimension need not to be aligned in every case.

So, back to Timnit's case, I feel like there *might* have been a morally right reason for her to do what she did, but we don't know the full story. And frankly, everyone's moral compass is different. So what I am advocating for here is not to take her side of the story at face value, but also why should we give Google, a billion dollar company whose financial goals are not exactly aligned with questioning the ethical aspects of their business model, the benefit of the doubt? This is why I am surprised that nobody here seems to be willing to suspend final judgement on this case and live with the ambiguity.

&amp;#x200B;

&gt;This isn't about sex or race.

I wasn't trying to imply that she was fired based on that. Fact is, what you and I think about this doesn't matter because it was received by many people in this way, and that is the only point I was trying to make. It is something that is probably going to haunt Google for a while, whether it was justified or not.

I hope I could make my point a bit more clear, I feel like we probably have a lot more common ground than it originally seemed."
MachineLearning,"Idk the situation, but I would consider it ethical to download documents showing unethical behavior on Google’s part with the intent to leak them to the press. I have no idea if that’s the case or not, but if I came across damning documents at my company I’d think the right thing to do would be to leak them, not adhere to company policy."
MachineLearning,"And the most recent winner is... another meat eater. I'll grant you, it varies, but it looks 9/10 to me."
MachineLearning,[That is objectively wrong.](https://medium.com/four-pursuits-ventures/worlds-strongest-man-is-a-vegan-c2db543c62c8)
MachineLearning,"Ultimately, meat being necessary for peak fitness is undebatable. The world record holders are not vegans."
MachineLearning,"The main benefits of meat are high caloric density and Vitamin B12. Both can be found in healthy vegan diets. However in many cultures a vegan diet can lead to malnutrition; it's easier to mess up than traditional diets. But if you look at asia where over 50% of consumed meals are vegan and at large studies that show that Chinas shift to meat based diets brought with it the same health issues so prominent in the west, you can see a clear connection.

The problem is that the animal produce industry manipulates media and research funding using similar tricks as the tobacco and oil industry to control information flow. But if you look at WHO nutrition guidelines you see animal produce at almost the same importance as sugary desserts.


The fact that the optimal diet can strongly differ from person to person doesn't make it any easier."
MachineLearning,"If you load all the dependencies from a server that you maintain for the next 15 years - and use present versions of all software, you'll be set."
MachineLearning,"it is kinda ironic, she as ethical AI researcher but failed to apply basic ethic of the company's policy"
MachineLearning,"&gt;  “No Datacenter Deployment.

Yes?

And why do you think this is against deep learning?"
MachineLearning,"sorry i am just passing by to this sub, but what bitcoin mostly use instead of GPU then ?   
is that also applies to another crypto ? like ADA, MIOTA, TRON, STELLAR ? only ethereum using GPU ? isn't this is a pro for ethereum then ? since GPU is powerful and easy to get, no wonder ethereum becomes the #2nd"
MachineLearning,"Except for the health benefits of meat. It's better to function at peak capacity if you want to do good, including for the planet. You've got to wonder how many people will suddenly rediscover meat in pregnancy."
MachineLearning,"Except for the health benefits of meat. It's better to function at peak capacity if you want to do good, including for the planet. You've got to wonder how many people will suddenly rediscover meat in pregnancy."
MachineLearning,"If you also believe murder of humans is unethical and unpack why (they experience pain, they don't want to die so it's violating their free will, they are genetically similar to me [which cows and pigs are being similar mammals], alternatives are readily available) then it is clearly for the same reasons that eating meat is unethical."
MachineLearning,"I do not think so, no. Everything on Colab goes over the network. I’ve only loaded data through web apis, GitHub, and Google Drive."
MachineLearning,"I faced almost a same problem. What you do is that you append the facial landsmarks as another channel, i.e RGBL (L for landmarks), do your stuff (augmentations, resizing, cropping, etc) and afterwards split it back to RGB and L. Works like a charm."
MachineLearning,"&gt; And even if technically they violated company policies and that technically gives google the right to do things that they did, does that mean they did the right thing? 

Yes

If you threaten your company with resignation, they won't give in to your demands, they'll fire you. No need to even make the demand something outrageous, like ""name the people who rejected my paper so I can have them canceled""

And if you  move private company documents outside the company when you were specifically asked not to, and a lengthy internal investigation confirms this, then you will be fired. What do you want them to do, positively reinforce this kind of behavior.

&gt;did immense damage by sending a horrible signal to women and BIPOC in tech

This isn't about sex or race."
MachineLearning,"Are you expecting to update, like the other commenter was talking about? Because in that case I would just version freeze everything for the duration you needed to run. Or is this question about running continuously without memory leaks and other reasons for crashes?"
MachineLearning,That also sounds like a good explanation.
MachineLearning,"Could be. I am more convinced by "" The rightous mind"" explaination, that moral behaviour is the result of instincts, not reasoning, so ethics professor are simply better at justifying ad hoc the stuff they were emotionally guided to do anyway."
MachineLearning,"Cool. Any particular topics in differential geometry I should focus on? I know bits and pieces of arc length, curvature, surfaces and  vector calculus of R\^3 in the language of differential forms but nothing more."
MachineLearning,"Thanks a bunch for your suggestions. Will look into them.

By differential geometry, do you mean geometry of curves/surfaces or manifolds in general? Or maybe, I could look into a paper and learn the related math on the fly? 

I'll PM you for more details whenever required."
MachineLearning,"Given how dependent most ML repos are on very specific versions of libraries being installed, there's essentially zero chance you can go through a decade of updates without needing to make changes."
MachineLearning,"Congrats on getting a PhD offer! As many others said - supervisor is much more important than institution. Most of what you'll learn over the PhD is hard-to-define skills like organizing ideas and research taste which come from an apprenticeship with your advisor.

University brand is probably more important the further your career takes you from ML research. E.g., companies often won't know that X is a great prof, and will go by uni brand. Eh, you'll be fine either way.

Interesting bug with CS rankings: only looks at CS depts. But at lots of unis great ML work happens in statistics and engineering departments (in addition to applied stuff in other departments). More generically, you only need one research group at your uni, it doesn't matter that much if there are dozens of others. So long as your supervisor helps you, you can set up collaborations with labs at other unis. Zoom + Slack is really good!"
MachineLearning,"Phew so it seems like every one is picking a very extreme side in this debate, can I leave a comment here for everyone that feels torn? Timnit &amp; Margaret made very good points in their paper, but the internal handling by google seems more than dubious to me. And even if technically they violated company policies and that technically gives google the right to do things that they did, does that mean they did the right thing? I feel like nobody here will ever know the full truth, whether the re-telling of the story on Twitter is correct and that there was real injustice to be fought, even if it violated rules, or if it is all just drama and google was completely right in what they did, like so often, I expect the truth to be somewhere in between. I think the only only thing we should all be able to agree on is that it was a disastrous PR move for google and did immense damage by sending a horrible signal to women and BIPOC in tech. And now their research will also be viewed through whether you sympathize with Timnit or Mitchell although it makes points that are more relevant than ever to discuss; everything else should be viewed with a healthy degree of uncertainty."
MachineLearning,"This comment is just nonsense. Consumer is just one pillar of NVIDIAS wide portfolio (constantly shrinking in percentage).

I’m just buying professional grade NVIDIA cards and there is simply no shortage. Maybe the GEFORCE shortage is intended? Ever thought of that? Shortages justify higher prices (iPhone coincidence back in 2007/8/9???) ... maybe the current supply is their sweet spot? 

Running a semi conductor supply chain is very costly (worldwide networks). Sometimes building another wafer fab comes along with profit loss because of reduced overall utilisation.

Why does NVIDIA not simply double their production capacity to fulfil the high demand for gaming cards? Why does Lamborghini or Ferrari not do mass production? I smell coincidences."
MachineLearning,"Deep learning =/= data center deployment

Lots of places doing deep learning with one or two cards, or perhaps sharing a standalone server with 4-8 cards"
MachineLearning,"Well do you kill and eat an unconscious unfeeling organism or do you kill and eat a conscious animal that has feelings and feels pain. Given that many vegan diets are not only sustainable but healthier than diets with animal produce, the ethical and moral choice is by definition very clear."
MachineLearning,I read a study a while back that hypothesized that those that try to work on company ethics subconsciously feel like the have some sort of ethical credit or surplus and therefore allow themselves some exceptions they wouldn't otherwise do.
MachineLearning,Didn't they recently prohibit use of consumer cards for deep learning?
MachineLearning,"Thanks, edited to change to the more generic term. The underlying point that crypto miners are affecting supply remains, though, I think."
MachineLearning,Yes. I remember a post about the 3080 in this subreddit saying it was limited on a driver level.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Are you aware of how Bitcoin mining is achieved? Either with specialist ASIC systems or GPUs.
MachineLearning,"Your comment is not correct. Consumers is just one of NVIDIAS pillars.

Limiting the hash rate of their consumer graded cards would be a good move though."
MachineLearning,you are downvoted but correct. GPUs are used to mine ethereum mostly.
MachineLearning,"Most biased opinion as I'm one of the developers of skorch: at our company we use skorch often in conjunction with other sklearn models and pipelines, mainly for more classical tasks and less surprising model architectures (classification and your typical feed-forward architectures) but there were also projects that used novel approaches (e.g., deep neural decision trees, mean teacher, virtual adversarials) and projects that required very specific loss-setups and skorch was always flexible enough to not be in the way/be productive.

For myself I found that I used skorch for all my experiments eventually, be it DeepSpeech transfer learning experiments, reproducing the not-so-recent-anymore neural cellular automata or trying novel recurrent architectures which required investigating individual layer gradients and very deep introspection. In almost all cases skorch was very helpful to prevent shooting myself in the foot by strictly separating train/ and test-phases as well as having a proper logging infrastructure. One thing I personally did not try but others did was to implement GANs but my experience with models like mean teacher and VAT tell me that this would also not be too much fuzz to implement.

Of course ignite or lightning (and other libraries) will give you a similar experience but I found that having the sklearn toolkit at your side is beneficial in many cases."
MachineLearning,Bitcoin miners are decently NOT buying GPUs.
MachineLearning,Sarcasm?
MachineLearning,"NVidia is a consumer brand. If consumers can’t buy their hardware, they’ll buy it from a competitor. If they lose a generation war simply because of supply issues that’ll hurt them in future years. Bitcoin miners are not reoccurring customers with brand loyalty. Nvidia invest millions and millions in creating this loyalty."
MachineLearning,Why would they do that?
MachineLearning,"Yours is a good analysis and I'm so glad that you're one of the people who see Timnit as toxic because people like her are, no matter what others say about her. What I don't agree with you (even me as a progressive) is your preference for extreme left AI instead of extreme right. It'll never work because the extreme left is as hateful as right, just the groups they hate are different. 

We need a group where everyone has space, left leaning and right leaning. We need a group with Timnits and Pedros and people who hold neutral opinions about things. AI cannot lean towards any identify to be as ethical as possible.

This is why I think that what Google is doing is good. It's getting rid of extremely toxic people and probably making way for better people."
MachineLearning,The Nvidia deep learning components of the chip are very very different from the integer alu operations of ethash
MachineLearning,"x = x / 2225 \* 2048

y = y / 1555 \* 1024"
MachineLearning,"Just to add to the above comment: you should look up where their past students ended up (or ask if its hard to figure out), as well as try to figure out how many students ended up leaving/dropping out. The latter can honestly be tricky to ask about, so I would recommend asking to be put in touch with a current student and then asking them the more sensitive question (about people who have dropped out, transfer to totally different advisors, etc). This is a big decision &amp; time investment in your life, so you should really try to get a full sense of prior outcomes."
MachineLearning,Two hidden layers.  One hidden layer worked fine in the 90s.  It's with two hidden layers that all of the tricks like Adam or Glorot normalization became important.
MachineLearning,"What they did was ask ethics professors to rank actions by how ethical they are and then secretly measure if they behave accordingly. (It was from another, earlier study, not the one I cited)."
MachineLearning,"Maybe it's just my twitter, but almost none of the replies have replies. I've noticed this a lot lately, that replies have little to no engagement anymore. Unless it's just twitter's algorithm keeping all the spicy stuff out of my view."
MachineLearning,Genuine question but why is eating vegan unquestionably the ethical right choice?
MachineLearning,"I have no experience doing these types of machine learning projects, but I've seen people done similar stuff using Generative Adversarial Networks (GANs). Hope this can give you a direction."
MachineLearning,"try precision, recall, ROC curve to evaluate the performance of the models."
MachineLearning," I'm running a program that is going to take around 3 days to run, the temps of the CPU is around 75 degrees, do you think I should undervolt the CPU or something?"
MachineLearning,"I see a lot of people talking about removing those documents, but arguably, if the Gebru firing was unethical, there’s an argument Mitchell’s behavior was justified.  Ethically.  Obviously Google would fire her for doing it.  Doesn’t mean she was wrong

I’m also surprised so many people are giving google the benefit of the doubt on ethics. Like, Google doesn’t give a shot about “ethics”.  They care about money and reputation. This team was a PR move and now that the PR is bad they are trying to save their rep by finding “ethics” people who will praise them."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"maybe AI and ML will work fine in proper environments where everything have been studied and the predictions are some how high in accuracy ,  my real concern is how AI will act in unpredictable environments ( societies ) etc etc .
Since AI/ML is some how a replica of our understanding of things , and we as human tend to fight / destroy what we don’t understand or fit to our norms , then we study later after we fuck up .
I do believe AI should be tamed to specific tasks and not to be relied on in every thing .in the future"
MachineLearning,"I'm new to scikit-learn. I made a dataset of 5 points. so I want to use PolynomialFeatures to fit a single curve but it gives me multiple curves. can you help me with this?

Here is my code:

 

import numpy as np

import matplotlib. pyplot as plt

from sklearn.preprocessing import PolynomialFeatures

from sklearn.linear\_model import LinearRegression

x=np.random.normal(size=5)

y= 2.2 \* x - 1.1

y= y + np.random.normal(scale=3, size=y.shape)

x=x.reshape(-1,1)

preproc=PolynomialFeatures(degree=4)

x\_poly=preproc.fit\_transform(x)

x\_line=np.linspace(-2,2,100)

x\_line=x\_line.reshape(-1,1)

poly\_line=PolynomialFeatures(degree=4)

y\_line=poly\_line.fit\_transform(x\_line)

plt.plot(x,y,""bo"")

plt.plot(x\_line,y\_line,""r."")

&amp;#x200B;

&amp;#x200B;

I want a single fit curve but it gives me multiple fit curves. can anybody help me with this?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I think they already nerfed DL capabilities. I guess the algorithm involved between all these usages have pretty different computation set so they can detect the usage and add latency.

But I guess there are ways to hide the computation you're really doing or just to use / develop more open-source drivers. 

I don't know if someone has already been able to evaluate the impact of already existing restrictions on DL performances."
MachineLearning,Can't believe the Wednseday Frogs guy is into ML
MachineLearning,"Try Resized_landmark_point = resized_size ÷ original_size × original_landmark_point

E.g. if the landmark was at point x = 500 and the original x dimension was 1000, then if you resize it to 2000 the landmark x will be 2000 ÷ 1000 × 500 = 1000. Similar with y."
MachineLearning,It's ... a bit crazy that's the first time I read about this after following weeks of the drama ongoing on twitter.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Use remote ssh extension on vscode and execute your code directly on servers.
MachineLearning,Oh i thought you were gonna check for size
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"One big problem in my opinion is that we don’t have a body (locally or internationally) for issues concerning ethical AI. I don’t trust corporations and I think universities on their own are limited. But also believe that a big problem lies within governments not tacking AI seriously and them being technically behind. Unsurprising because many politicians, based on different hearings in the past such as the one with Mark Zuckerberg, have shown not be very technically literate. 

There have been blueprints on how to tackle AI and all its aspects, but it’s vague. We don’t need norms, we need rules."
MachineLearning,"I think universities are rather slow in brining Social Science and Humanities closer to a STEM and engineering audience. If I remember correctly, Harvard has done some steps in this direction adding ethical classes to CS undergrad courses, but otherwise hasn’t been much progress."
MachineLearning,"It doesn't :P

They are grouped as a single class."
MachineLearning,"Would be ironic, if this issue wasn't already researched topic). For example: 

# [The moral behavior of ethics professors: A replication-extension in German-speaking countries](https://www.tandfonline.com/doi/abs/10.1080/09515089.2019.1587912?journalCode=cphp20&amp;)

This is a recent paper but there are many more of these. And it's pretty clear that ethical researchers aren't any more moral than the rest of us. Except maybe for sub-catagories like eating vegan."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The authors already provided clean JAX code and even PyTorch code of the scaled weight standardization in their earlier paper, so this was rather easy to convert."
MachineLearning,"Happy for you! 

Apparently I was too dumb and I picked the exact advisor (type) that you neatly skipped.

&gt; super intense, required us to be in the lab 16 hours a day, and would nitpick the research/ final write ups of papers so much that it was hard to push stuff out.

Sounds exactly like my advisor :(. However, I am trying to stick with it... I am mid-way through and I am afraid of sunk costs."
MachineLearning,How are you guys able to convert scientific papers to code so fast. Make my imposter syndrome even worse.
MachineLearning,Man that twitter thread is bad. Everybody sympathising with her without knowing the reason she was fired.
MachineLearning,You can try [google ngrams](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html) as a heuristic as well. It has word counts computed from thousands of books.
MachineLearning,Great work!
MachineLearning,I have read two papers on zero-shot text classification. The summaries and papers are here: https://amitness.com/categories/#zero-shot-learning
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Her AI research may have been ethical but everything else about her certainly isn't.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I did a loan to buy a 2070. if you do research then you are paid and you can pay the loan. 😀
MachineLearning,"Good lord, the theatrics, lol."
MachineLearning,"Also in tech industry &amp; curious about the job.

\+ Technically, unless the data-driven approach could be interpretable, the morality thing is impractical. 

Example, how do they link the moral policy gradient setup to stochastic process burst for optimisation? 

\+ If the simulation is non-ergodic, how does she figure out the error bound?

Anyone?"
MachineLearning,"I like your comment. I have been a coward my whole life, and I believe it's time to take some risk. Thank you for your reply! Let's see how deep the rabbit hole goes."
MachineLearning,"I understand the risk of spending several years of your life pursuing a degree you dont feel so sure about (especially if you do it just for a job). But Reddit in general and this subreddit in particular is filled with kids who like to exaggerate a lot just to one-up each other. See any thread about renting prices (""Do you think 5000 a month is too much for a place? In my city you would be lucky to get a closet for that amount"" ""Really? A closet in my city is 10k/month a minimum and there is a 500 person waiting list for each one"".  Same with machine learning. ""My uncle told me a position was opened for an entry level data analyst, 40k a year, he received 500 applications, including 100  genius-level PhDs from top 10- programs each one with more than 10 papers accepted in main conferences"" Then later you see some kld newly hired in google with only 2 papers in which he switched 2 layers in a machine learning model or something.

Look, nobody is saying that saturation is not a risk and that there is no hyper-competitive spots, but you wont get far in life by being a coward. After graduation from an advanced  degree the success of a person is almost uncorrelated with the prestige of their program. Factors like hard word, luck, personality, perseverance are more important for a career. If you like machine learning go all in (especially if young) keep improving and trying and see what happens, that is the fun part in life."
MachineLearning,"Ethical researcher fired for unethical behavior, the irony! Keep up the good work Google."
MachineLearning,I understood him perfectly
MachineLearning,"People have tried selecting the samples that have the highest error, highest entropy, blablabla. Unfortunately no result seems to offer any real benefits when you scale it up to imagenet type datasets :("
MachineLearning,"skorch is amazing for early stage experimentation. Skorch will just remove so much boilerplate for training and testing models that there's no point not to use it early. 

If you have to productionize the model a specific way, you can always retrieve the trained neural network from the skorch wrapper and just use that."
MachineLearning,"It does, I think authors mean that self-attention models are just very big and that make large batches not workable at single GPU."
MachineLearning,It’s funny to see Twitter going crazy over this. All the celebrity tweeps are literally crying out without looking into details that she downloaded and moved thousands of documents outside the company clearly violating the company policy. She was put through proper inquiry and fired after a lot of discussions. She has very cleverly never revealed this in her tweets and simply playing victim of sexism. The whole episode is beyond disgusting given that these people were supposed to put an example of accountability and transparency.
MachineLearning,OpenReview
MachineLearning,"makes sense to me! Hopefully we manage to last that long before everything explodes... 

And yeah, even just as an semipassive stakeholder-- in as much my own work will be regulated-- I can partially sense how challenging and demanding this problem is. Looking forward to seeing bipartisan policy/legal nerds fill some of these roles soon, and not just ""overton window pushers"" (not to say they're not important.)"
MachineLearning,"Google has collected a very sensitive trove of information, I bet security is very serious. I am sure Mitchell didn't have access to personal information from user accounts, but their response reflects on the company's stance on security."
MachineLearning,Ensure that reviewers actually read the paper. Too many reviewers just don't put in the time for a proper review
MachineLearning,Yeah. People act like this is new. No this is corporate policy. It's a double edged sword. It comes with protections but it also means you can't willy nilly steal or leak proprietary information. I'm not on the side of big corporations here.  Just saying don't be surprised.
MachineLearning,So it is true! I see...
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Mods are censoring the below because it goes against the “AI ethics is good” narrative:

——

- 135,000 Google employees
- 2,000 signed a petition to support Timnit 
- 2 employees resigned to support Timnit 

When it comes to voting with your feet, no one actually gives a shit. 

https://www.cnbc.com/2020/12/04/thousands-petition-google-for-answers-on-timnit-gebru-departure.html

https://www.cnn.com/2021/02/04/tech/google-employees-quit/index.html"
MachineLearning,"I'm new to transformers. Is fine-tuning all you can do? I've looked at couple of models, it seems all the controls are limited to dataset you train the model on, and the hyperparameters you might tune to generate output.

And, how do you evaluate generative models? Other than eyeballing the output. I'm trying to generate phrases using quora questions pair."
MachineLearning,Care to elaborate?
MachineLearning,Already tried
MachineLearning,You can consider trying the approach discussed in this [link](https://stackoverflow.com/questions/45724955/find-new-coordinates-of-a-point-after-image-resize).
MachineLearning,Did any of you get a referral? My friend told me it's not possible.
MachineLearning,[removed]
MachineLearning,"&gt;Whose definition of fairness, or safety, are we going to use? 

This is a really good point and I'm really worried about it. Let's take NLP for examples. Recently there was a lot of criticism saying that models are biased because, for example, Man is to Doctor as Woman is to Nurse. Let's suppose this is bad and needs to be changed... But how? Would someone write some special texts, which will be gender-neutral? But what about, for example, non-binary and other groups of people? What about some other groups of people, for which models are biased - will there be attempts to create texts, which are ""correct""? Is it possible to balance texts for all groups of people?

What about the fact that in different societies different values are considered to be acceptable and unacceptable? What about the fact that these values change over time? Additional problems arise when the question at hand is political, ethical, legal, engineering and social problem at the same time.

And one more problem that currently it is really dangerous to argue with those, who speak loudly - because then you would be proclaimed to be a biased, unethical person, deserving to be criticized everywhere."
MachineLearning,Not working
MachineLearning,"Mods are censoring the below, despite 10+ upvotes on every thread:
——

- 135,000 Google employees
- 2,000 signed a petition to support Timnit [1]
- 2 employees resigned to support Timnit [2]

When it comes to voting with your feet, no one actually gives a shit.

[1] https://www.cnbc.com/2020/12/04/thousands-petition-google-for-answers-on-timnit-gebru-departure.html

[2] https://www.cnn.com/2021/02/04/tech/google-employees-quit/index.html"
MachineLearning,Resized_landmark_point = resized_size * original_size / original_landmark_point
MachineLearning,What a time to be alive 😉😉
MachineLearning,The rules section of Christoph Molnar's interpretable ml book is probably a good place to start! [https://christophm.github.io/interpretable-ml-book/rules.html](https://christophm.github.io/interpretable-ml-book/rules.html)
MachineLearning,"Conditional image synthesis like SPADE seems like a good place to start. You already did the hard part of segmenting instances of Poland ball characters. Now the question is whether 60k training data sufficient, especially given the variation you might observe between different styles of the same country."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm glad that there's still some semblance of sanity, human decency and proferssionalism still exists in this subreddit. Thank you all. Y'all still give me hope."
MachineLearning,"g00gle is reaping what it sows. You hire people based on identity politics and how good they are at whining and complaining and kicking up a fuss (ie 'AI ethics') they're going to be good at identity politics and whining and complaining and kicking up a fuss. When the bell rings and its time to get back to work they aren't going to suddenly switch expertise and passion to engineering good neural networks. 

This is 100% their fault."
MachineLearning,"imo the ecosystem has gotten to the point where any swe can pick it up, so it's the usual advice: only go for a phd if you actually want to do research for the rest of your life. you won't be unemployed, there are always postdoc positions"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you for your detailed answers! I think I'll have to go for a PhD. Thank you again for taking the time to answer my questions!
MachineLearning,You dropped this: /s
MachineLearning,"Frankly getting a good paper while not being in an institution is a little difficult, but there are a ton of jobs out there. Getting into a PhD program without publications is feasible, though, so if I were you I'd apply. Did your MS have a thesis/project portion? Have you read academic papers to apply in engineering? There are a lot of angles here that work."
MachineLearning,"I can't speak to this, but I don't think it's true. If a PhD or MS holder applies to a normal software engineer position, usually they just start a few rungs ahead, i.e. the equivalent of where you'd be if you had a BS + experience. My SO recently accepted a SE job at Amazon, and since she has an MS she's starting as SE 2 basically. There are also a ton of jobs for PhD holders. What matters most is what *you* want. If the research interests you, do the PhD. It's funded differently than undergrad anyways"
MachineLearning,"You do have a point that ML operates functionally differently than statistics. I imagine this would especially be pronounced in NLP.

The concept of language being alien seems a bit excessive however. As humans we have knowledge of our own language which we can use to make data more representative.

In your example you could sample verbs, adjectives, nouns, names, places, things, etc.

Is the concept here that we don't know enough about our language or models to know what we're looking for, thus we shouldn't make informed constructs like this?"
MachineLearning,I remember when Google lost 90% of its staff over the firing if the memo guy. It's amazing the company survived that loss.
MachineLearning,"Python is the de-facto automation language.  It's not going anywhere.  It has some of the best support out of any ecosystem around, a giant number of libraries and SDK tools, and works for almost anything you want out of the box with a single pip install.

It's also comparatively simple to develop in and has (arguably) the most intuitive syntax around (I'd arguably put it ahead of Ruby, despite the fact that Ruby was designed from the ground up to be intuitive).

Sure, C or GoLang is significantly faster, and Java can run on more stuff.

But you aren't going to beat the fact that you can import an AWS, Cisco, GitHub, or whatever SDK and start coding.

It also has a couple of pretty decent web frameworks in Flask and Django, even if Rails is more popular in the MVC space and Java Spring or EJB in enterprise."
MachineLearning,"Hey, thank you so much for your reply! I'm not entirely sure if I can get a scientific paper through industry work as the job offer I have accepted is definitely more focussed on using existing cutting edge algorithms to actually implement systems and get results. With that being said I would love any guidance or advice you have in terms of writing a research paper out of cutting edge work in industry even if the role isn't officially that of a ""research scientist"". 

With regards to understanding if I would enjoy research I believe I have a little bit of a chicken and egg problem. I feel like one more semester in my masters and I would have had the opportunity to apply and try out research however, now that it's over I'm not sure how I can get back the opportunity to experiment with research without applying to another Master's program or getting a research role at a company (99% of which I believe just go to PhD candidates). A second masters in both monetarily and logically infeasible according to me so I do feel a little bit stuck unless I can use my job to generate scientific research."
MachineLearning,Calling things virtue signaling is virtue signaling.
MachineLearning,"Ideally, conferences should hire people for this and overall cost should trickle down to all the authors throwing papers into the conference. It would get marginalized if the conference has sponsors anyways. 

Ideally if ""most"" conferences adopt such a model then the number of bad/unreproducible papers would reduce as it would harm a lab's reputation if they. constantly get reject from conferences because of such criteria."
MachineLearning,You clearly have your morals subject to political propaganda.
MachineLearning,[removed]
MachineLearning,Spot on. Whole thing is virtue signaling.
MachineLearning,"ML doesn't work like stats. Generally in stats you have an idea of what you're looking for, and you design your model to pick it up, like pointing a telescope at Jupiter. Great if it works for you.

ML -- at least in my area: NLP -- is more like being dropped on an alien world, putting microphones in all their cafes, and matching/repeating responses as best as you can to ""communicate"". When he grunts like that, I'm supposed to scratch my armpit. There's no way to know what the ""best"" datapoints are, you just vacuum up everything you can, throw a lot of computation at it, and pray. There's no ranking AFAIK in NLP."
MachineLearning,But doesn't employers consider phd as overqualification and prefer BS/BA degree holders?
MachineLearning,"With AI you're implicitly getting a CS/math degree too, and that's super useful for pivoting if you had to in the worst case. Software isn't going anywhere, and neither is general technical ability."
MachineLearning,Thank you for the reply. AI is a fantastic field to study but I am just worried it is too competitive to get a foot in the door and there might be another AI winter and I am left unemployed after paying tens of thousands of dollars for tuition.
MachineLearning,"Thanks for the help, and many thanks for the repo!"
MachineLearning,Valid. I work in ML in finance and occasionally med. There are meta studies indicating the majority of published findings in finance don't hold up. Would be nice to catch these.
MachineLearning,What about the research side? Will ML remain to be an interesting field to study 5+ years from now?
MachineLearning,"There's so much more to do, and it's not going away any time soon! Modern AI is very far from human intelligence, and covering that distance, in my opinion, is going to take fifty to a hundred years. However, there's a huge distinction between the engineering applications of ML and the scientific understanding and creation of human level intelligence. Just look at job numbers for ML on linkedin, for instance, or research funding numbers."
MachineLearning,"I will be messaging you in 1 month on [**2021-03-20 02:34:27 UTC**](http://www.wolframalpha.com/input/?i=2021-03-20%2002:34:27%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://np.reddit.com/r/MachineLearning/comments/lnmzv2/p_dataset_60k_labeled_polandball_characters/go2tkeq/?context=3)

[**CLICK THIS LINK**](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2Flnmzv2%2Fp_dataset_60k_labeled_polandball_characters%2Fgo2tkeq%2F%5D%0A%0ARemindMe%21%202021-03-20%2002%3A34%3A27%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%20lnmzv2)

*****

|[^(Info)](https://np.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://np.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|
|-|-|-|-|"
MachineLearning,!RemindMe 1 month
MachineLearning,"What do you think the field will look like in ten years? I am a freshman who got interested in machine learning. Will this field be ""solved"" by then? Or do you think what we have learned nowadays will be applicable to General AI like the research in 1960s finally blossomed in 2010s and 2020s?"
MachineLearning,Did google write this bot ?
MachineLearning,"It's being used so it's not hype. However, it will become like mathematics or programming where every engineer or scientist knows how to apply it. Expect ai to be a standard part of all science and engineering curriculum."
MachineLearning,Oh man how is it gonna tell between monaco and poland
MachineLearning,"You genuinely seem like a pretty cool guy, just wanted to acknowledge that"
MachineLearning,"There's massive demand on both sides! Getting high-quality talent is hard, and the field is exploding. Just focus on the content of your research, and individual positions you're applying to."
MachineLearning,"First of all, it's worth applying, and I think you could clarify your situation in your statement of purpose, but as an undergrad currently going through my PhD apps, it seems like research experience (especially papers) is the most important part of a PhD application. However, it seems like you have a lot of the other components. So I wouldn't be surprised if you could get in as-is, but I also would recommend taking a year to do anything resulting in a scientific paper (industry lab of some kind?), just to make sure you really enjoy research, since obviously a PhD is a huge commitment. Of course, take what I have to say with a grain of salt, but this comes from me seeing statistics on how rough PhD admissions are right now."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Do you think this field is still in its infancy? I am little worried this field is overhyped and when I finish all the formalities 10 years later, I might find myself unemployed and the trend shifted dramatically."
MachineLearning,"That is good on paper but doesn't work in practice. What is the chance the reviewer runs the code or does a code review? Near to Zero. If they are long-time academics, then there is an even smaller chance they will indulge in re-running the codebase or reading the code. 

The reviewing system should have ways to include ""Test Cases"" for reference executions."
MachineLearning,So do you think the phd is still worth the time if I want to do the application part? Do you think machine learning is still in its infancy and yet not saturated? I'm worried this might end like previous hypes
MachineLearning,Social science is full of crackpots and eventually they exasperate engineers with their progressive conspiracy theories. If this seems unlikely to you then you probably have not looked too deeply into their rabbit holes. The decolonizing math is one of my favorites because it so barking mad. It is kind of delightful to find serious academics this barking mad.
MachineLearning,Mandatory inclusion of data and code for reproducability
MachineLearning,"You can always become a mad computer scientist and work in the computer lab of your secret lair. 

Technically you can buy what amounts to a super computer compared to the early days of computing. There are plenty of abandoned efforts that might benefit from modern resources. I was reading an old book on genetic programming where they had to do things in assembly language given the slow computer systems in use back then. 

And there is plenty of exotic hardware you can buy from China. I bought a few boards using the Kendryte AI chips but I have no idea how to use them and few people are exploring the possibilities. Then there are field-programmable gate arrays allowing you to create your own circuitry if you know that advanced skill. 

But unless you are being paid to do whatever you like you probably don't have the time to dig this deep into the dusty corners of technology."
MachineLearning,"These examples are of people who are either setting up new divisions or leading large teams... and ""applying ML to research stocks"" is harder than you'd think lol. 

That being said, if you're exceptional you can still make it work. The top hedge funds are willing to hire B.S. candidates for quant research if they pass interviews, and I've seen a few BS/MS candidates in research roles at places like DeepMind/Google"
MachineLearning,"Even if you go directly into industry in an applied role, you'll still be competing with PhD's/experienced people for machine learning positions.  In terms of skill sets, what these companies probably need more of are (talented) data engineers and machine learning engineers to deal with all the domain specific data challenges that exist in their areas."
MachineLearning,"I refused to join the top schools you listed to attend another school that is also well known for ML research and has excellent faculties in my fields (applied ML fields) whose research interests align well with mine.

As others mentioned, research interest matching with your advisors and the surrounding research environments are core factors for your Ph.D. success, unless you are already so experienced that you do not need any help from your advisors to publish papers at top-tier conferences. Going to those top schools does not immediately guarantee that you would have the environment you could flourish. Some people in those schools struggle due to the differences in the research interests/advising style of their advisors or some other factors, and haven't published a single paper since the beginning of their Ph.D.

In the research field, your publication records do matter (way more than the school names unlike new grad SWE jobs), so I think the most important factor would be on which school you could maximize your research outcomes. I see many Ph.D. graduates from the group got awesome positions both in academia and industry, as they've been constantly publishing good papers in their focused field. School names may help during the initial screening process, but as long as the school is ranked in the top 15 in CS and is well known by the researchers in your community, I think that should be fine.

I would not deny the aspect that there are some benefits of enrolling the prestigious schools (e.g., school names do help when you need to communicate people in non-CS or non-research fields / there are strong alumni communities and the network / prestigious schools often have better funding or collaboration situations / your peer students are also really talented and you would feel good peer pressures from them that sometimes push you to work even harder... etc) Yet, it seems that you care more about how successful you could be as a researcher, and those factors may not play major roles for the purpose."
MachineLearning,"I've been reviewed and done reviews.  I'm sure it would be too much trouble and slow the publication process, but a ""Peer Review"" needs to be more like a ""Thesis Defense"", with interractive q&amp;a and discussion, than anonymous people (occasionally you can figure out who they are) taking pot-shots at, or supporting, your work (occasionally for purely political reasons)."
MachineLearning,"&gt; What about not the top ML research jobs in FAANG but other companies or fields like graphics or stocks?

Hedge fund hires U Washington professor:

https://medium.com/syncedreview/pedro-domingo-will-lead-new-d-e-shaw-machine-learning-group-3c722e41aafc

Adobe hires Stanford professor for PhotoShop AI

https://9to5google.com/2020/07/20/pixel-marc-levoy-adobe/


If you’re okay with being a data engineer working with CSV files and SQL, then yes you can work on “applied ML”"
MachineLearning,Not have Peers.
MachineLearning,"$40k in living expenses? Unless OP has a family to support, that's insanely high for a student. Living off campus, having flatmates would cost half that amount imo."
MachineLearning,Is the school Umass Amherst?
MachineLearning,Is it necessary to be published to get a good MLE/research scientist job?
MachineLearning,You keep posting the same thing in different threads
MachineLearning,"You sign a contract there are company policies in place, you can't just leak documents and breach the confidentiality of the company. That's how corporations work, they protect their own interest and have the right to terminate contacts"
MachineLearning,"It's a corporation with many cogs, they don't have to meet with ""demands"" of former employees there's a wide talent pool for them to reach out on"
MachineLearning,What methods did you use for segmentation and grouping by flag? What was the training process like?
MachineLearning,"Google response (from Jan):

""Our security systems automatically lock an employee’s corporate account when they detect that the account is at risk of compromise due to credential problems or when an automated rule involving the handling of sensitive data has been triggered. In this instance, yesterday our systems detected that an account had exfiltrated thousands of files and shared them with multiple external accounts. We explained this to the employee earlier today. We are actively investigating this matter as part of standard procedures to gather additional details.”

[https://venturebeat.com/2021/01/20/google-targets-ai-ethics-lead-margaret-mitchell-after-firing-timnit-gebru/](https://venturebeat.com/2021/01/20/google-targets-ai-ethics-lead-margaret-mitchell-after-firing-timnit-gebru/)"
MachineLearning,Guess that means my CNN and CNBC links are fake...
MachineLearning,Any good intro ref to look into this method? Looks really interesting.
MachineLearning,Wtf... What about not the top ML research jobs in FAANG but other companies or fields like graphics or stocks? Isn't there some companies or universities that are interested in applying machine learning technology to better the existing softwares like Blender/photoshop etc or make new one or apply ML to research stocks?
MachineLearning,"I believe all companies like Google will always try to use user data (the legally accessible part at least) to make money. I agree that they are evil and all...  
However, I think you are mixing privately stored data and publicly accessible content that violates the term and conditions of these cloud providers (Parler case).  
Also, with all my respect, I think you are influenced by some of the conspiracy theories. I hope you don't also believe COVID was developed to control people and 5G is causing it?"
MachineLearning,"Yes it's simply something I find interesting. I like drawing Polandball comics and I think treating it as a data science problem is fun.

One aspect is that /r/polandball has very strict rules for drawing comics, something which is more chaotic outside of Reddit. Automating drawing could help some people in comic making and could improve the horrible auto-generated flags which are shown on the [Polandball Wiki website](https://polandball.fandom.com/wiki/Polandball_Wikia) for certain countries.

It is mostly beneficial for complex flags which can take close to hours to draw."
MachineLearning,pls post paper when done
MachineLearning,Agree. I just wanted to emphasize that NOT getting an ML job doesn't mean your bad at research.
MachineLearning,"Thanks for the reference. Yeah at it's core it's a way to transport or warp one distribution into another. So it has ties to differential geometry given that you want your transformation smooth and differentiable. Looking into diffgeo is a can of worms though but partially necessary. I would recommend looking into function registration using square root velocity functions which is like a 1d version of optimal transport. Youssef marzouk's team at MIT also has papers on it. Look for the main one. It has a gentler intro.

Just an FYI. Even for me and I've been doing ML and math for years, it's pretty technical. PM if you need more help."
MachineLearning,"**ig nobel contender, this should be.** 

*-lunaticneko*

***



^(Commands: 'opt out', 'delete')"
MachineLearning,This should be Ig Nobel contender.
MachineLearning,"I'd like to rephrase this and ask why are you choosing to do this project, targeting Polandballs, instead of, I don't know, other memes? Is it because you find Polandballs funny? You want to see them into space?"
MachineLearning,Isn't this an argument to use the cloud indeed? It is quicker and more efficient to train multiple variations of your model in parallel in the cloud and have the results really quickly. You can't do this locally on a single GPU since you need to run them one after the other...
MachineLearning,Do you think machine learning phd is still worth the immense competition if I am interested in experimenting with the possibilities of applications of machine learning technologies?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You'll have to wait this time around with all the action just starting
MachineLearning,"It’s being commoditized. If you want to research, go to a school for a research heavy PhD in artificial intelligence or related. Another year or two and it’ll be pretty basic skill set just to implement an existing architecture."
MachineLearning,"Not CycleGAN in its original form if I want it to be conditional on a specific flag.

But I am trying to incorporate a GAN into the design."
MachineLearning,I guess you could use cyclegan to convert flags to polandballs and visa versa
MachineLearning,"I think the second point you brought up (more social science expertise contributions) will become better over time as computational social science and digital humanities education becomes more popularized and we have more researchers whose undergrad and graduate education is in CS+Social Science/Humanities.

One challenge with implementing responsible AI practices in production environment is that these issues lie at the intersection of law, policy, data, and engineering. Right now, there are few people who can serve as the bridge between these areas, but I think a new job role that serves as the bridge between law and data/engineering (something like an AI Red team mixed with a risk/compliance officer) will arise in the next few years."
MachineLearning,"It's on the right track, there's a visual overview [here](https://dsp.stackexchange.com/a/71399/50076).

Speed depends on number of samples; on my machine, 1 sec: (10kHz, 1.8sec), (100kHz, 39 sec). Implementation is vectorized and JIT-compiled, scaling well with size."
MachineLearning,11 hour old account...
MachineLearning,"Eh, just because someone didn't resign or didn't sign the petition doesn't mean they don't give a shit though! 

I, for one, do think AI ethics is important. It's not what I personally want to work on, but I sure do wish we had people working on it that are representative of current society."
MachineLearning,You got it. S(t) will give you the proportion of survivors. You can also get hazard from it. This will give you the probability of action in the next time period as a factor of regressors.
MachineLearning,You got it. S(t) will give you the proportion of survivors. You can also get hazard from it. This will give you the probability of action in the next time period as a factor of regressors.
MachineLearning,"
- 135,000 Google employees
- 2,000 signed a petition to support Timnit
- 2 employees resigned to support Timnit 

When it comes to voting with your feet, no one actually gives a shit about Timnit or AI ethics. 

https://www.cnn.com/2021/02/04/tech/google-employees-quit/index.html

https://www.cnbc.com/2020/12/04/thousands-petition-google-for-answers-on-timnit-gebru-departure.html"
MachineLearning,"No, you cannot use MAPE, because the normal distribution is not concerned with the average absolute deviation from the correct value. It is concerned with the squared difference summed (and some other terms). You have to look at the log-likelihood itself to make these determinations."
MachineLearning,"Thank you for your response, you bring up some great points!

You're definitely right about csrankings. It's a useful tool overall, but the ""faculty can solely advise a CS PhD student"" can really throw it off. Two of the programs I applied to where I actually felt I connected best with professors were in ECE rather than CS. I was confused as to why the professors there who publish a lot in ML conferences weren't on csrankings until I figured out that they didn't have affiliate appointments in the CS department. I'm glad I realized this, because it would have led to me not applying to departments that actually have lots of people doing what I want to do.

Your point about the PI being more important in than the school in terms of both my research output and career prospects is also reassuring to me! My number one goal is definitely to build a successful career in ML theory research, not to work at a top tech company (if I can do that work at a tech company, great, but I would much rather be a researcher at a smaller institution than a software engineer at Amazon). Since that's the case, it seems like as long as I'm with a professor who's doing the kind of research I want to do and publishing in those venues, and whose students have gone on to the types of jobs/postdocs I want, I would be in a good position."
MachineLearning,"&gt; We could use schema.org as Google Dataset Search does, but from our initial experiments its usage on official dataset websites is not that common.

If you joined GDS in using it, that would help make it more common practice, I'd point out."
MachineLearning,why the fuck not!
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"In general, I only see about 5% of resumes with anything substantial so they are already doing better than average

Also most of the applicants are international and I have no clue of their reputation nor do I care to look them up so I feel this levels the playing field a bit better"
MachineLearning,"It is the experience of every PhD student who wants to make significant contributions. Just to give an example Yoshua Bengio’s university of montreal is not even in Top 40 or 50. Similarly Yann lecun got his phd from France from similarly ranked university. It is not about the university, don’t make this mistake. Good luck !!"
MachineLearning,"Is there anywhere I can read a rundown of the recent history here? I’m familiar with Gebru’s firing story, but not with this situation. 

Scrolling through that Twitter thread I did not find much."
MachineLearning,First Timnit now this.   Does anybody know why?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Holy shit, I never would've thought I'd see zimonitrome posting on a machine learning subreddit."
MachineLearning,"Person posting above is embarrassingly wrong to the point that they haven't done a basic Linkedin search to check and see if they're wrong.

&amp;#x200B;

Here, just scroll through this.

&amp;#x200B;

[https://www.linkedin.com/search/results/people/?keywords=applied%20scientist&amp;origin=SWITCH\_SEARCH\_VERTICAL](https://www.linkedin.com/search/results/people/?keywords=applied%20scientist&amp;origin=SWITCH_SEARCH_VERTICAL)

&amp;#x200B;

You'll see a LOT of people from 30s-ish schools. And the rule of thumb I've seen is to subtract a little rank if they're not a CS PhD since math/stats/etc. are less competitive.

&amp;#x200B;

just look at Amazon and Microsoft BTW. Since those are top tier."
MachineLearning,"If you are asking for this project in particular, then yes.

I am fairly new to Kaggle so let me know if there are any permission issues."
MachineLearning,"I took a grad survival class sometime in the past but didn't retain much. I use some type of method to evaluate S(t), and then I use that distribution to make predictions? I'm fuzzy about how the outputs of survival analysis are used."
MachineLearning,"Go where you mesh the best with the advisor! 

I'm a current Ph.D. student - I got into a fancy school (It might have been that MIT place) that had a decent stipend but the PI was INTENSE, and a less fancy school and I picked the less fancy school. Wouldn't give it up for the world. 

My advisor has let me basically try whatever I want, is super supportive, has me already writing grants/ mentoring people, is ok letting me take months at a time off for family reasons, and strangely enough has provided a better environment for me to publish like crazy! By coming here I've been able to lay down a very strong path towards a faculty position, and the school really is interested in keeping me on! My PI also lets me work on the side, so I've been able to continue to make actual money at a data science job.

Now the other advisor.... well I worked with them prior to committing to my school (I volunteered with the PI's  to test it out because I was stressed about this decision too) and found they were super intense, required us to be in the lab 16 hours a day, and would nitpick the research/ final write ups of papers so much that it was hard to push stuff out. The other PI was also notorious for keeping Ph.D.'s on forever. I can't stress it enough... go with the advisor you mesh with!"
MachineLearning,"https://github.com/zimonitrome/polandball-flag-mapping

I am working on a method to automatically combine flag and outline into Polandball characters that adhere to /r/polandball rules.

But yet again, why?

It is fun."
MachineLearning,"Got it, there's sort of a PhD track and a non PhD track. I ask because I just landed my first ML gig at a funded startup and I'm wondering how best I can climb the ML ladder. I actually have 0 proper academic background (B.S. materials science plus ran my own startup) so maybe the ""normal"" route is not optimal for me."
MachineLearning,Ok... But why?
MachineLearning,"60,000 That's like... Three times too many countries."
MachineLearning,Are you taking about UMD?
MachineLearning,thank you for sharing
MachineLearning,thank you for sharing
MachineLearning,"hehehe, awesome dude! thanks"
MachineLearning,"Good work, and ambitious. What’s your next step?

Before putting too much time/money into this, maybe check if lack of expert coaching is a bottleneck in the fitness world. I suspect it might not be (but I’ve been wrong a couple times in life!)"
MachineLearning,"Yeah, so can someone explain to me why doesn't the GPU memory scale linearly with the batch size? Or am I just stupid and it does?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt; (Can you guess which school I'm talking about yet?) 

University of Wisconsin?

Anyways, some thoughts:

1. With regards to rankings, do you truly know what the ""most prestigious"" truly are?  It appears you're going off of csrankings which has its drawbacks.  For instance, ML isn't contained to ""faculty who can solely advise a cs graduate student"".  ML theory is optimization (read: industrial engineering, math, etc.), statistical learning (stats), and so on.  Electrical &amp; Computer Engineering perhaps somewhat surprisingly contributes to a significant amount of the ML theory.  Why? Because the language of signal/image processing, compressed sensing, information theory, etc. is the language of ML.  Some of the most prominent ML researchers at Michigan, Georgia Tech, Wisconsin, and many others are in ECE and don't even show up on these rankings despite routinely publishing ICML and NeurIPS papers.
2. It probably stands to reason that wherever your PI's students go, you could go to somewhere similar.  If 90% of your PI's students are at FAANG, there's probably something at play (e.g. directly relevant research, connections, etc.) that would also work in your favor.  If a large contingent go into academia, clearly publishing research of similar quality/frequency to others in your lab should make you competitive for faculty positions/post-docs
3. If your concern is being able to work on fundamental ML research (as opposed to something very applied or more akin to software engineering), then honestly I think your research output is the #1 factor (which, all things considered, is more of a function of PI than school; Professor X is more responsible for your research training than University of Y).  Good research is good research.  And when it comes to theory, someone competent in the field can easily recognize it.  And for a research role, this person will be the person the hiring you.
4. Oddly enough, the further you get from a research job, the more your school may matter.  Looking for a software engineering role?  Well your school's career fair is probably going to come to the forefront at which point you're dependent on who comes to campus.  A complete 180 to management/strategy consulting?  McKinsey pitches ""MIT PhD"" to their clients.
5. All of this said, it sounds like you're going to a reasonably good school and based on the fact you're focused on ""pursuing a career in ML theory research"" as opposed to ""working at Google Brain"", I am fairly confident that the door is wide open."
MachineLearning,What's the Polandball character for Kaggle?
MachineLearning,Best of luck with your PhD!
MachineLearning,"Upvoted for absurdity.  


Polan cannot into global optimum"
MachineLearning,"Wow, that's really interesting. Considering what it takes to get into those schools I wonder if the majority feel like they've ""finally arrived"" once they get to college. This is different from my experience, but most of my friends were pre-med so everyone was pretty much forced to continue doing extracurriculars in order to be competitive applicants to medical school."
MachineLearning,Gold.
MachineLearning,"Yeah they're pretty competitive so even if you're good enough to get in they only have a handful of slots each year so they have to reject someone. But if you're in a top 10 school don't worry, you're just fine.  People I work with did their PHDs at more like a top 200 kind of school and they still managed to get a post-doc at places like Harvard, MIT, etc. afterwards.  

Main thing is regardless of where you go, don't let off the gas peddle and you should turn out fine. You clearly have the ability, you just need to maintain it."
MachineLearning,Finally!
MachineLearning,Thank you! I'm so glad to hear that this was your experience and you ended up happy with your supervisor and your decision!
MachineLearning,"I think this makes sense. I'm sure prestige counts for something, but people do overcome it. Besides, the place I'm talking about is still top 10-15 in the field, which is very good in the grand scheme of things...it's not as if I would be coming out of a total no-name place, just not somewhere that has the name recognition of the very elite."
MachineLearning,"It's free to create a small competition, but if you want to make a competition that is on the public Kaggle competition page, you need to be an organization"
MachineLearning,"Well, I'm a 4.0 with ""extras"" and I don't think I'm getting into one 😂 At least not one of the Stanford/CMU/MIT trio. I managed a top 10-15 school, but I don't think I can crack the top 5. If a surprise offer from one of them comes in last minute, I might take it, but I'm not expecting anything. But wherever I go, I'll make the most of it! You're totally right that many of the postdocs at great places have ""leveled up"" from the place they did their PhD."
MachineLearning,"Its good to be aware of the effect of prestige and its implications. You can still make a name for yourself if you're good. That said, ive seen people who completely downplay this and seem super naive/ignorant. Prestige matters but its not everything. Fight with what you have and make the best out of it."
MachineLearning,Is it free to create a competition on Kaggle?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Yes, I'm aware. That's why I said I'm expecting rejections. I have already done virtual visit days for three programs, gotten acceptances from two, and I'm expecting the rest will be rejections or maybe a waitlist.

Of course prestige is a factor, but surely it's not the only factor? Is somebody coming from a top 10-15 PhD program with a good resume really going to be so screwed that they can't get a research scientist role, an academic job or even a postdoc at a good institution? This just doesn't seem right to me. I know it's competitive, but not everyone in those positions can possibly have a PhD from Stanford/CMU/MIT. In researching graduate PIs, sure, many of them came from those places, but many of them didn't. Have things really changed so much in the last 2-3 years?"
MachineLearning,"PhD applications can be stressful, but you got in to a good school. Attending a school in the top 10-15 in the world for your field will not lower your marketability or opportunities. Take a step back, breathe, and be happy at your good fortune."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your videos are so regular and quick, I sometimes think you are a bot"
MachineLearning,"Very nice!

However. You say "" 1NN-DTW \[3\] is the accepted baseline in the time series classification community. Though this method suffers from high running time.. "".  You then report taking 27 minutes training time for 1NNDTW!

Using lower bounding and early abandoning (decade old tricks), you can reduced this to well under 10 seconds. [https://www.cs.unm.edu/\~mueen/DTW.pdf](https://www.cs.unm.edu/~mueen/DTW.pdf)

In addition, you can make  1NNDTW more accurate, here is is suffering from the need for prefix-suffix invariance , and the need for warping constraint (again [https://www.cs.unm.edu/\~mueen/DTW.pdf](https://www.cs.unm.edu/~mueen/DTW.pdf))"
MachineLearning,I really want to combine the not so popular privacy techniques to some popular GANs...for now
MachineLearning,"Hey, thanks for the interest! You should definitely be able to do this without much work. A `freewire.Model` is designed to behave like any PyTorch module, so you could incorporate it into i.e. a vision network. It just expects that the input is flat, shaped \[batch\_size, input\_dim\]. It transforms the input and carries out the freewire ops using the `tape` attribute, which is set up lazily the first time you call the forward pass.

This is some old code of mine and I wish I could point you towards something newer and less experimental, but I'm not familiar with any projects that have this specific utility. [This](https://github.com/google/brain-tokyo-workshop/tree/master/WANNRelease) is the project that inspired my repo."
MachineLearning,"Ensembles of other models are probably less interesting to compare to, but that depends on the complexity of the underlying model.

But ideally an AutoML offer should beat a single tuned model which uses only basic, ""mindless"" encoding. Because the latter could be compiled to something like a simple AutoML function itself. The linked notebook uses a packaged tuner, too.

Consider bagging the final model like the notebooks, if you haven't done that yet."
MachineLearning,I can't answer your ambiguous and borderline nonsensical question.
MachineLearning,"What is the best way of building a database for efficient/effective ML? Which programming language/software? Like, what would large scale industries to e.g., report details of their operations. When the scales are so big that excel documents wouldn't be feasible (to say the least))."
MachineLearning,"you would run \`ffmpeg\` from the command line on your computer after you download and extract the zipped up images locally.  If you don't have ffmpeg on your machine (comes on most linux and macs at this point I think), you should also be able to run ffmpeg on the collab too, by copy pasting that line and adding it in a new cell with and ! in front.

Then it just takes a long time to download the full mp4 in my experience."
MachineLearning,"So you went out into the world, got some real world work experience, made some money, and then got hired by a top tech company. Why is that bad?  Seems like a very solid career path. 

Who cares if you get hired directly or indirectly?  It only amounts to bragging rights, but at the end of the day if you're goal is to work at a FAANG then it only matters if you achieve that goal right?"
MachineLearning,"It looks like synchrosqueeze is good at narrowing the broad bands that result from the FFT time-freq tradeoff. Is that simple explanation on the right track?

Also, how long does this take to calculate for a 1-second audio clip on a modern CPU?"
MachineLearning,"&gt; So much of he hiring process neglects to capture any long term potential in candidates. 

Why hire someone based on long-term potential?

When there exists another candidate with that same long-term potential + realized potential through existing accomplishments?

Working hard does not entitle you to a ML job."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"From what I've checked the notebooks are tuning single model or running ensembles of several models previously tuned.

I will try to simply run the AutoML for a longer time period locally. The results should improve."
MachineLearning,Cascade correlation networks are making a comeback?
MachineLearning,"Oversampling usually can be done by replicating the rows or using SMOTE. If the class difference is huge then SMOTE will fail to capture the pattern in minority class. So I would suggest you to use undersampling technique in this case, like take a sample from the majority class."
MachineLearning,"Yeah it's this, on github links the first image it finds appears to be the github user's profile pic"
MachineLearning,"Thanks! Hear hear for Common Voice. Yes, I'm a member of the SpeechBrain team, and there's going to be recipes in SpeechBrain for training models on this dataset!"
MachineLearning,Done! I've helped out Mozilla Common Voice before (and I also work with ASR) so happy to help out. Are you involved in SpeechBrain?
MachineLearning,"Thanks for sharing this is super cool!  


I've gotten to step (9) and am not very familiar with ffmpeg. While I ran ever other step from the Google Collabatory, where do I run `ffmpeg -framerate 10 -i story_hallucinator.%d.png -c:v libx264 -crf 0 story_hallucination.mp4` ?  


Thanks in advance!"
MachineLearning,"It would be best to beat all public notebooks which use only standard operations (e.g. encoding, tuning a GBDT) and which do not use any elaborate insights into the data. Is that the case? I haven't checked the content, but some notebooks claim to be simple."
MachineLearning,"Awesome - thanks! I'm working up something over the next few days that should use the same process. Hopefully I can get that to work, &amp; if so I'll go back and update the old code where I was making all those unnecessary copies! Thanks again for the guidance!"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I would continue to pursue that masters if it’s something you love. There are so many jobs hiring, although the competition is fierce. A masters is great, but the traits that are more important (in my opinion) are your passion for the field and relevant experience you bring with you. I’m a college dropout and I’m working on a data science team at a respected company. I skipped out on formal education, but I back it up with years of doing projects and a huge amount of self education. It is the harder route, but I made it a point and statement to prove that it’s possible, with discipline. If I can make it in the field, then you certainly can with a masters!"
MachineLearning,"You don't have to believe in me, I don't have the source for that specific case. But regarding the general situation, I sent one link, and there are multiple others."
MachineLearning,"&gt;That's a ~~dick~~ evil move.

fTFY

To be fair, this is basically the final stage of google becoming Microsoft"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Will check it out. Thanks!
MachineLearning,[removed]
MachineLearning,"I'm the furthest thing from a Google fanboy, but I'm not sure how credible that is. There's zero evidence, someone literally could have made it up and we'd be none the wiser.

I'm more inclined to believe the related issue (the removal of the video trailer), but even that could equally be explained by exceeding quota limits or disabling of public sharing."
MachineLearning,"Thank you, that was a really helpful way to put it."
MachineLearning,"\&gt; you think you can get "" highly skilled research scientist with a background in materials development and electrochemistry. "" without going to a grad school?

It's not the common path to expertise, sure, but I think it's definitely possible.  Many famous programmers and quite a few famous ML researchers did not study ML or programming in school.

The way I interpret Elon Musk's statement is that education is nice and all, but what matters is whether you do well on the interview.  Whether you know your stuff.  Whether you can get the job done.  And that individuals who don't have traditional backgrounds but have gained tremendous expertise anyway deserve to be evaluated on the basis of their skills, and not their background.

This approach feels right to me -- and I expect, all things being equal, that companies that follow it to outperform companies that do not.  This may help explain (in a small part) the staggering success of Musks's companies."
MachineLearning,"In the case of binary input variables, it is very similar to logistic regression. Otherwise it uses rules to discretize different parts of the input space before combining them to make a final prediction."
MachineLearning,Why’s there a pic of naruto lol
MachineLearning,My own research is geared towards smaller models. But I didn't know it was a trend. Do you have some articles or posts about this? :)
MachineLearning,"I only knew of one case (directly from the researcher who had her newly written article disappearing from her own google drive), but a quick research showed the policy in a more general fashion:

[https://iharare.com/google-drive-now-taking-down-users-personal-files-covid-19/](https://iharare.com/google-drive-now-taking-down-users-personal-files-covid-19/)

It is important to notice that the researcher I saw talking about this had not yet published the article. That is, google employers were looking into her personal stuff, found the article, read it, and decided to delete it. Regarding this part, I can't remember the sources though. Anyway, Snowden frequently talks about this mode of privacy intrusion by both the government and big tech."
MachineLearning,"now day business transformation using digital technologies is a long haul, executing the program without any plan can be detrimental to business, as this involves people along with model and culture. And, partnering with reliable [digital transformation services](https://www.hakunamatatatech.com/our-services/digital-transformation/) is highly recommended for success."
MachineLearning,I have an RTX3090 and it trains imagenet stupidly quick. 1 day sounds about right
MachineLearning,"&gt;it has messed with and deleted a researcher's paper in her own google drive, because the paper she wrote had some implications regarding covid policies),

Source?"
MachineLearning,I disagree. Have you ever worked in a small shop? Do you have any work experience outside of academics and internships? There’s a lot of big data platforms that have PBs of data sitting in storage. Real easy to build simple models off that data and gain value from it. Time series anomaly detection is a good example of this
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"My GTX 1080i can train a FastSpeech model in a couple of hours. 

There's definitely still a place for workstation GPUs. The current focus seems to be shifting towards smaller models, too, so I think that place will only increase."
MachineLearning,"I discovered your videos recently \~ 2 weeks ago, and i must they are such a good material to learn and discover, thank you for your work

Btw the rate at which you are producing those videos is almost alarming 😂"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,I'm on a different hemisphere but I can confirm most of my friends got their jobs through referrals.
MachineLearning,"I *think* that Ray flow should work pretty smoothly. If you're sticking with the direct usage of NumPy, h5py, and Ray, let me know how well that works!

If you're not too deep into the project, Dask seems like a great fit for your use case. It provides the same NumPy or Pandas APIs that we're all used to, and automatically parallelizes those operations over your data where possible; you should be able to get not only parallel reading from HDF5 files but also parallel row-wise statistics calculation (where parallelism is possible), all using high-level, parallelism-agnostic APIs. The meat of it could be done in a few lines of code I think!"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Title:LambdaNetworks: Modeling Long-Range Interactions Without Attention  

Authors:[Irwan Bello](https://arxiv.org/search/cs?searchtype=author&amp;query=Bello%2C+I)  

&gt; Abstract: We present lambda layers -- an alternative framework to self- attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.  

[PDF Link](https://arxiv.org/pdf/2102.08602) | [Landing Page](https://arxiv.org/abs/2102.08602) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.08602/)"
MachineLearning,"You need a book on speech signal processing. Here are a few:

[https://www.amazon.com/Speech-Audio-Signal-Processing-Perception/dp/0470195363/](https://www.amazon.com/Speech-Audio-Signal-Processing-Perception/dp/0470195363/)

[https://www.amazon.com/Speech-Communications-Machine-Douglas-OShaughnessy/dp/0780334493/](https://www.amazon.com/Speech-Communications-Machine-Douglas-OShaughnessy/dp/0780334493/)

[https://www.amazon.com/Theory-Applications-Digital-Speech-Processing-ebook/dp/B008VIXWYY/](https://www.amazon.com/Theory-Applications-Digital-Speech-Processing-ebook/dp/B008VIXWYY/)

Also see the ""Speech analysis"" links on the Praat website, halfway down the page on the left, [https://www.fon.hum.uva.nl/praat/](https://www.fon.hum.uva.nl/praat/)"
MachineLearning,"Just finished training. I trained it for 4 hours on Kaggle kernel and got 264th place / 989 teams. The link to kernel https://www.kaggle.com/mt77pp/mljar-automl-tabular-playground-feb-2021 I just used raw data, with no tricks"
MachineLearning,"No, the main problem is that most ML tasks in the industry either require vast amounts of data and compute power, which smaller companies don't have, or are trivial enough to be done by one or two data scientists with Scikit Learn.

ML is inherently risky, since you don't know in advance whether a bigger model will give more useful predictions than something much simpler. Unless a company is already big and profitable, that risk often isn't worth paying another salary for an ML researcher."
MachineLearning,That’s what I’m tinkering with just now but doesn’t seem to be doing much for me.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Have you looked at imbalanced-learn? Interfaces with the scikit learn api so nicely
MachineLearning,"From what I read in the comments, I could be forgiven for thinking ML masters and PhD is actually a proper bad idea right now. It seems like every stem graduate who’s not sure what to do tries to break into it and its importance to actual companies that exist and will hire in the real world is limited outside of about 10 big names that everyone tries to join. Do correct me if I’m wrong"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm not the dev either, but I was reading the repo a bit and I don't see why you couldn't.I would probably be inclined to try and inherit the model class, add my convolutional layer, and override the forward class so the CNN runs first on x, and it's output is fed into the tape. That might be sufficient. 

..or, if you want to get hardcore, you could write a function to automatically produce convolutional layers within the freewire graph structure. :p

 Would love to hear what u/noahtren thinks is the best usage is."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"well, now there are 50 jobs queued on our cluster for the BBOB in 2,3 and 5 dimensions with 100*d evaluations. Fingers crossed that 1 day computing time per job is enough."
MachineLearning,"What? Amazon and Google sell every kind of personal data, from email messages to google drive files. Google has been reported for tinkering with and deleting personal files for political reasons (for example, it has messed with and deleted a researcher's paper in her own google drive, because the paper she wrote had some implications regarding covid policies), Amazon has terminated business also because of political reasons (as in the case of Parler), not to mention all censorship and political persecution in youtube, and, of course, the selling of data and complete absence of privacy in every single service owned by these companies. I mean, Google employers are literally hearing to and laughing about your personal, private conversations with your wife in your bed at night, or the kind of porn that you watch if you ever get to submit an application to work there. Don't you think stealing a good machine learning model is a no brainer for them?"
MachineLearning,Is it in any way related to Co-deepneat?
MachineLearning,"I'm desperately searching for these workarounds right now, haha. Can you help me finding some ideas? I'm trying to process imagenet on kaggle, and the best thing I have right now is using IPFS for loading pieces of the dataset."
MachineLearning,Thanks! Hope it's useful!
MachineLearning,Nice to see that you are doing this for 3D as well.
MachineLearning,"There's a saying in my first language: ""There's no shame in not knowing, there's shame in not learning"", here's a chance to learn -&gt; [Fort Minor - Remember The Name Lyrics](https://genius.com/Fort-minor-remember-the-name-lyrics)"
MachineLearning,"Yeah I couldn’t agree more, data engineers are very underrated"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Any type of real time big data logging tool on the market. Demand vastly exceeds supply of talented engineers. Especially in the security space
MachineLearning,"I would say that the number of companies that can use a ML PhD is very low (I.e., probably just FAANG etc and maybe some hedgefunds). 

For most, 90% of the problem is going to be data/software engineering."
MachineLearning,"The ""transport"" part is often framed in terms of differential geometry iirc"
MachineLearning,You should look into differential geometry a bit
MachineLearning,"Exciting. Can we see it yet? :) I'm expecting a notebook and a LB entry named AutoML MLJar (teamname). It would make this library quite visible.

Currently, there is a public notebook rank about 50/900. Not sure if it contains sophisticated tricks, but if it's simple it would be nice to beat it. Or learn from it what one could include.

I'm a data scientist not in a data startup, but I certainly believe in automating anything which is a repeating task."
MachineLearning,"I trained for more than a week, but It did not converged completely."
MachineLearning,Very cool!
MachineLearning,Thank you! I'll look it up 😉
MachineLearning,"I trained more than a week with single 2070 super, but it did not converged completely."
MachineLearning,"Maybe try and apply for the same type of roles at a non-FAANG company. I know we all want to say we’ve worked for one of those companies and it can look real good for your career, but there are so many companies that might give you a better or fulfilling experience. Don’t overlook the smaller companies.

Life isn’t about working at Google or Amazon, although it’s something I see a lot of people in this community strive towards. It seems to be the golden trophy, but I bet most that land those top internships/jobs (or any job in general) got in through networking. Just a hunch though from my experience."
MachineLearning,Super cool! How long did training take?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I agree.

Last summer I got a research internship at FAANG as a grad student at an Eastern European university. It took me a few years and about 10 recruiters because they wouldn't let me interview.  I know others from the region who had research internships at FAANG. The team I worked with had senior research members with Eastern European credentials. Admittedly there were more people from Western places.

It sounds like you messed the interview up somehow. It's very hard to imagine that anybody would object to using Python for a research position."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Https://neuralnetworksanddeeplearning.com
MachineLearning,"Lookup Michael Nielsen free book
neuralnetworksanddeeplearning.com"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You can use conda package it comes with python and need libraries for ml
MachineLearning,"When you reach the tech interview, you will be assessed by your skills alone, and if your online resources were really enough, pedigree won't matter here.

However, in any other step on the recruitment process, reputation matters *a lot*. You can't expect HR staff to delve into your CV/Github/StackOverflow/Google Scholar and understand the details which make you potentially outshine an ivy leaguer.

Your best bet, therefore, is to look for open positions with a specific skill set requirement which is an almost perfect fit for your current experience. Those usually skip all the HR nonsense due to very localized demands. For this, you will need industry experience, so I advise you to apply to startups and smaller tech internships/jobs before aiming higher."
MachineLearning,Like?
MachineLearning,Nah there are some types of jobs out there that are the exact opposite where any decent candidate will get scooped up at high pay
MachineLearning,Do you think it’s because the people understand it in theory but can’t apply it on the job?
MachineLearning,"&gt;notoriously difficult vision tasks, e.g

\*crying in 1070 8gb\*"
MachineLearning,I don’t know why everyone is stuck on internships...
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Wait for the crypto market to crash =&gt; buy second hand GPU from disappointed minners for cheap bucks.
MachineLearning,"I thought the same thing when I was in college. After hitting the workforce, I discovered that employers do not care about what college you went to or what degree you have. They care about your experience and if you have the skills needed to do the job"
MachineLearning,"Let me tell you, meeting with ElementAI was one of the most disappointing experiences. They had the hype, they had the talent and they had the funding. Do you know what they didn't have, a desire to actually build a business, almost refused to. They would not meet with a 5B+ customer.....they wanted to know if they were ""ready to buy"" they hadn't even met yet!"
MachineLearning,"oh man when i first learned about NN i implemented a version of this with manual gradients in julia, it could somewhat do MNIST aha. I can't vouch for if it would work better than traditional layers (because in traditional layers, you can set some weights to 1 or 0 to simulate this freely wired networks), something [https://arxiv.org/abs/1803.03635](https://arxiv.org/abs/1803.03635) explores, which I guess would be fun to do that distillation and implement it as a freely wired network aha"
MachineLearning,"It's nice, even though the results are underwhelming. I think this is due to the initialization from ViT, the transformer has to learn language from relatively small data  
I'd be interested in seeing the results with Roberta Encoder, ViT Encoder + a cross modal transformer"
MachineLearning,"Look up SSH port forwarding, it solves this issue easily"
MachineLearning,"A bit off topic, but I’m planning to start a masters at a top 4 US school in ML because I love the field. What could possibly be more fun than ML, which is the intersection of Stats, Math, and CS. But, should I change to another field for employability reasons? This seems like a nightmare."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Thank you very much!
MachineLearning,Thanks :) Good luck ahead
MachineLearning,It's great when you aren't at your desk.
MachineLearning,The real question here is how someone picked up a capable GPU for $200 in the current market.
MachineLearning,"Hi, sorry. I'm not the developer.

u/noahtren is, so might be worth messaging him."
MachineLearning,"Ok so I don't really have the time or energy to dig deep into any deep understand of ML stuff right now, but basically what I want to do is train something based on about 1000 landscape photographs, and then generate more based on that set. What tools should I look into? I'm looking for literally the easiest, hopefully 15 line python solution. I'm imagining something on Github with \~100 stars, released in the last 4 months. (I have no ML experience but plenty of experience banging my head against the wall while programming)."
MachineLearning,"Well you'd probably have to cluster user actions so that they reduce to a few variables, otherwise there would be too many combinations and without a ridiculously large dataset it wouldn't be possible to build a model that would be able to generalize. Another option would be to use a kind of tokenization that NLP uses, where each action would be assigned a value that would be closer or nearer to other user actions' values depending on their 'relatedness' (for example, browsing sport product X and sport product Y would have a similar value since they're both sports products)."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I went  through this paper. Got to learn a lot about **Batch normalization**.

 You can read summary at - [**Paper summary Link**](https://www.linkedin.com/feed/update/urn:li:activity:6767776421348741120/)**.** 

Would love to hear your thoughts."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Very very cool! I was looking for something exactly like this to try my hand at neural evolution!  
One question: since the graph is a pytorch module, could I build a larger ""grafted"" model by adding the freewire model to other pytorch models? I would like to do some freewiring after more traditional convolutional layers, even if it takes a bit of finagling to do."
MachineLearning,Try www.google.com
MachineLearning,"Yh, and my CPU can train imagenet in a day, to 2% accuracy"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Also: If you applied in December, that's already kind of late for an internship, and some (most?) of the positions would most likely already have been filled. Which is why a lot of places will have sent you a ""we're not moving forward email"" without sending you through an interview."
MachineLearning,"Thanks. 

I guess I should really start learning measure theory and functional analysis. I have been putting them off for a long time."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Thanks for your answer. One paper \[a presentation, actually\] that I looked into was [https://mathematical-coffees.github.io/slides/mc01-courty.pdf](https://mathematical-coffees.github.io/slides/mc01-courty.pdf). Another paper is [https://arxiv.org/abs/1803.00567](https://arxiv.org/abs/1803.00567). I felt that my math knowledge wasn't complete enough to understand the material.

I am particularly interested in the theory of transfer learning, domain adaptation and generative models outside of my work in industry. I want to understand the (scientific) principles first before using any pretrained model from the web.

I will surely look into the Rosenblatt transformation. 

Thanks again"
MachineLearning,"Good point. We are currently thinking of having it so you take it in turn to send a message, so can't double text and you have a limit of 1 minute to reply."
MachineLearning,"This makes sense and I don't have a great answer for how to solve it. Maybe I can make it give an answer like ""I don't like talking about current events"" by default. Or maybe filter out conversations about current events, asking the user to talk about something else."
MachineLearning,Great point 👍 The question this project is  trying to answer is more about if it can fool the general population so it differs from the classic Turing Test in that way.
MachineLearning,"People have this bizarre idea that you only train a model once. That's not what happens, you typically do it many many many times. Cloud doesn't make as much sense then."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I can share some surveys and some papers, and from there if you search a little bit you'll find the ""big"" papers:

[https://arxiv.org/pdf/2004.05214v2.pdf](https://arxiv.org/pdf/2004.05214v2.pdf)

[https://arxiv.org/pdf/2007.00095v2.pdf](https://arxiv.org/pdf/2007.00095v2.pdf)

[https://openaccess.thecvf.com/content\_ICCV\_2019/papers/Zhou\_Vision-Infused\_Deep\_Audio\_Inpainting\_ICCV\_2019\_paper.pdf](https://openaccess.thecvf.com/content_ICCV_2019/papers/Zhou_Vision-Infused_Deep_Audio_Inpainting_ICCV_2019_paper.pdf)

[https://arxiv.org/pdf/2010.04556v1.pdf](https://arxiv.org/pdf/2010.04556v1.pdf)

[https://arxiv.org/pdf/2010.10915v1.pdf](https://arxiv.org/pdf/2010.10915v1.pdf)

[https://openreview.net/attachment?id=B1eY\_pVYvB&amp;name=original\_pdf](https://openreview.net/attachment?id=B1eY_pVYvB&amp;name=original_pdf)

[http://proceedings.mlr.press/v80/oord18a/oord18a.pdf](http://proceedings.mlr.press/v80/oord18a/oord18a.pdf)

[https://openaccess.thecvf.com/content\_CVPR\_2020/papers/Jin\_Exploring\_Spatial-Temporal\_Multi-Frequency\_Analysis\_for\_High-Fidelity\_and\_Temporal-Consistency\_Video\_Prediction\_CVPR\_2020\_paper.pdf](https://openaccess.thecvf.com/content_CVPR_2020/papers/Jin_Exploring_Spatial-Temporal_Multi-Frequency_Analysis_for_High-Fidelity_and_Temporal-Consistency_Video_Prediction_CVPR_2020_paper.pdf)

etc.. I can't really put everything, and everything isn't interesting. You can also read the list of tasks on PapersWithCode. Some paper combinations wouldn't make sense and of course I'm not even sure that the tasks and the papers I focused on are interesting for building AGI (I'm sure many papers don't). I didn't focus on NLP because everyone is already trying with NLP and I already know how it works.

If we really need a complex manually engineered NLP engine (the equivalent of teaching a child how to understand words/sentences etc.), this won't be the hardest part of building an AGI. 

I also know that some papers aren't SOTA but I focused more on how they handled the problem."
MachineLearning,"If it was ever proven that any cloud provider stole their customer's data (models or code or anything else), the entire cloud computing segment would be over.

Amazon or Google wouldn't risk billions of dollars in revenue for your models. It would be much cheaper to just hire you!"
MachineLearning,"I think in most scenarios it doesn't make sense to have your own server or computer, when the rented version is mutch less of a pain to work with once you got used to it.

However, at least where I live (Germany), owning your hardware for legal reasons (mostly data security) is vital if you have to handle anything that is not publically available i.e. most buisness applications."
MachineLearning,"Hi! This is awesome, how can I try it?"
MachineLearning,"I've just started Kaggle notebook for Tabular Playground Feb-21 :) Thank you for the idea!


You have a lot of nice ideas. Are you working on some data startup?"
MachineLearning,"You definitely can.
I've mainly faced 2 issues:
- Both kaggle and Colab timeout really easily.
- Storing huge datasets is difficult

But there are workarounds for both."
MachineLearning,I'm curious what exactly you mean by entry level research engineer / scientist? Like roles designed for freshly minted PhDs?
MachineLearning,"Can you do all your tinkering inside a single jupyter notebook, with only gdrive for persistent storage, and with the timeout and ""must keep window focus"" limits? That's what it comes down to.

The restrictions imposed by Colab can still justify a single-user workstation."
MachineLearning,"Did you finish your dissertation? Because it makes a difference if you have already the doctoral degree. Also try to constrain the scope. Apply to some consulting agencies and some lower companies. You will still get a well paid job and more importantly you will get work experience. As far as recruitment tests are concerned, I can only say that they are sometimes very unfair. Some companies don't even want to hire people, but still show a willingness to hire. So they offer unsolvable tests and if you master them anyway, then it's because of the wrong programming language. My tip: avoid this companies."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,tbf if you forget the name of the company you're applying to then E(x) ~ 0
MachineLearning,"Ya, but you should see that they mostly be not direct hires. They had to jump through hoops. They end up in FANG not start from them."
MachineLearning,"Very insightful. Maybe a parallel coordinate plot can help use grasp the ranking better? Overall, you method is winning?

What would actually be good for the story is to compare the scores against the best public notebook (from early in the competition). This can serve as a baseline, because that is what you get with ""relatively modest"" time investment. It seems even more interesting than comparing it to other AutoML.

The current monthly table competition can be an interesting benchmark too."
MachineLearning,functional analysis and measure theory are all you need
MachineLearning,"Sure that is a thing, but I think people don't realize it's not as bad as many make it out to be.

Go look at the list of postdocs for a random department lab at MIT. You'll find many did their PhD and BS at normal universities. Go run through the faculty list at any university. You'll find tons of normal universities. Same with tech companies. They still hire people from regular universities.

The bias is really overplayed. If you put together a stunning body of work someone will hire you."
MachineLearning,Yes. Sometimes you don't want your data and/or algorithms on someone else's machine.
MachineLearning,My models are junk! They can have them.
MachineLearning,Kaggle has paid accounts?
MachineLearning,Would you care to share the papers you've gone through? I think you make a very valid point. One more thing that always nags at my mind for someone saying we could just combine these systems is also the thought of potentially compounding the errors from the individual components (especially if there are more of them) to a point where the AGI becomes dumb.
MachineLearning,"Actually they do. All practical algorithm (working on real cars) pay to braking/accel a lot of attention. Also other cars, traffic signs, road marking, pedestrians, and likes. However from research point of view, for imitation learning and especially reinforcement learning, learning accel/steering combo is order of magnitude more difficult then learning only steering. As toy problem steering only is more easy way to get results."
MachineLearning,"&gt;You are correct about all of the above. Ecoset was not designed to address these issues, but rather to take the subjectivity out of the category selection process. We use linguistic corpora and human concreteness ratings as guiding principle."
MachineLearning,"But, bias is always there. Our own actions are biased for everything until we are willing to change (variance). We humans don't like re-training even if there is new data."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hah, only one paper at a top-tier conference.   Get a load of this, guys!  
Good one mate, but gonna need at least 3 minimum nowadays."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Woah, yeah, anyone who is saying “go for a PhD!!!” needs to read this comment. 

It is absolutely wonderful that you are enthusiastic, and I hope that energy never goes away! But take many dozen steps back here. You’re asking whether you should spend the next 8-10+ years in $600k debt, working constant overtime as an academic and making next to no money, when it seems you haven’t taken a single class in ML and have a very shallow understanding of even different subfields in CS and what a SWE job might entail. 

Not to be harsh, but please focus on taking lower division programming and math classes and doing well in those. The only way you’ll reasonable learn the answers to your questions is by learning the subject material. 

Furthermore, even if you want to do a PhD in ML, having significant programming and CS knowledge is basically a necessity; how do you implement, experiment, and deploy models?"
MachineLearning,You should step outside of the umbrella of ML and branch out horizontally to Data Science or CV. DS is a largely emerging field and its largely the same workflow
MachineLearning,But then Amazon/Google have all your models for free.
MachineLearning,I don't buy this whole push for responsible AI. It all stinks of marketing and PR. Like phillip morris doing philanthropy work. I also think Timnit violated corporate policy and dared google to fire her. And when she got fired she whined about it. Both sides were disingenuous.
MachineLearning,What paper are you referencing in particular? I know a little bit about it but not sure if it's in the context you are referring to?
MachineLearning,[deleted]
MachineLearning,[deleted]
MachineLearning,"Not telling the team about the reorganisation, and having them learn about it on Bloomberg... That's a dick move."
MachineLearning,"I have to say I completely disagree, and this perspective could be straight up dangerous to them. 

**You’re encouraging someone who is going to be $600k in debt before the START of a program, with no prior ML research or experience, to pursue a PhD in ML, without even asking why they’re drawn to ML in specific?**

My very serious advice for OP here: double down on your CS classes. Focus on taking your standard classes on data structures, algorithms, programming languages, design patterns, computer architecture, compilers, operating systems, etc, etc - everything in a normal, rigorous CS degree. THEN think about going into ML after taking some upper division courses at your university. 

If you take the classes and find you’re incredibly passionate about it, then go for an MS / PhD and sit on your $600k of debt, but be aware that the field is far more volatile and competitive than most on the sub will admit to. Your strong CS background will either be incredible useful or necessary by that point in time either way. If you don’t have an affinity, graduate, and look for a very achievable FAANG job with a BS in nearly any other subfield or as a standard SWE, and start chipping away at your debt with a 150k+ salary out the door."
MachineLearning,What's a rule set? How does this different than say straight forward logistic regression or longest regression?
MachineLearning,Want to drop a link?
MachineLearning,"So here it's not used as a regularization term, but rather just as a (seemingly) arbitrary distance metric. The author provide no justification for why they use this fractional Lp metric instead of, say, L2. They also use it in many different contexts; basically, any time a distance is called for, they use L 1/2. It seems unintuitive to me, to the point of raising an eyebrow."
MachineLearning,This is why I love scitkit learn and it's API and I encourage all my colleagues to use their API. For the simple cost of using it you gain loads of third party extensions. Very cool!
MachineLearning,"It works quite well for predicting/mirroring brain data, better so then many newer architectures. That is why, for computational neuroscience, it is commonly included."
MachineLearning,"Just out of curiosity(I am new to this scene), why is AlexNet still being used in new papers? Isn't it very outdated?"
MachineLearning,"Look up the difference in salary for ML jobs with a Masters vs PhD. It is maybe 10-20k per year. Contrast this to the 5 years of income, work experience and life-satisfaction lost during grad school, and a PhD seems like a lousy career move.

If you are independently wealthy and income does not concern you, or you want a job where a PhD is a non-negotiable requirement (i.e. professor or university staff), it might still be worthwhile.

Also, if you are a prodigy and know for a fact that you are going to max out the career ladder, you can disregard this comment."
MachineLearning,"I understand your frustration, but if you are doing your PhD to work at FANG you've got the whole thing backwards, and I don't doubt that this comes through in interviews."
MachineLearning,"Okay, I was using the 14 million imagenet dataset for the math, but the 1.3 million one is probably the one people are using in these examples. I think you are right."
MachineLearning,"Not sure how comparable it is to software engineer internships given the additional expertise required in ML, but I'm an FTE who does interviews for these each year. The other week I found out off-hand from our recruiter that our *entire internship pipeline* this year was referrals. I'm pretty infuriated on behalf of the poor candidates who *actually applied*, and that they'll pretty much be rejected without having their resume reach human eyes.

Anyway, it taught me not to take generic rejections personally, its a shit-show here and it could be a shit-show in ML at FAANG+ too. Companies aren't incentivized to fix a broken process if they still end up with a good-enough cohort of interns."
MachineLearning,"It's sloppy if they call that a norm. But it *is* a quasinorm that induces a valid metric over measurable functions, and the corresponding metric space is complete. So there's definitely some useful structure there. The application I've seen the most for fractional Lp spaces is sparsification. For example, using an Lp quasinorm with p&lt;1 as a regularization term behaves a bit like an extra-aggressive lasso. Decreasing p successively approximates an L0 penalty. This is a technique some have used to approximate NP-hard sparsity-constrained problems."
MachineLearning,"you think you can get "" highly skilled research scientist with a background in materials development and electrochemistry. "" without going to a grad school? 

no wonder why he is the richest man in the world. whatever he says people believe it. 

I can say I don't need any degree and say btw I want to be expert in quantum physics. I don't think the book secrets will cut it."
MachineLearning,"They don't right now, although we are planning on adding support for several of them!"
MachineLearning,"Don’t do it for getting a job. It should be mainly for research. If your main focus is to do research but are worried whether there will be job prospects, well in my opinion ML is here to stay. Can’t say for certain but I believe it will continue to be in demand."
MachineLearning,"I would recommend that you consider time-to-event analysis, like survival models. A binary setup would require strict assumptions on the action window and will not capture propensity in terms of rate to action. A survival model will allow you to estimate how exposure influences a user's likelihood to perform an action into the future"
MachineLearning,"There's 3600 seconds in an hour, meaning something around 3.6M images processed in an hour given 1000 images/sec. Since imagenet is ~1M images, that's 3.6 epochs an hour.  90 epochs at that rate is only 25 hours.  300 hours would be more than an order of magnitude slower, where did you get that number?"
MachineLearning,Well said mate.
MachineLearning,"Frankly, I would use a simple panel regression. Maybe a second order polynomial for the continuous variables and fixed effects for the categorical variables. The good thing about regression methods is that they will be easy to diagnose because you'll know the contribution of each variable"
MachineLearning,"Interesting project, but I think it’s likely that people will be able to discern the difference quite quickly, particularly if they do what I think they will - discuss with others (perhaps even through the chat!) optimal strategies for figuring out the AI. Would the AI give a satisfactory answer to “figuring out who is an AI”? I think not. GPT-3 can SIMULATE a human reasonably well by virtue of its massive memory, but it’s actually a step backwards from the concepts of reasoning that have underpinned AGI and Turing-test-esque workloads. 

I think that older NLP techniques may well make a comeback in the field of reasoning and logic, whereas Transformers will continue to dominate in simulation and pattern prediction. In your case, it’s easy to steer a conversation towards the former, where GPT is weaker."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"And it’s free, thanks."
MachineLearning,"For real. Welcome to the job market lol. This is literally every job. Unless you’ve got an in on hiring, good luck even getting your resume looked at."
MachineLearning,"Hm.  My immediate reaction was to be disappointed.  But then I tried to verify their statements on my own.  I followed the link in the above article, and looked at some job postings that seem to require some significant technical skills.  I looked at only a couple, and haven't seen any mention of degrees.  Example:

[https://www.tesla.com/careers/search/job/senior-cell-active-materials-engineer-80702](https://www.tesla.com/careers/search/job/senior-cell-active-materials-engineer-80702)

So I want to register some doubt re the validity of the above article."
MachineLearning,"Is it an L-(1/2) norm or a p,q norm? [https://en.wikipedia.org/wiki/Matrix\_norm#L2,1\_and\_Lp,q\_norms](https://en.wikipedia.org/wiki/Matrix_norm#L2,1_and_Lp,q_norms)"
MachineLearning,I have seen good results from [Artbreeder](https://www.artbreeder.com/).
MachineLearning,"Portfolio beats college branding everytime. Build stuff. Put it on github. Contribute to open-source. An interesting Github beats an Harvard summa cum laude any day.

Source: c-level executive at a top-ish institution with a stellar ML reputation."
MachineLearning,"Probably not the thread, but are there research intern positions for undergrads?"
MachineLearning,"Hi OP,
I can appreciate your frustration and can tell you it likely stems from what you perceive to be interesting work. There are loads of companies that aren’t FAANG that are doing interesting work and you likely use many of their products! 

I’d say expand your view of what “top-tier” is in industry. While true that pure research positions are at companies with deep pockets, there are many applied opportunities - meaning use what you know to solve problems in a novel way. 

In fact, my team has a position for an MLE internship and we usually have one each year. If you’re interested, let me know."
MachineLearning,[https://thenextweb.com/tech/2020/02/21/elon-musk-says-he-doesnt-care-about-degrees-tesla-job-listings-suggest-otherwise/](https://thenextweb.com/tech/2020/02/21/elon-musk-says-he-doesnt-care-about-degrees-tesla-job-listings-suggest-otherwise/)
MachineLearning,"Thanks a lot for your valued comment!  
Yes indeed, topic modeling is something I am thinking about too to implement. 

Regards, N"
MachineLearning,"Thanks a lot for your valued comment!  
Yes, I went through the article and found it quite useful. Something to keep a bookmark of."
MachineLearning,"Thanks for your comments. Could you post the code? We did not use vt - mt^2 mainly for the concern that this might generate negative values, which would cause numerical problems. We will take a closer look if you could provide more details."
MachineLearning,"&gt;I don't understand what other language they'd want you to write ML in, python is clearly the best supported, so IDK what to say about that, just another example of poor hiring practices.

Wait, you mean to tell me your company isn't still using LISP?"
MachineLearning,"&gt;The only two places that I seem to have made some progress are based on friends' strong internal push, making it seem like that the only way anyone lands an internship is through internal referrals. Now, my confidence about finding a decent internship and possibly a job is so low that I'm literally unable to get myself to apply to more places because of having to face this impending rejection barrage.

The not so secret secret is that every large company's process for hiring has all sorts of issues. There is just zero way to get technical talent to review and filter resumes, know all the teams needs, etc. Hiring teams are often not seen as ""revenue generating"" and often don't get the funding they really should to hire and retain better recruiters. This happens everywhere. I've literally never heard of a big company that thought their hiring game was on-point &amp; didn't let people slip through the cracks. Even when technical people are are recruiting events, you get 200+ resumes and things slip through, you don't remember details, and maybe a handful of names stuck out as ""we need to try and get this person to join us"". All of this is also in hyper stupid mode due to COVID. I'm not going to name names, but I know many companies have waffled 3+ times on whether or not they are doing a hiring freeze or not or restricting the number of offers going out and how to implement it. 

This is also why going to a top-name school has a real benefit to your career. Its not the education, its the _networking_. More than anything knowing someone at the right place, and them pushing for you, is the fastest and least error prone way to get a job there. In 10 years your cohort of fellow students, undergraduates &amp; MS students you taught or advised, will all have jobs at different companies and be apart of your network. The question is if you keep that network alive and refreshed."
MachineLearning,"I mean to say DA (data analyst), I'm sorry for the typo. But good guess from u/morenoh149"
MachineLearning,So brave
MachineLearning,"&gt;The holier than thou mentality is so infuriating.

Its pretty annoying how much this mentality rubs off on their employees too. Too often i hear someone introduced as an ex-FAANGer like that is some monumental achievement that completely defines a person. Take for example Google at ~135k employees with a turnover rate around 1.1 years. Needless to say, there are a lot of ex-googlers running around with a turnover rate that high. Introducing yourself as one isn't all that unique. Kinda interesting to learn at a happy hour, but the quickest way to get me to tune out your sales pitch."
MachineLearning,"What papers / methods are having trouble with without manifold theory? Oddly I don't find much of manifold theory that useful in manifold learning though I'm coming at it from the other way (math first) so i may be biased.

Like usually your just looking for smooth low dimensional surfaces embedded in a higher dimensional space and you only really need to know that they locally will have a tangent vector space space that looks like R^n at every point (like a how a circle has lines as its tangent).

The problem is most math courses start with concepts like topological manfolds that are much harder to develop intuition for as they lack that local vector space structure so you have to learn a ton of stuff.  

Its probably easier to work your way up through manifold learning papers ... IIRC the locally linear embedding one is quite straightforward and all linear algebra and gives a great intuition. You just learn a neighborhood structure and then unroll it with eigenvector decomposition and many manifold learnign methods just improve on that."
MachineLearning,"\&gt; have you seen the PhD/Ms/Bsc breakdown of Tesla? 

No, have you?"
MachineLearning,"I just finished the job search in preparation for finishing my PhD in the spring. The results were odd. I applied to \*hundreds\* of positions and (unsurprisingly) rejected or ghosted by almost all of them, no big deal.

The odd part was getting no response from a place I previously interned (with a great recommendation), or knew people (prior students of my advisor). Maybe that just doesn’t matter at big companies, or maybe the job market is really competitive right now. I’m a pretty good candidate though, got an offer from everywhere that interviewed me (except one) including FAANG.

Keep fighting OP"
MachineLearning,"have you seen the PhD/Ms/Bsc breakdown of Tesla? It is not what you expect. 

He is the PR department of Tesla, whatever he says gets in headlines. 

I would check the distribution of degrees in Tesla before believing his PR stunts"
MachineLearning,"IMO this is really how it should be done. Why spend time evaluating an ML specialist on out of domain topics? I'd rather be confident in an applicant's ability to do ML research than their ability to memorize leetcode solutions. Unsurprisingly, a person's ability to memorize a SWE specific algo doesn't have that much carry-over to ML science... I'm not FAANG, but I am still Fortune 500 and that is how i run my interviews."
MachineLearning,Just google performance of 3090 rtx
MachineLearning,OP is going to community college tho?
MachineLearning,[removed]
MachineLearning,"well yeah, but suppose you ask the interviewer for a pref, none is proffered, and then you get dinged for using the wrong one - that is in no way a problem with the candidate, but it does suggest some serious communication issues in the team."
MachineLearning,For atleast Microsoft and Google you can put down a preferred programming language actually.
MachineLearning,I guess it depends on what you define as competitive. They probably reject more people but the threshold to entry is significantly lower than the FAANG Research roles
MachineLearning,"Lots of applicants maybe, but if you’ve published at top conferences you will stand out for very applied / data science roles."
MachineLearning,I’ve been focusing more on people with different backgrounds. I feel like it’s easier for someone with a strong math foundation to pick up ML-specific skills and domain knowledge than to find someone with a cs background who’s capable of and interested in the nuances of our application space.  Plus diverse backgrounds is overall great for innovation.
MachineLearning,Seriously? Where can I read about this?
MachineLearning,"For Nvidia, I had two interviews. In both they asked 1 coding question and the rest were machine learning/nlp/vision stuff. For Google, I had a similar experience, I had both coding interview and research related interview."
MachineLearning,It’s a matter of competition. If you pick a crowded space you should expect competition when applying for a job.
MachineLearning,What are you talking about? A single 3090 RTX can train Imagenet in roughly a day.
MachineLearning,What did you expect exactly? There are thousands of candidates like you. Most CS PhDs are doing ML these days. It’s a competitive market. Especially if your PhD is from Europe. US candidates often get precedence for US roles.
MachineLearning,"Many thanks. We do plan to change the angle.

We explicitly don't want to suggest any solutions. I think that muddies the water a little, and distracts from our point."
MachineLearning,[removed]
MachineLearning,I think I might have interviewed with the same semiconductor co that also ghosted me lol.
MachineLearning,"It depends on the position.  If you're applying for a research oriented position papers do matter.  If you're trying to get a more applied position then it's a bit less important.

It's especially good if you have research experience similar to the specific subgroup you're trying to get into."
MachineLearning,"Exactly, unless you’re superstar you have to interview."
MachineLearning,"Mostly applied for Texas and CA, the interview was for San Jose."
MachineLearning,"for all that process, they sure don't have one once you get to the interview: can't connect the job ad to the interview, can't communicate a preferred language, it's just so disorganized"
MachineLearning,"&gt; I solved it in Python (after asking the interviewer), and after the interview I was told it would have been better if I had used a different programming language, which the interviewer said nothing about. 

guessing amazon, but who knows - other groups could easily be a clown show too"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I mean, if you said how many kinds (of fruit) AND the bot was coded to look at more than just the previous line, I might be more disappointed. Not sure this shows anything significant."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I really appreciate this comment! It is interesting to know that companies consider a diverse pool of candidates. It would be interested to hear how, from your perspective, does the companies consider international students vs. American Students in general and in the current pandemic situation."
MachineLearning,"I'm afraid no one can predict what will happen in ten years, but if you do a PhD for the sole purpose of getting a job after five or ten years, you are very likely to be disappointed. You should only do a PhD if you genuinely care about the actual science, regardless of whether it's because you want to contribute to the human understanding or are simply curious."
MachineLearning,[removed]
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Try some medium or small company.
MachineLearning,"Google colab pro is as useful for prototyping as my 2080 Ti desktop - and it's only $10 per month.

Do everything in the cloud imo"
MachineLearning,A closer reading of the paper asserts that the topic representation is being done in Euclidean space. Wouldn't using an L 1/2 norm be unsuitable for this topology?
MachineLearning,Thread from the author: https://twitter.com/aran_nayebi/status/1362452694267437058
MachineLearning,"Lp norm with p &lt; 1 breaks the norm definition but it’s still the way we refer to that distance metric. I’ve seen several other papers using that notation. (usually about sparsity). So I’d say the authors aren’t abusing notation, the notation is just contradictory. If it’s a useful metric, do we really care if it’s a true norm or not?"
MachineLearning,"I was an undergrad student at IIT Bombay, India (Best college in my country). At the end of my 3rd year, I interned at IBM research Zurich. After my final year, I interned at Amazon in Cambridge, Massachusetts. Both research interns involving deep learning and reinforcement learning. I got a full time role in Amazon right after my internship as an applied scientist when I was just 21years old. 

The only thing that matters is YOU"
MachineLearning,"&gt; I think top colleges still help you in becoming more competent

Top colleges typically only admit people who are already quite competent. Not only that; based on the relative handful from *those* who actually further go on to win say, a Nobel Prize, *everyone* who went there is assumed to be ""of that level"" or ""in the same class"". Even worse, anyone who *didn't* go there is *automatically* assumed to be less competent."
MachineLearning,Was this in the SF bay area?
MachineLearning,Will not work - in FAANG you have to go through process. No Director will go against it (most of them already tried and failed)
MachineLearning,Oh is that the same as DE.
MachineLearning,Data warehousing? What a data engineer would setup on big query or redshift to collect all of an organizations data for olap.
MachineLearning,"One of the problems with imagenet is that a significant number of images contain multiple objects from different categories which can cause issues during training and evaluation. I didn't read in your paper if you tried to control for this in any way.

One of the key limitations of imagenet is that it is exclusively composed of photographs. This greatly limits the usefulness of imagenet pretraining in other non-natural domains like art, cartoons, symbols, etc. Photographs also almost always feature the target object centered in the image and from common angles or in common poses. This limits the usefulness of imagenet pretraining for applications like video where target objects are often not centered, partially occluded, cropped, in poor lighting, motion blurred, viewed from uncommon angles, or in uncommon poses. Photographs of certain objects also tend to be taken in certain environments which can lead to ambiguity as to what the model is actually learning (to some extent it may learn to infer an object from the background or the composition of the photograph rather than the object itself).

Just some more things to think about."
MachineLearning,"The general pattern of ""develop the model on a workstation"" -&gt; ""train a big model in the cloud"" seems pretty common at most companies I've worked at. Iteration is way faster locally.

On the other hand, building something in between these two (say an 8 GPU machine with shared tenancy among employees) seems not that useful anymore."
MachineLearning,"I mean, with the free accounts. The hacker in me also wants the physical computer, definitely. I just can't help thinking it might be a sort of consumerist choice, not an actual investment."
MachineLearning,Sample complexity = number of evaluations
MachineLearning,what do you mean? didn't you read my paper? hire me!
MachineLearning,"Im not well informed on networking,systems or security jobs, but I don't think i ever heard them mention Phds and masters as requirements, almost always i hear about associate degrees, unless you're specifically talking about reasearch in which case it would make sense"
MachineLearning,You should run a simulation on that.
MachineLearning,And 100% reason to remember the name
MachineLearning,"With the post's context, I am assuming that it is a research role. So can you let us know if the interviews at Google, Nvidia and oterhers were based on your research work alone or had high level coding problems as well?"
MachineLearning,Wouldn’t those be more competitive because that is easier and many more people can apply for it?
MachineLearning,What’s DW?
MachineLearning,Mhmm I see thanks
MachineLearning,"I don't have paid accounts for either, but I imagine yes.  However, the hacker in me wants the thing I'm experimenting with on my desk, fans a blazin and everything.  I don't know why, it just helps me learn better and it feels more real to install drivers on the bare metal myself."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Quick question, can't you do the tinkering in, say, kaggle or collab?"
MachineLearning,"I've worked at FAANG and managed/hired ML interns and full-times myself. Not surprised by your experience. Big tech companies are like this. Lots of interviewers are pretty ignorant, especially to intern applicants. Once you are hired, they won't give you meaningful projects, either. My advice is don't be fixated on FAANG. So many other companies are much much better than FAANG in terms of treating interns. You will get a chance to grow a lot faster."
MachineLearning,"&gt; So much of he hiring process neglects to capture any long term potential in candidates. 

These are ML research internships at FAANGs. They are highly sought after and not infinite supply. 

They can afford hire based on your actual *realized* potential level at X instead of hiring at Y with a hope of realizing a long term potential at X."
MachineLearning,Do any of these work for multi-class classification?
MachineLearning,Totally agree. All the publications and leetcode in the world will be utterly worthless if someone isn't likable. Wish they'd drive this home in Universities more.
MachineLearning,Side-note: there will always be an oversupply of people that want jobs at the best firms in the world.
MachineLearning,"So I do AI for work and for personal projects and can chime in here. 

At work, you would get the side eye for doing any intensive work on your own hardware.  Not for the reasons you said, but because it isn't reproducible.  If I train some model, my team needs access to the Docker image, hardware tier info and run results to reproduce and maintain it.  So no, you wouldn't do deep learning on your own computer in that environment. 


For home, I actually just bought my first dedicated GPU specifically to do my own, personal learning on.  However, I bought an 18 month old graphics card for like $200, not a blessing edge RTX900millionbillion, for all the reasons you said. 


In a nutshell, buy a computer that can do some deep learning if you're interested in tinkering, but leave the BERT-training TPU jobs to machines in Google/AWS/Microsoft's warehouses."
MachineLearning,"Your experience sounds pretty normal. Welcome to the real world dude. You are going after top tier, world class jobs. You won't get them without a fight, yet it seems some irresponsible people have warped your expectations. Recalibrate your expectations and get out there and fight more. If you give up now then you get what you deserve."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Actually, I am using a newly created dataset so I am not sure that the features are informative enough. What are methods I could use to get more insight in that regard? Correlation matrices? Something else?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm a first year PHD student applying for ML engineer internships and I received a lot of coding tests. There is one semiconductor company where I had a good interview and the hiring manager indicated an offer. But, then they ghosted me after a couple of weeks."
MachineLearning,"Title:The Wavefunction of Continuous-Time Recurrent Neural Networks  

Authors:[Ikjyot Singh Kohli](https://arxiv.org/search/cs?searchtype=author&amp;query=Kohli%2C+I+S), [Michael C. Haslam](https://arxiv.org/search/cs?searchtype=author&amp;query=Haslam%2C+M+C)  

&gt; Abstract: In this paper, we explore the possibility of deriving a quantum wavefunction for continuous-time recurrent neural network (CTRNN). We did this by first starting with a two-dimensional dynamical system that describes the classical dynamics of a continuous-time recurrent neural network, and then deriving a Hamiltonian. After this, we quantized this Hamiltonian on a Hilbert space $\mathbb{H} = L^2(\mathbb{R})$ using Weyl quantization. We then solved the Schrodinger equation which gave us the wavefunction in terms of Kummer's confluent hypergeometric function corresponding to the neural network structure. Upon applying spatial boundary conditions at infinity, we were able to derive conditions/restrictions on the weights and hyperparameters of the neural network, which could potentially give insights on the the nature of finding optimal weights of said neural networks.  

[PDF Link](https://arxiv.org/pdf/2102.09399) | [Landing Page](https://arxiv.org/abs/2102.09399) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.09399/)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It looks like your classifier fails to learn. When it has no clue of the correct answer, predicting the middle values is most advantageous since it minimizes the mean-squared error. you should make sure that the correspondence between X and y is not somehow scrambled, and the features of X are informative enough."
MachineLearning,"Try to use a model with aleatoric uncertainty, which will summarize the change in the y with respect to the change in x. This is equivalent to quantile regression for neural network.

Check the paper, *single model uncertainty for deep learning*.

You just have to train a model with (de/dx)\^2 in the cost function. This should take care of learning the changes in the input with respect to the output."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I didn't see this posted prior to this &amp; it's tangentially related to something I was considering trying, so I wanted to see what people more competent than myself thought about this &amp; continuous attention in general before going much deeper."
MachineLearning,"Title:Sparse and Continuous Attention Mechanisms  

Authors:[André F. T. Martins](https://arxiv.org/search/cs?searchtype=author&amp;query=Martins%2C+A+F+T), [António Farinhas](https://arxiv.org/search/cs?searchtype=author&amp;query=Farinhas%2C+A), [Marcos Treviso](https://arxiv.org/search/cs?searchtype=author&amp;query=Treviso%2C+M), [Vlad Niculae](https://arxiv.org/search/cs?searchtype=author&amp;query=Niculae%2C+V), [Pedro M. Q. Aguiar](https://arxiv.org/search/cs?searchtype=author&amp;query=Aguiar%2C+P+M+Q), [Mário A. T. Figueiredo](https://arxiv.org/search/cs?searchtype=author&amp;query=Figueiredo%2C+M+A+T)  

&gt; Abstract: Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.  

[PDF Link](https://arxiv.org/pdf/2006.07214) | [Landing Page](https://arxiv.org/abs/2006.07214) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2006.07214/)"
MachineLearning,"Having been on the other end of this my impression is that it is actually very hard to find good people. ML has become much more hackey than it used to be. There are less foundational concepts and more trial and error. Someone good is someone that has understanding beyond application, that can evaluate a problem and results, reason intelligently about them and have that guide their actions. This seems to be a fairly rare ability."
MachineLearning,"Agreed. I'm would be surprised if OP couldn't get any Data Science or ML internships. The ones that just  focus on using sklearn and pytorch libraries, even at FAANG, are considerably less competitive."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"It is Huawei of course, they have been reaching out to lots of first authors of top conference papers that are doing their PhD in Germany. They are on an absurd hiring spree in the EU in general, and hire for research positions."
MachineLearning,I feel your pain but honestly it’s like that in every field. You think you’re a solid candidate for a certain subset of jobs but you never even get a look in. In my experience the job application process is a farce and all it does is separate the mythomaniacs and cronyists from the qualified candidates. It has completely failed me and I have no faith in it whatsoever. All the jobs I’ve had were gotten because I knew someone there who could either pull strings or who was pally with someone who could pull strings. It sucks but that’s the age we live in.
MachineLearning,"Lots of good comments here, I just want to say that hiring is an absolute crap-shoot. When I was doing hiring (small company, not FAANG), I can't tell you how often my stack would get so large that I missed excellent talent. Most of the apps just put the newest candidates at the top, so if I spent a week working on actual projects, there was a good chance the people who happened to apply at the beginning of the week were overlooked for no fault of their own.

Then some companies have people interview you who know literally nothing about the job you're going to be doing, it's some misplaced idea about well-roundedness I guess. I don't understand what other language they'd want you to write ML in, python is clearly the best supported, so IDK what to say about that, just another example of poor hiring practices.

The whole point is, don't get yourself down. Hiring and school admissions are both hard problems and it sounds like you're doing great in school. 

&gt; Now, my confidence about finding a decent internship and possibly a job is so low that I'm literally unable to get myself to apply to more places because of having to face this impending rejection barrage.

If your only goal is a job at FAANG, then good luck to you. That's like saying I want to be a lead actor in a major motion picture. It's possible and definitely something you can raise the chances on with hard work and dedication, but I would consider focusing your goals on more internal value. ""I want to do research on machine vision for medical science"" rather than ""I want to work at apple""."
MachineLearning,"Tell me about it! I passed all of Google's interviews and I'm in the team search phase. Interviewed with a team in NY (they reached out to me!!) and waited 2 months only to get rejected by that team! It's horrible

Edit: I got accepted into Nvidia anyway and the intern salary is very high. To hell with Google 😅"
MachineLearning,"I was gonna say, there are thousands of people working on PhD's in ML in the US alone. It's easily the most popular field for CS research, even though the job market has been lagging quite a bit."
MachineLearning,"Not a ML eng or anything like that, but I am in the midst of changing careers from IB to ML Product Management/ Product Owner. I went to a top ranked Canadian University in Research in the ML field but I wasn't in any compsci program. I did my undergraduate in finance but now recently finishing MIT MBan program, this has opened a lot of doors for me. I wasn't ever the smartest but I was really good at having people works towards the same goal. Currently in doing my role as a PM and I see a lot of members from my team coming for MIT/Stanford. In my interview with my current company, they never really asked about schooling, it was apparent they wanted experience. My software eng manager was pretty clear on his standards on the hiring, along the lines if candiates not only can do the work but can contribute in a meaningful way. But I do see about an even amount of people from smaller schools, I would say a third of my team are from non-target California schools. So I believe you have a chance, its just a lot of networking!"
MachineLearning,"Oh man - that's good to know. For the particular step that most needs parallelization, the processes do only need read access. I know for sure that when I originally tried to implement it - the hdf5 file was already open, so perhaps if I just close it before calling the get function on the ray calls, then allow the ray processes to re-open. Worth looking into again. Thanks for all the links! Probably should have posted here before doing the lazy thing and having it make copies of the file haha.

As for my use-case: It's really it's just a numpy matrix - usually about 20k-30k rows by &gt;100k up to maybe \~3Million columns. The part that really needs to be parallel is just generating row-wise statistics comparing different column subsets to each other."
MachineLearning,"This is absolutely not my experience. I had a research scientist intern position which I’m coming back to this summer and a standing job offer - I never had to do any leetcode type problems. 

I think it really depends on the domain, most of my interviews were about my research and theirs, the only technical questions I was given were about math/ML and some domain-specific knowledge in my area. Honestly I never looked at those problems because I always thought leetcode type problems were for software engineers."
MachineLearning,"[If I had a dollar for every post like this that was made ...](https://www.reddit.com/r/cmu/comments/lhrkkd/trouble_finding_internship/gn0jat0/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf&amp;context=3) 

On the positive side, it’s not just you. TL;DR of the many posts, ML is incredible overcrowded and there’s far fewer jobs than people are led to believe. 

As usual, the comments are typically filled with strawmen (“well I have a friend who ...”, “I just got my PhD from Stanford and 5th CVPR publication ...”) or comments implying your academic or prior work record is not sufficient; if you see my recent post, someone with a similar record said one simply needs to accept jobs making around $20 an hour! The perspective and defense of these conditions is almost cult-like around ML, as in literally any other field in CS (systems, networking), you wouldn’t fathom struggling for a job after top tier conference publications and degrees."
MachineLearning,"Well, not really. If the dataset is small you can train 200.000.000 parameter (scalable transformers [https://arxiv.org/abs/2005.00341](https://arxiv.org/abs/2005.00341)) even on a RTX 2070 with 8GB of VRAM in 1.5 days on a standard (2gb) dataset. You could even train a 10 billion param transformer on a SINGLE V100 with 32 GB of VRAM using offloading + FP16 ([https://arxiv.org/pdf/2101.06840.pdf](https://arxiv.org/pdf/2101.06840.pdf)). If you fine-tune it from a pre-trained uncoditional model it takes \~ a week. Don't cry, try. ;)"
MachineLearning,"From what I gather matters a lot. You'll lead a decent middle class lifestyle if you go to an OK school, but can only have social and class mobility if you go to a top school. 

In other news, I'm middle class and feel like dying every day"
MachineLearning,ye
MachineLearning,The twitter rubric said Amazon was top company? I find that specious
MachineLearning,"&gt; The top rated schools tend to give more opportunities

This is why I want to die every day tbh"
MachineLearning,It means you won't get an interview from randomly emailing or meeting people there and making a good impression. They have a strict process for vetting and evaluating applicants they don't deviate from.
MachineLearning,"Life is more than grades and achievements, although theses are important. Ask yourself this: will you be happy with such a large amount of debt hanging over your head when you can probably start earning right now as a software dev and do ML in your spare time? I have been programming for a ot of years and the financial security has allowed my to not only learn a valuable trade but also do ML in my spare time and eventually get employed in the field.

&amp;#x200B;

Most SE companies don't care about your pedigree - they care about how well you code. I've worked with and learned from some astounding self-taught programmers. 

&amp;#x200B;

I would also echo the sentiment that a lot of ML seems like just throwing mud at the wall and seeing what sticks which is probably why its becoming increasingly automated."
MachineLearning,"Image sizes are quite varied, the paper has a plot showing the distribution.

Would be great to see how ecoset compares in pre-training benchmarks indeed. We were mainly interested in comparing to human vision, so that is what we started with."
MachineLearning,thanks for sharing
MachineLearning,thanks for sharing
MachineLearning,That's actually 15%. Reason to remember the name is sigmoid(x) as x -&gt; inf.
MachineLearning,"You're right. It's strange that standard Bayesian optimisation has not been tried on COCO before, or maybe it has but I missed it or it hasn't been published. The results would be interesting. I think the main reason is that since it assumes expensive objective functions, it also takes much more computation time for every function evaluation. The method of this thread for example runs a complete evolutionary algorithm for every function evaluation, that makes sense for hyperparameter tuning but not for COCO synthetic functions."
MachineLearning,"Always go with cloud if your tight on budget. Unless you can afford a decent gpu(s) that would still be useful in a couple of years, cloud is the way to go. Also as someone else mentioned, AWS spot instances are very cheap. I used them to train object detection models &amp; they worked perfectly fine. Have some scripts to launch &amp; destroy instances &amp; attach an EBS volume with the code, data &amp; the env so that u don’t have to prepare every time. For me it was just fine."
MachineLearning,Well some IVY League schools have extremely high grade inflation. The average grade at Harvard is a B to B+
MachineLearning,"I have a friend in ML at Microsoft HQ.  He told me they have a ""top 5"" list of schools the recruit from.  Georgia Tech is on the list

He's not the only guy Ive heard things like this from."
MachineLearning,You are the best
MachineLearning,"Ty very much for the answer, I think I will stay with the 2 x 3060s for a while"
MachineLearning,"Because you say ""applied ML jobs"" I'll say that the bulk of the day to day work for those is data/software engineering or business requirements. There's not a lot of novel ML going on there. You'll be surprised how much stuff runs on linear regression.

If you want to research the next giant transformer then sadly the trend seems to be the FAANG like are looking for PhD (there's always exceptions).

Your college reputation plays a part in getting your foot in the door, as unfortunate as it is. Other things that can help here instead are relevant internship experience, relevant self started projects, and the big one is referrals. Get a referral if you can, they're always helpful, and anyone that says otherwise is lying to you.

Also straight up luck is a factor here, so being perseverant helps.

""You miss a 100% of the shots you don't take - Wayne Gretzky"" - Michael Scott"
MachineLearning,Some in industry do care about what you have on Github. Source - was responsible for hiring DS/ML folks as a senior DS
MachineLearning,"I would love to know about some hypotheticals and induction opposed to just interpolation between text that already exists.  For example, please tell me how it does with the following:  


""The ShuffleHop-900 is the latest model from Mercedes Benz"".  
""DiggerBatLizard is a new species of marsupial found in australia.""  
""The ShuffleHop-900 and the DiggerBatLizard both entered into a race, the winner was...""  


Can it understand Mercedes Benz -&gt; car manufacturer, so ShuffleHop-900 is a car, and  all/most cars are faster than most animals?   


If it fails, maybe change ""Lastest model from Mercedes Benz"" to ""a car"", etc."
MachineLearning,"Interesting. Performance on some model pre-training benchmarks would have been nice to help make your case.

Also what resolution are the images?"
MachineLearning,"You can define a time window that is acceptable for your application. In that time window, it doesn't matter when the event happens, if it happens it is a positive class. If the time window is large enough, data imbalance is mitigated.

Other option is to model this thing as a time-to-event prediction."
MachineLearning,"The data-transfer bottleneck on PCIe Lanes shouldn't worry you too much because it's rarely the bottleneck of ML algorithms.

Using 2 GPUs for 1 training is perfectly fine but it'll be less efficient per GPU than only one. Though this impact can be greatly reduced depending on the library / code you're using (for example for Pytorch DataParallel is less efficient than DistributedDataParallel, Horovod etc), but again this reduced efficiency will probably not come from the PCIe lane but from the synchronization between GPUs.

Stay with the 2 x 3060, run multiple benchmarks and if you see that there's a problem you can adapt your config but I seriously doubt that you will have one."
MachineLearning,How often do you see an ivy league resume with nothing else on it?
MachineLearning,"In that case, OP should probably keep in mind that most PhD students in STEM fields at first- or second-tier institutions end up getting all of their tuition funded and likely much of living expenses as well."
MachineLearning,"Off-topic but if you're interested in computer graphics and ML, you might want to look into synthetic data generation for computer vision tasks. This is actually likely to be more SWE-heavy and probably not require a PhD."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"College reputation goes basically MIT/Stanford/Harvard/Oxford/Cambridge/ETH Zurich &gt; everything else

Nobody is going to look up your college and see how good it is if they don't immediately recognize it. So if you're not at the very top where a random person off the street has heard of it from Hollywood then it doesn't matter."
MachineLearning,"Having a PhD from any pretty good school is a huge boost at almost any employer.   

The special special ones, FAANG and wanna-bes, they will wring the life out of you like a wet cloth anyway and you can spent your 30s regretting the loss of your 20s to mere work. 

If you want to have fun, joining a high end consultancy doing specialist ML work on projects all around the world is a great way to do it. You will probably have other very high performers around you, because they want results for their customers quickly.

Having a PhD cures most people of the wish to be an academic researcher, but its unfortunate if you still want it but academia doesn't want you."
MachineLearning,"Resumes from Ivy Leagues and nothing else substantial is also a red flag because you know they were offered great opportunities but didn't or were unable to pursue them

Basically I want to see extracurricular items on the resume and care less about the school but I'm a hiring manager, not a recruiter"
MachineLearning,"It matters a lot.

For the rest of your life, you’ll be passively referred to as an MIT-grad or Stanford-man, whether you are an entry level SWE or CTO of a trillion dollar company. This most definitely helps with promotions/career objectives.

Think of how many times you talk about another student, coworker, or candidate, but can’t remember their name. But you usually remember which school they went to."
MachineLearning,"&gt; The only two places that I seem to have made some progress are based on friends' strong internal push, making it seem like that the only way anyone lands an internship is through internal referrals.

It's not _just_ ML internships."
MachineLearning,OP is an undergraduate student
MachineLearning,I feel that when I learned Einstein notation it felt like a superpower. It expresses so much from so little.
MachineLearning,I applied to roughly 200 entry level &amp; intern research engineer / scientist positions in January and February and faced a shit ton of dynamic programming questions. AI residency at Apple was 3 DP questions (leetcode hard) and other big tech companies commonly sprinkle 1 or 2 in as well.
MachineLearning,What does it mean that “FANG is too process driven”?
MachineLearning,Lol?
MachineLearning,"From the dozens that I have seen only like 10% are hare, the rest medium. I believe easy ones are followed up by more questions in the same interview."
MachineLearning,"Internships yes, papers definitely not.  HR doesn't read your work.  They hire who they know.  Elon Musk may not care about your education, but their HR team totally does.  There are thousands of people who can pass the coding tests, but the one with the prestigious degree who previously interned with the company gets the job.

Also, ....publishing is free.  It's reading it that costs $$."
MachineLearning,"In the US, nepotism rules the game.  If you are the absolute best in your field but have no connections, you might as well not even bother.  Can you school provide you with the network you'll need to succeed?  

I went to a crappy state school and despite getting every fellowship and grant available (including an NSF GRFP), I couldn't get a post-doc or a job in my field after I finished.  My network was small, and everyone looked down on my school.  I published 5 papers, went to conferences, etc.... nothing helped.  Now I'm in a new field ten years behind.

It's amazing how powerful the networks are at top schools.  As mentioned elsewhere in the comments, the jobs literally COME TO YOU if you go to MIT or UC Berkeley or others.  Unless your school has some special connections, including internship possibilities and partnerships with a FAANG, you'll be left in the dust.

Skill and talent are also super important.  But that's easy by comparison to the network.  You can build skills in your basement, but you'll never get access to the network.  The folks at top colleges are skilled AND have the network to succeed.  You need both.

Also, consider the exposure you'll get at a smaller school.  Folks at top colleges are working on cutting edge shit with DOD grants and private VC funding.  If your school doesn't have good projects or tech, you won't even get to work on the coolest stuff out there.  At my school, I had to use ancient equipment and therefore all of my papers were 10-20 years out of date compared to the state-of-the-art.  Not exactly inspiring."
MachineLearning,"Are these local opportunities or are you applying all over the country/world? 

Internships in general seem to have diminished due to the pandemic, my company had a huge internship program in the summer of 2019 and it was much smaller in 2020. That is likely a factor that is working against you right now. 

It sounds like you are doing the right thing by networking, but perhaps you also need to ramp up the volume of applications and try networking with College/Campus Recruiters (this is a specific role at most large companies). I've seen it take 25-50 applications to generate 2-3 interviews and 1-2 offers... sometimes even more applications depending on the skill set and other factors."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,My guess is google. All the people in my lab had been there.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"So much of he hiring process neglects to capture any long term potential in candidates. I've been there. I know it's frustrating. That's part of the reason I am dreading making the transition to industry. Also, can I just say something to all the FAANG companies who brag about hiring the best. Why do you need to hire to best to work on making your shitty ad algorithm better? Seems like a screwed up metric and a waste of talent. The holier than thou mentality is so infuriating. My first industry offer told me to get in the proverbial rocket ship. A month later the team was laid off."
MachineLearning,"Well, saying ""mix video, audio, text and images"" is easy.

Knowing what it means concretely is harder of course. But that's actually the main thing I'm working on.

The main problem I have while trying to do this is that for image processing, we're good. For text processing, we're good. But for audio and video processing we're so so so bad. The research is 5 years behind image and text processing.

Some metrics aren't well defined.

Plus one of the most important feature is unsupervised learning and while it's quite developed for processing texts, it's quite under-developed for images, sound and video processing.

I've passed January reading 20+ ""sota"" papers for all these tasks and the gap between unsupervised image/text processing on one side, and unsupervised sound/video processing on the other is very large. For texts, unsupervised pretraining is the standard actually so it's ok. For image processing it's quite a niche (few people follow the ""well-defined"" task to evaluate such learning), and for audio/video the task, the dataset and the metrics are really unstable. There isn't a dataset like Imagenet but multiple datasets describing multiple behaviours etc.

Plus that's just the mathematical parts. You need to connect that to some knowledge from psychology, linguistics, neurology, cognitive sciences etc. I recommend to study that, the cooperative principle from Paul Grice, cognitivism, connectivism, theories about reinforcement learning (pavlov's dog) etc. All learning theories, most linguistic theories, everything from cognitive sciences

Building the bridge between all these theories and the raw mathematical models we know is hard if even feasible. And GPT3 doesn't even do 1% of that work.

But tbh, all theories have been made on AGI. I'm tired of reading theories, if someone wants to prove that he's right, he just has to do it."
MachineLearning,I think this is the key point. If you go to any of the top uni’s then the recruiter know just from that that you have the right drive. If you don’t you need to show it in another way.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You can probably get human-like enough conversations with regular people. If you ask someone who was some experience with GPT-3, for example through AI Dungeon, they probably know a few ways of conversing that will make it fail in not exactly human ways. Talking about rhymes and puns are hard for GPT-3. Comparisons can cause issues too. Directionality as well, for example it will easily mix up who will who pay when talking about buying or selling."
MachineLearning,"Honestly,  I don't think that this is a ML issue, it's an internship in specific and hiring in general thing.   As someone who's led projects and had interns, the process is a mess and project leads don't have the time, interest, or power in making it better.   Lack of communication between the hiring group and the technical groups is endemic.

As someone says below, don't ever take it personally.  Getting hired is largely a crapshoot, but persistence and connections go pretty far.  Just keep trying.  Also, IMHO FAANGs are overrated.   Find a startup or small firm, where you can make a difference, get in on the ground floor, get some stock options, and you'll do better.  


Edit:  BTW, if you cannot find a job / internship, start a company.  I don't know how easy / hard it is where you are, but in the US it's ridiculously easy.  So, form a company, create a product / service, put it on a web site.  You should be able to do that in a couple of months.  Put it on your resume.  The fact that you may never get a single sale or dollar (or euro!) of income is irrelevant."
MachineLearning,100% concentrated power of will
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Great explanation
MachineLearning,"It's not about what university \*I\* know or not, and Sharif is a bad example: I've had dozens of very bright colleagues coming from there. Iranians are well represented in FANGs (at least in Canada; it's harder to get visas in the USA), so recruiters are used to the top schools.Now, look at how many people around you went to school in Japan, Lithuania, or Honduras, and tell me if you believe tech recruiters can tell you how how good their universities are? They might not even have been exposed to a single candidate from these countries, and the average candidates from there might still be brilliant. Do they know the difference between a French university and a Grande École? (From experience at FANGs having recruiters asking me questions, I'd say ""no, they mostly have no clue""). On top of that, in some countries, there's great research happening in very average universities because salaries are fixed at the country level and some researchers don't want to relocate. So, a student from a lab at University of Shithole might be top material, while a student from another lab at the same university might be meh material.

So, if you come from a top known university, it helps. If you don't and it's from abroad w.r.t. to the recruiter, chances are that it doesn't matter the least."
MachineLearning,"&gt; you’d better be a master at leetcode hard level problems in under 45 minutes

I can only offer my own anecdotal experience, but I haven't seen algorithm problems discussed for a research scientist position at that level. More like leetcode easy  and, occasionally, medium difficulty. I have seen such requirements for research engineers though."
MachineLearning,"Linear Algebra, Calculus, Optimization techniques like Convex Optimization, Mathematical Statistics, and Probability Theory."
MachineLearning,"I dont know who is worst, the fucking companies acting like they are the mecca of knowledge and accomplishment, when 90% of what they produce is trash or the student's credentialism. Boohoo, Google does not want you? Go to any of the other 10000 companies, create a product, take a teaching position, go a do a postdoc, FAANG is not the end-all , being-all of ML and frankly any person who believes that wont contribute significantly to the field in any case."
MachineLearning,I am waiting for the result after the interview from the professor I applied to and I find it so long to arrive ... I want to email him but I know I shouldn’t.
MachineLearning,"Create 2 fake “like for like” resumes and 2 fake email addresses. Apply for jobs and see if the school matters. 

If you only hear back from one then you know. 

Why speculate when you can get instant feedback."
MachineLearning,"&gt;Top tech companies have loads of foreigners coming from universities that recruiters and peers might never have heard about.

Or the recruiters have heard about them and you just haven't?

For example you might not have heard of Sharif University in Iran, but I can guarantee you that Tech Recruiters have, because not even hyperbole, it seems like every university in the US has at least 1 professor in EECS that is a Sharif Alumni.

MIT for example has 4 Alumni from Sharif as professors in EECS."
MachineLearning,"Not when you have really good (ie. big) unsupervised models; the bigger they get, the less RL feedback is necessary to finetune (eg [text summarization for GPT-3](https://openai.com/blog/learning-to-summarize-with-human-feedback/)). Then RL is just a small cherry on top of the cake."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Recent FAANG MLE intern, 4th year PhD student at a mediocre institution here. Here are a few tips, based on my years of struggle:  
1. Hiring, and especially hiring for DS/ML jobs are noisier goddamn stochastic gradient descent (note I said stochastic, not even minibatch). Do not, ever, take it personally.
2. The job landscape is exceptionally overcrowded and the industry people still don't exactly know what they're looking for. Don't take them too seriously.
3. More people make a transition from within the company. So say, DW, SWE or DE transitioning to MLE or DS. Half of what MLE/DS do are production or basic data analysis anyway (depending on the company). I'd try applying to SWE jobs given your background, and move my way into MLE position from within. I know after academia, this idea sucks. But hey, then stay in academia, why are going to industry in the first place?

Note: My acceptance to the FAANG internship was 95% luck and 5% grit/skills etc."
MachineLearning,"I see, so overhyping and misinformation"
MachineLearning,"There is a definite art and soft skills to interviewing and resume presentation.

And also sounds like some of the interviewers you had were not great, which is unfortunate but expected. 😔

are the people handling your resumes and doing the interviews also machine learning PhDs?"
MachineLearning,"What happened with GPT-2 is that OpenAI only released the smaller model (124M), out of concern that the larger models such as the 1.5B could be misused by bad actors, which caused a highly misleading news cycle.

When it became obvious that GPT-2 wasn't being used by bad actors, OpenAI relented."
MachineLearning,lol harsh af
MachineLearning,"If you’re doing ML interviews for FAANG level companies, you’d better be a master at leetcode hard level problems in under 45 minutes. That’s just how it is right now because of the oversupply of candidates right now.

Also whoever rejected your code solution in python because of the language choice was a moron for the obvious reasons."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The vast majority of PhD student positions are fully funded (i.e. the school gets grant funding to pay your tuition) and come with a stipend from the university. The stipend might not be quite enough to cover living expenses in the Bay Area, but a grad student will not generally be paying 100K/year."
MachineLearning,Sent in an application - also have a connection who I emailed to help fast track my application
MachineLearning,Current thought is to have no limit
MachineLearning,"Like some of the other posters said, statistics and information theory are at the top of my list.  However, arguably the best way to learn statistics and information theory is through their application in machine learning contexts."
MachineLearning,Yet another domain the transformer eats away.
MachineLearning,Not there yet -- it'll probably fail big time!
MachineLearning,"Have you considered a heat map, maybe with an additional time since pageload axis represented?"
MachineLearning,I did briefly skim through a lot of articles about the subject.
MachineLearning,I think you’re confusing gpt2 with a different model that had articles like that written about it. Those articles were also completely sensationalized and ridiculous to anyone remotely familiar with the area.
MachineLearning,"Okay thank you! I think I'm starting to wrap my head around it a bit better.  I understand how machine learning would prevent overfitting if say I fit some squiggle to my data instead of just a line, but it seems weird that doing the training/testing would give a better prediction then just fitting a line to all the points.  But, I guess that's what machine learning does"
MachineLearning,how did you get access to gpt3?
MachineLearning,"I don't know. I'm not educated in the US and I'm not a recruiter. If your CV shows good projects and/or papers (and, I guess, grades), I'd move forward with a phone screen."
MachineLearning,"A few considerations:

First, I strongly recommend thinking about how to make sure the humans try hard here. I imagine many people will log on and mess around, maybe trying to act like a robot, etc. One possibility is doing repeated matches, and then giving people ranks at the end on how human they are, and how discerning they are. Hopefully people try to maximize those.

Second, make sure that your delay time is appropriate, or standardize it, so that timing can't leak information!

Very cool thought. I think it will be easy to tell, but it would be cool to find out otherwise."
MachineLearning,It's somewhat hidden. Will try and see whether we can update the dataset on codeocean to include the pdf.
MachineLearning,Thanks! Didn't see the supplement
MachineLearning,How long can a given conversation last?
MachineLearning,"It isn't about where you go to school so much as it is who you go to school with. Many of the opportunities that have come up in my life have come through people I know. Smaller schools, less well known schools, put you at a disadvantage in that regard.  


I will say, this isn't a good or fair system. Systematic inequality is perpetuated by this kind of benign patronage. But it is also the way the world works."
MachineLearning,I learned MathJax so I could reproduce mathematical formulas in my notes which I keep in HTML. I have my own cheat sheet which I add to when I encounter unfamiliar notation.
MachineLearning,"Find a high-level researcher/manager whose work you like and is aligned with your own research, somebody with hiring power. Send them an expose directly, circumvent the entire interview/hiring process ;)"
MachineLearning,"I agree. The people who can get you the interview are usually working at HR and they couldn't care less about GitHub or blogs... I've been recommended to both my internship and my first job, but before that I had to go through the standard process of applying and receiving constant silence.  


My previous message was only about how the technical interview went, I'm sure prestige helps to a larger extent when it comes to standing out and actually getting the interview in the first place."
MachineLearning,"&gt;  language models aren't supposed to know much about vision

For some reason GPT-3 can answer many of these questions. I assumed the same limitation until I started testing various ideas with it after /u/Wiskkey pointed out [examples](https://old.reddit.com/r/MachineLearning/comments/iemck2/n_gpt3_bloviator_openais_language_generator_has/g2jpbq8/) (they're below that comment). It has a lot of intuition about the world and what is possible. Images are worth a thousand words and the input data for GPT-3 is a lot of words, many of them about visual things."
MachineLearning,"Yes that list is in the accompanying paper (and there in the supplement).

[https://www.pnas.org/content/pnas/suppl/2021/02/12/2011417118.DCSupplemental/pnas.2011417118.sapp.pdf](https://www.pnas.org/content/pnas/suppl/2021/02/12/2011417118.DCSupplemental/pnas.2011417118.sapp.pdf)

Page 9 onwards is a table with all categories, number of images per category, concreteness rating, linguistic frequency of the noun, etc."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Go to Pittsburgh. Pittsburgh has a relatively low cost of living and a long history of robotics and artificial intelligence research. ""Since its founding in 1979, the Robotics Institute at Carnegie Mellon University has been leading the world in robotics research and education."""
MachineLearning,[deleted]
MachineLearning,[removed]
MachineLearning,"Where I'm stuck is even more fundamental, I can define the covariates, but it isn't obvious to me how the observations should be keyed. Just inventing the negative case for ""user took action after {user history}"" is difficult because defining the same user history on every non-event could lead to gigantic storage costs (and is probably meaningless). If I try to define the outcome as ""user took action during time window"" I can define that window analogously for a ""user didn't take action"", but there is still the problem of defining the time-stamped user-history prior to an event in such a way that I have a (relatively small) meaningful set of covariates. 

&amp;#x200B;

Does that make sense?"
MachineLearning,This trend is shifting though. By the time you 'graduate' nobody may focus on it anymore.
MachineLearning,"The biggest advantage of these colleges is the environment and intellecutal climate. You're surrounded by like-minded, highly driven individuals leading to (healthy) competition, inspiration, and collaborations."
MachineLearning,"Initially you might have trouble getting calls because of location. If you're too far away from thr company then they would have to fly you in so often they prefer candidates applying from nearby. However you might still get chances if you are from good universities and your resume fits the job profile really well. For your first job don't stress out so much. Most people have this misconception that straight out of graduation you need to go into the best dream company. It's hard, rarely happens and people switch within a year or two because your initial salaries will be low balled and you won't know how to negotiate properly since you don't want to risk losing your only option. Instead just forget that whole concept and think about getting a job that will allow you to relocate to places with lots of companies. Like Bay Area. Once you have a foothold here you'll notice how easy it is to switch or get job interview calls. You'll be inundated with emails, offers or interviews almost every day. Then you can switch easily and make your way up to your dream company in a year or two if you keep an eye on your daily prep. This is the best way to go about it."
MachineLearning,"[Didn't find it, but might help](https://code.likeagirl.io/resumes-that-made-it-into-faang-f7de9b0c4396)"
MachineLearning,You’re advisor matters more than university.Top university tend to have more famous/upcoming faculty but several mid-tier schools also have great faculty. If you can work with such faculty in your university you’ll be in good shape.
MachineLearning,"A degree from an elite university is a guaranteed invitation (within reason: if you're a fresh graduate and apply for a senior lead position, nobody will take you seriously of couse). But that's about it. Your degree opens doors, it doesn't actually help you pass the interview. You know what else opens doors?, an impressive track record: papers, internships, etc. So I wouldn't optimize for it. If you want to get a degree look for universities whose research aligns with your interests and an academic advisor who will have your back and allow you to explore your own ideas rather than making you a proxy for exploring theirs.

Also consider whether you want to pursue a degree altogether. The trend is increasingly shifting towards valuing demonstratable skill over degrees. Elon Musk already ""doesn't care about your educational background"" - so long as you can ""pass hardcore coding tests"". But beware: self-learning is tough. It requires a lot of discipline and dedication. Publishing can get difficult and costly. (Some) formal training is highly recommended and a mentor can be invaluable. I'd recommend you take at least some classes (esp. CS), find like-minded people to study/publish with, and find a mentor to guide you."
MachineLearning,"I'm guessing Amazon?  Or perhaps one of Alibaba/Baidu/Huawei?  I have the same question, however..."
MachineLearning,"Balance the dataset and explore relationships between the variables you have and the target variable? You'll need to come up with variables that might be of interest beforehand of course. The relationships between variables can be handled by the model, or in case you find any obvious one through exploratory analysis, you can create a variable that takes the related variables into account instead."
MachineLearning,"Wow you didn't land an internship in companies like Google? I bet you're the only one who got rejected from those companies, poor you."
MachineLearning,"Things have been shifting with hiring and now there are other factors beyond your school that can be equally or in some cases even more important:

* **Diversity** \- Racial &amp; Gender 
* **Work Authorization** (You mentioned ""in the US"" so I assume this means you are an international student, and work authorization can have a tremendous impact on future opportunities) 
* **Ability to Code** \- If you have the skills you can usually find a path to FAANG or other top tech companies. In many cases you may have to begin with a startup but can pivot relatively quickly, and a lot of it will come down to your ability to pass coding challenges.  

Of course those from top schools have a bit of a 'head start' since companies are actively recruiting from their schools and their schools carry more positive branding, but on the flipside if you go to a smaller school you may be able to save a significant amount of money and also have more time for passion/side projects - something that is ultimately a very important criteria for many junior-level roles."
MachineLearning,"No, no chance.

A Turing test isn't fooling someone who is going about it carelessly. It's fooling a skilled tester.

This would likely mean that the tester would lead the machine down paths where it gets pathological and uses that to decide whether it's a human or not. Perhaps he tries to explain how to play a simple abstract board game and sees whether the machine understands what he is trying to do.

If you talk to a human and say something like, 'You know a keypad, with the first row 1 2 3, the second row 4, 5, 6 and the third row 7, 8, 9: we could play tic-tac-toe on it: I will say a number and then you will say a number, until one of us has a row of three'.

GPT-3 is not at a level where there is any chance whatsoever that it will play tic-tac-toe with you. It will instead respond with some kind of waffling."
MachineLearning,I can explain my thought process to you if you are interested in interacting more on this topic. May be we come up with a new method of inversion.
MachineLearning,"1. Wasn't' there a separate subreddit for career questions?
2. Don't get into debt over college, if you're good at programming/ML there's plenty of good jobs out there, get hired and learn on the job, only go to college if it's free or sponsored.
3. Work on interesting stuff, love what you are doing, EoD most people are very disinterested, actually liking what you work on is enough.
4. Don't get into debt over college, if you're good at programming/ML there's plenty of good jobs out there, get hired and learn on the job, only go to college if it's free or sponsored.
5. Get some work experience during college, even if it's not fufull-timell time work, gig works or internships help a lot and will help you figure out what you want to do
6. For the sake of all that is holy, read ""The case against education"" or any other work exploring the US college system and don't get into debt over college, it's \*not\* worth it."
MachineLearning,"Yep, OP can easily make double that without a PhD as a software engineer/MLE/DS - not every ML job is at a FAANG. 

OP - you seem a bit confused about what you actually want - maybe working for a few years with a Bachelors will help you get clarity on your future goals. A good CS background will be useful no matter where you end up."
MachineLearning,Do you have any news ?
MachineLearning,"&gt; reaching the absolute top of the industry

Which in many cases has much more to do with real-life experience, networking, team working skills, leadership etc than having spending half of your life at uni."
MachineLearning,"Fresh out of school? You'll be tempted to think it matters, people who only know 'old school' thought will tell you it matters and you will probably *think* it matters.

In all actuality? It doesn't matter much at all. The older you get, the less it matters. Once you're in late 20's and beyond - it's going to be something that is glossed over on any resume you provide.

Early on I bought into the hype of ""good college name"" and all that junk. Here I am much later in life with a good gig - and not once (in my entire life) has where I went to college even been discussed. When I was fresh out of college, it would get brought up in passing like ""oh, yeah, I know a guy that went there"", beyond that? It didn't do anything for me professionally. Still got the gigs I wanted, where I wanted to get them.

I know plenty of people with good gigs in top tech companies who came from no-name schools. Some even didn't have a degree when they started at these places but had a wealth of examples to back up their skills.

Ages ago (literal decades), schools used to factor in a bit more. If you're wanting to work at a company who is even remotely 'modern'? They won't place much concern/weight on it at all. Got some examples of your work? Any additional publications and such? How about open source contributions? That matters far beyond ""where did you attend school?""

Learning how to effectively sell yourself is far more important and will bring you much more success than the name of your college."
MachineLearning,"But hyperparameter tuning problems are still different from synthetic functions + sleep/wait. For one, they are real life functions, and from what I know the performance of optimisation algorithms on real life functions is different from synthetic functions for reasons we don't quite understand, unless there has been much progress in this area. Second, the expensiveness can vary because changing hyperparameters does not just influence accuracy but also the speed of an ML model, so you'll want a method that can deal with that (though standard Bayesian optimisation doesn't deal with that). Third, you'll want to see how well the hyperparameters generalise to similar data sets, so the objective function you give to the algorithm is different from the objective you're actually interested in, which is not the case for COCO."
MachineLearning,"Arent FAANG research ML internships super competitive and the supply is less than the number of the PhD student body of the top 5% of students in the US alone much less the world?

The one lesson I would take is that unless you form a long bond with someone at a conference what they say especially in regards to positions is of little value. When you are an employee at these conferences you are encouraged to drive applications to the roles and thats it “full stop”"
MachineLearning,"I went to community college. That is why I work at a lowly non-profit company. On the plus side, I did learn bowling at college."
MachineLearning,"There is a big distinction in what matters to get the interview opportunity and and what matters to pass the interview.

For getting the interview I really doubt that recruiters care about github and blogs."
MachineLearning,"Hmm, I think RL for language generation is not really a thing yet. RL is hard and very data-hungry."
MachineLearning,"I thought the turing test was more conjecture than a real test, are there any actual test suites in use?

Usually the big problem with the Turing Test is the subjectivity of humans. You generally need a better judge for an AI test."
MachineLearning,"I recently came across measure theory when I was reading a paper in strategic Multi-agent RL. 

It used the abstraction of [probability measure](https://en.wikipedia.org/wiki/Probability_measure) which I found quite profound as a mathematical tool. From my understanding, It basically helps transform the occurrence of multiple independent events into a single probability value. Which can be super useful when we look at things like RL. Have u seen papers in AI space where people use similar abstractions?"
MachineLearning," probably $60,000 out of state tuition + living expenses in CA"
MachineLearning,"probably $60,000 out of state tuition + living expenses in CA"
MachineLearning,"I agree, the developer I've hired seems to be very good. He is very forthcoming with what can/cannot be done. I want to hear all the bad news about my project first :) Bad news isn't wine, it never ages well."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Not OP, but can you tell me which company do you mean for the 20% crowd?"
MachineLearning,[deleted]
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Shapash doesn't work with image and text.
Maybe we will develop explicability for text in the next months/year"
MachineLearning,I wanna know how GPT2 was described as being too powerful and subsequently shut down
MachineLearning,"&gt; You absolutely need to understand certain topics in real analysis and measure theory to be able to approach manifolds and differential geometry.

I think the only measure theory I've ever used in a differential geometry course was having to know what measure zero sets are for Sard's theorem. You definitely don't need measure theory to start learning differential geometry. 

You may, however, need measure theory once you reach the point of learning the intersection of differential geometry and ML, depending on the direction you go with it (e.g. information geometry). Even in my manifold learning course I don't think there was any mention of measure theory except maybe once or twice in passing."
MachineLearning,There is work on Manifold learning WRT GANs. Even I am struggling with this right now. I am trying to learn these concepts properly first before visiting connections in DL ie GANS.
MachineLearning,"If you are among the top 5% PhD's in Machine Learning in Europe, message some FAANG industry researchers directly. 

If you don't you think you are in the top 5%, but you are in the top 20% of PhD students in Machine Learning in Europe, and you want an internship, you probably already know the company you can get an internship in, no matter what your race or gender is. Only thing that matters is that you are not a US citizen."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I would try to make GPT-3 output answers a human couldn't give. A human can't instantly provide code, but GPT-3 can. That's another thing, you'll need to limit how fast GPT-3 can respond. It will happily provide pages of text faster than any human could type."
MachineLearning,"If it gives the wrong answer to that question that doesn't mean you can tell who's on the other end. It could be a person that doesn't know the answer, or is putting on the wrong answer on purpose. GPT-3 can also lie as shown with AI Dungeon. You can even get it to admit it lied."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Where do you end up using your manifold knowledge? I assume you use some sort of differential geometry perspective?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Elon Musk has repeatedly said that his companies care only about what you can do, and not about your degree.   Perhaps the same holds true for ML startups. (It wouldn't surprise me if big bureaucratic companies like Google, Facebook etc would care about credentials a lot more due to risk aversion).   

You do need to stand out in some way to make it to the interview..."
MachineLearning,"Yeah, same problems with the PC space. The contamination shows up in the first (largest explained variance), and all the subsequent eigenvectors, so it informs the whole thing. Thanks though."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,How did you get it? Are you building a product that needs the help of GPT-3?
MachineLearning,"Yeah, while not being the avenue OP wanted to optimize, it seems that this dimension has the highest potential for optimization and the greatest impact to their life and happiness."
MachineLearning,"Thanks for posting this. I'll save it for future references. 

I'm starting my journey into ML and have no background in math, so I begin learning statistics recently (at the same time I'm studying a book on ML with Python). I've heard this book is a nice reference for all the math required to do ML, so I wanted to know what do you guys think about it? It's a book worth reading to fill possible gaps?

https://mml-book.github.io/"
MachineLearning,"Moving from a community college to a well ranked CS programme is already hard enough, let alone ML.

Even during my time a decade ago, people who got into top 10-20 EE/CS programmers had stellar high school and college credentials, and that was before the hype.

The competition is crazy now. If you want to do CV/ML and don't want to be an SWE, consider some EE, cognitive science departments which may be working on related problems as well and may be easier to get in.

I have no idea why you are even remotely considering having your parents fork out hundreds of thousands for your PhD so you can work a 60k job? If you want to work in CV just do a Masters and get a job in the applied research sector. PhD is just terrible ROI.

Also if you are worried about a 3.5 GPA in a UC it doesn't bode well at all for a PhD. Most guys in my program were almost 4.0s in their respective ugrad programmes at top colleges."
MachineLearning,Cant find it Unfortunately
MachineLearning,Hey can you please share the link
MachineLearning,"I will be honest with you. A lot. Not just their programs but the name. I also did something similar, went to a state school, went to job fairs and google, amazon, fb, apple, netflix were not present. I went to one of the ivys for phd and the job fairs had all the faang plus more. My state school had a batter engineering program than the ivy I went to. The top rated schools tend to give more opportunities.

Also, dont get too hung up on top companies, i applied and got interviewed and even did the interview correctly but didnt get hired. Now i work as a data scientist as my same institution making about the same money and less stress. Connections are very important!"
MachineLearning,Will UT Austin/GaTech be included at the same level as MIT/Stanford/CMU/Berkeley?
MachineLearning,"It does make a difference, especially for getting your first couple of jobs. One of my friends got hired as an engineer at a local company and spent the first week explaining how git merges worked to somebody from Harvard :/"
MachineLearning,"&gt; Amazon seems to 

The individual hiring manager probably depends more than a generalization to their own company.

Many seem biased in favor of their own alma mater."
MachineLearning,"Needed to be more specific. Objects, measurements, are vectors. Positions (indices like #8053) in the vectors contain specific information. Index 8053 contains different information that index 8153, etc. I can shuffle the measurements in and out of the training set, but I can't shuffle the features inside a measurement.

Contamination appears at particular indices in the vectors, displaced from where they are supposed to be but class-specific in shape. It IS valid class information, but is not reproducible if measurement conditions change.

So it's been suggested to build a model that ignores this information. I tend to think this is cheating. Looking mostly for vague outside takes."
MachineLearning,"Ask it about any major news event that happened after GPT3 was trained? It won't, for example, know the outcome of the 2020 presidential election in the US."
MachineLearning,"I went to a free public liberal arts college in the US and got interviews (and offers) from almost all of the FAANGs. I also got interviews for Quant Research roles and other very good jobs. 

Most of my classmates had trouble getting any interviews and simply blamed it on the college being unknown. Reality is that I simply went out of the way to make many personal projects and publications while the majority of students simply go to class and don't do anything extra."
MachineLearning,Now that's a good idea. Thanks again!
MachineLearning,"Group theory, especially Lie groups, algebraic groups. I'm drastically fascinated by works of Quantum Gravity Research guys. Digging into geometry of higher dimensions is quite hard."
MachineLearning,"&gt; What's wrong with using it in your classifications and explanations? 

It's displaced in position in the feature vectors. Features should appear at specific locations in the vectors. Contamination appears at displaced locations. But it's specific in shape to the class."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,ITT: a bunch of people who read the title but not the paper. The paper is more of a case study in extreme document summarization; nobody is pretending like this will ever replace human reviewers
MachineLearning,"Are there any particular questions that come to mind that you would ask?

I am going to be prompt engineering it to produce human-like conversation."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,David McKay's course and textbook on Information Theory and Learning Algorithms are available online for free.
MachineLearning,"In your case, I agree. If a person wants to run 24/7 then it's better to get a personal one. Maintenance would be tricky though, unlike commercial cloud platforms where they hire technicians to maintain their data centres."
MachineLearning,"Not sure if my experience can help you, but I graduated from an ""ok"" University and then started a masters at the one that's the best in the field in the country.

I got indicated to a job position involving image processing and ML at a big smartphone manufacturer, but what made me stand out the most wasn't the universities, it was my personal website with several blog posts about DIP/ML and my github with projects.

I even avoided having to do live coding, one of the interviewers literally said ""I read what you wrote here, we don't need to do any live coding"".

So my personal experience was basically: focus on showing what you know. The university is important but what you do there is even more important. They asked me several technical questions about things I had written, and answering questions about things you did in the past is a million times more comfortable than answering random questions."
MachineLearning,"For Manifolds, Even I am not an expert as my understanding is the bare minimum to understand somethings in DL. There are low-hanging fruit medium articles. But even after scavenging most of those, I feel like I don't have enough understanding to use the concept the way I use self-attention or convolutions. 

For Information theory, Check the lectures of Dr.  Naftali Tishby. His lectures on the Information Bottleneck principle are where he uses a lot of these tools from information theory and that's where they made so much sense."
MachineLearning,a lot lazy ass hr
MachineLearning,"You remind me of me. First of all, take a deep breath. Don't overthink it. The fact that you will have a MS degree in a data field will help you A LOT, regardless of school. I'm a manager of a data analytics team, and honestly, I rarely even look at the ""education"" section of the resume other than to see what ""type"" of degree they have. Build up your skills, be able to talk about them in a professional setting, and be able to demonstrate that you can do things with code (Python, R, etc.) and you'll be fine."
MachineLearning,"This. What you learn and do while in school is by far the most important factor. I recently interviewed someone from arguably the best program in our shared field of work. All interviewers had overlapping notes that indicated the skills didn't seem to be up to our requirements, so they didn't get the job. Others, some from much less prestigious schools, moved forward."
MachineLearning,"Maybe Top2Vec can help you:

[https://github.com/ddangelov/Top2Vec](https://github.com/ddangelov/Top2Vec)

Just check what kind of topics this algorithms identifies for you and mabye group them afterwards againg to get your desired clusters

I asssume that in order to fit your use-case you probably have to preprocess your text data sufficiently - i.e. filter out words without relevance for your classification problem.

Good luck!"
MachineLearning,"Looks really cool! Do you have just a list of all the classes somewhere, without downloading the whole dataset?"
MachineLearning,"You seem to have a good grasp of it, but I’d add two things: being exceptional, standing out, at an “ok” school can definitely make up some of the ground. Also, getting jobs is about networking first, skill second. So so many people overlook this, while your in school, if you build relationships, talk with anyone you can, you can get an interview at these companies, then you just prove how great you are. It isn’t _all_ about the school, but it can be helpful certainly."
MachineLearning,"HR recruiters look for two types of ""top colleges"".  

If they target recruits who go to colleges with big NCAA football or basketball and ask about what fraternity or sorority you were in; they are looking for your daddy's checkbook.  

If you come from a small obscure college with solid reputation and graduates working in industry doing great things; they are recruiting you for your skills.  Elon Musk Google or Apple  don't care about your degree.  They want to know your skills."
MachineLearning,"There was a resume requirements for Twitter thing floating around on Twitter (ha) and several folks at FAANG confirmed they used the same rubric. If you search on twitter you can find it. But yes, going to a top school definitely improves your chances at an interview"
MachineLearning,"I actually managed to sort it - I run   
ffmpeg -r 45 -f image2 -s 1920x1080 -i story\_hallucinator.%d.png -vcodec libx264 -crf 25  -pix\_fmt yuv420p hallucination-story.mp4  
Instead of the code in the colab. Seems to be converting fine now."
MachineLearning,"I think there's a perception as well (that from my experience is true) that new graduates from the ""top schools"" have a better knowledge of statistical theory, linear algebra, etc. It's true anyone can pick up the ""Coursera"" bits of ML / implementations / applications, but on the job you probably aren't going to be spending much time self-learning properties of estimators, Bayesian statistics, and stuff like that."
MachineLearning,"Your goal if you want to go to a top tech company should be to secure an internship so you can get a return offer. An internship in ML might be tight (since it's very competitive these days), so I would also consider a regular SWE or DS internship. Even though they're not ML, there's enough cross-over that an internship in one is better than nothing at all.

The easiest way to distinguish yourself for ML / AI is to publish, but you probably won't have a lot of time for that, or good guidance in a Master's. I would consider trying to bolster other parts of your resume, like open source contributions. That might help for ML, but it definitely will for SWE."
MachineLearning,"No, I was talking about Richard Wei that worked in Chris Lattner team while at Google"
MachineLearning,"You can break down the problem of finding a job into two steps:  
1. Getting an interview
2. Nailing the interview

I think the major advantage of MIT et al. is that interview comes to you, especially through career fairs, alumni network, literally awesome people teaching and/or visiting for lectures/talks so you get to meet them in person.

I'm a PhD student at a Tier 1 school (Tier 0 is MIT et al.) but in the country where I'm from, my undergrad school was Tier 0 so I know the perks very well. The top companies would literally come to our feet, wine and dine us at fancy restaurants. So it's definitely an advantage.

So I'd suggest you to focus on how to improve (1). Networking is the key. There are *backdoors*. There are always more jobs than MIT et al. graduates so don't lose hope yet.

However, everybody knows these days that networking is key so they literally spam the recruiter in FAANG companies through email/LinkedIn. I've heard from many recruiter friends of mine that it become so frustrating that they don't even reply anymore. Alternative measures are required.

I assume it's easier to find info online about what to do for (2), although I'd say that space became obnoxious too. Stupid coding interviews for near-Excel positions, tests with bunch erroneous questions etc.

Finally, after interning for a FAANG company for 7 months as an ML Engineer, I decided I really want to work at a smaller company and on a more enjoyable problem. Don't get me wrong, it was an awesome 7 months. It's just a personal preference. I'm a generalist and FAANG is looking for (and the only ones that can afford and need) specialists."
MachineLearning,"Mitsuku is already able to pass some kind of tests with natural conversations.

But there's no AI out there that could pass a Turing test specifically designed to test if it's an AI or not. There's some tricks and some questions that only humans or very specifically engineered chatbots could pass.

Natural language only contains a part of the information of how the real world works. If you don't live in it, you can be able to make meaningful answers but you can't do the complex physical reasonings that humans can do. That's where AI still fail today."
MachineLearning,"Let's just say that I prefer not to discriminate by race. Ecoset's ""dog"" welcomes every kind."
MachineLearning,"I agree the ""Nvidia Tax"" is pretty skyrocket high, but the ML GPUs have benefits like error correction if you want really accurate results, not to mention it can deal with the heavy resource demands of architectural, medical, and scientific work better than gaming gpus."
MachineLearning,"It would be nice to be able to finetune GPT-3 weights using RL so that people are more likely to think it's a human, not sure if this is possible with openai's api though"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I feel like there is an area of probability/stats that comes up often in Bayesian modeling but is never covered in graduate coursework, or maybe I just started grad school too far into the deep-learning shift. For example I honestly have only a vague idea of what a Dirichlet prior is."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"&gt;that every year I spend in the States will cost me $100k.

I am confused, why does every year cost you 100k?"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Having been an engineer as well as a scientist, I can say that the boost provided by having a top college on you resume is quite a lot more for science roles (though it is non-zero for both)."
MachineLearning,how tf does it cost you 100k/year to stay in the US?
MachineLearning,"Personal story. As I can't read the brain of FANNG recruiters I have no clue about the general patterns :).  


I did my bachelor at a small unknown German university, did my master at University of Amsterdam, internship at MPI Tuebingen, now doing my PhD at FU Berlin which is a world-class uni, but not so much for ML. I recently got the offer for my dream job (research position) at \[UNDISCLOSED TOP TECH COMPANY\]. So it is possibly to get such jobs coming from an unknown college (=equivalent to a European bachelor).  


However, for me it also clearly helped having been at more well-known places in between. And it helped a lot that I was able to co-author a bunch of impactful papers (at least measured by citations) before applying. And this probably correlates strongly with those two better known places (Amsterdam, MPI) as this clearly prepared me to the level of research I am doing now.  


Regarding online courses etc.: I think they are great resources and I learned a lot from those (and books!). However, the benefit from Amsterdam and MPI was mostly having smart peers around, working on actual stuff, reading/implementing/understanding/discussing papers. So I am not 100% convinced that online classrooms completely substitute that experience. I don't think that it would have worked for me personally...   


Nowadays I think career building is to a large extent luck, and to another large extent grit + time. If you lack luck, you probably need more grit and time to get into places that you long for. And it is quite possible that it still doesn't work out - so having a plan B is good for your mental health :). Also think in stepping stones / ladders. It makes no sense to aim for the highest but perhaps to bubble up. I see my personal career that way: a mix of random factors, a lot of hard work and greedy local optimization. 

  
Regarding your personal situation: why not do a master program first before aiming for the next steps? If it is about money, consider Europe (they have very competitive schools which comparably cost you small money). After a master at a school with a strong ML department you have a lot more options just because of your network."
MachineLearning,"Academia is very hit or miss, funding dependant and usually hires in waves, but AI skills are certainly in demand in business and will continue to be so.

Keep a good portfolio together and only consider the jobs that meet your income requirements.

Often the gap is the bridge between the programming database side and the hard core ML side,  so it can only help to play with input and output manipulation, or pick away at learning related skills. Ideally you would be able to guide newer employees in the right direction, even if you don't have hours of experience doing the coding yourself.

I was reading an opinion about identifying and programming the narrow niche areas where ML could be successful. A lot of smaller companies simply don't know where they could benefit from ML or can't screen candidates, so presenting your skills as a specific short term business proposal with clear benefits might hit more marks.

Another continuing problem is lack of data. The company might be producing hundreds of items, but with insignificant internal tracking there is no way to leverage ML on the pipeline. This isn't 'your problem' per se, but it is an area that will present barriers. Finding a workaround here could lead to wider opportunities."
MachineLearning,"So here's something you can try that will also give you a chance to see what feature is considered important for the model. 

You have a small dataset so just convert categorical data to numeric data, while normalising you have to normalise different columns separately because after using LabelEncoder those columns are already normalised. So you will end up with 19 features if I counted them correctly and that's a small dataset.

I would use a Random Forest, SVM or something similar to those for this dataset and then you can get feature importance for each feature in your data then you can try taking out a column or two which have low importance (define a threshold experiment a bit with it) and see what changes. Also, use a collection of different metrics like AUC or F1-scores don't rely solely on Accuracy.

Since you have a small dataset you can experiment a lot very quickly so do that and you will learn a lot from that."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,You absolutely need to understand certain topics in real analysis and measure theory to be able to approach manifolds and differential geometry. It’s not really something you can dive into without the background and really understand it
MachineLearning,You absolutely need certain real analysis and measure theory topics to understand manifolds and differential geometry
MachineLearning," 

### What should be Roadmap to learn AI/ML

&amp;#x200B;

I have 4 years gap after B.Tech (ECE). What should be Roadmap to learn AI/ML? Should I pursue M.tech in CSE/AI or should I take a 1-year online course?"
MachineLearning,"They convert the work of research scientists into production code ready to be deployed and tested in real-life products. 

This is actually my dream job but it is arguably even more competitive."
MachineLearning,"For schools where professors are allowed to admit students themselves connections go a long way. You've got 3 ways to do this: know someone who knows someone, reach out to professors and engage with their work in a meaningful way, attended a conference (hard atm) and network like mad."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,What the fuck is this? I said the paper's great! 😂
MachineLearning,"How dare you get fed up with dogs XD

But this looks like an interesting dataset"
MachineLearning,Could you elaborate what research engineers do? I’m interested in both the research and application part of ml
MachineLearning,"Yeah. If you are into SWE that sounds good.
Although I dont know if you can work in a top company and do MS at the same time so you will start doing applied ML after some time."
MachineLearning,"If you wanna go into industrial research (Be a research scientist or research engineer in a company's Research arm), then yes, it matters.

If you want to be an engineer who builds ML products, then no it doesn't matter. E.g. you wanna be the team that implements Pinterest's content recommendation feed them you're fine. If you wanna be the team that publishes abstract papers on new Transformer architectures, then academic reputation and output is everything."
MachineLearning,"Did you quit medical school in the US and went back to community college? I am just curious. On a side note, the ranking of the undergraduate institution you attend isn't as important as you think. What matters the most is to get amazing recommendation letters and research experience. I don't mean good letters, I mean you need ""amazing"" ones to get into top schools. You might not able to get an amazing letter if you go to place like UC Berkeley, since all the kids there would be extremely competent and you might not stand out as much. Doing a smaller program where you can build more intimate relationship with professors would be better in my opinion."
MachineLearning,"Nah. Machine Learning Engineers (the broad title for roles such as 'software engineer in ML', 'applied scientist', shit like that) don't need a strong research background, or any publishing. Having a practical demonstrable project-based background (internships, prior experience, personal portfolio) is probably better than a publishing background

Research scientists absolutely need a strong research background, and the more reputable your academic faculty is, the better.

Research engineers are a mix. It definitely helps."
MachineLearning,"Not much.

Top tech companies have loads of foreigners coming from universities that recruiters and peers might never have heard about. And yet, those companies relies on these foreign brains quite a bit.  
However, the MIT/Stanford/Berkeley/CMU grads might be better equipped at succeeding at the interview or on the job, at least in the short term.

But it's possible that recruiters filter US universities because they feel they know the quality, but blindly accept all foreign name due to their inability to rank them..."
MachineLearning,AFAIK there’s no royal road (would love to be proved wrong here). All the college courses I’ve seen on the topic require a year of real analysis as a prerequisite.
MachineLearning,Depends. Def. an advantage if you’re from an elite school since top recruiters recruit there. You may have a “regional” advantage if your school isn’t prestigious but there are “good” companies that recruit mostly from that school in your area. This would be similar to the prestige advantage except its local and to a set of companies in your area that may or may not be prestigious. I think going to a better school is “better” in the sense it’s usually more rigorous and you get more out of it. Once you obtain industry experience I think your school brand matters less and less. For academia your school brand absolutely matters.
MachineLearning,Research papers required for even Applied ML jobs?
MachineLearning,"&gt;Take an ML view on the problem: it's very hard to optimize from a score of .95 to .99 and even harder to go to .999 from there. Obviously there is a lot of talk about the highest scores and the people who got there, but .95 is sufficient for a lot of applications. Including having a fullfilling and satisfying professional life.

To add to this in any industry a guaranteed 80% success with a proper understanding of failure cases are a lot more important that getting 99% success rate with no intuition about the success and failure cases.   
Even in small tasks the realworld is lot messier and difficult to solve using ML than standardized datasets in many cases imho."
MachineLearning,"This seems to support my observation looking at LinkedIn. Amazon seems to not focus too much on college names for their Applied Scientist roles, but other companies tend to focus on it.

Another route is to work as a Software Engineer in the ML Infra Team at a FAANG and then transfer to applied ML. Supposedly the applied ML teams value system-building skills and experience in ML Infra helps in that case."
MachineLearning,"If your salary goal is $60k, you don't need a PhD. If you get a PhD in anything and are knowledgeable about ML (enough to do work, not necessarily enough to have published any research) then you can find a good job. You're worrying a lot about reaching the absolute top of the industry, which is a good goal, but there are plenty of opportunities for success below that."
MachineLearning,"It matters in the sense that companies will recruit at programs they know. However, how you craft your resume and your experiences/skills matter alot more than where you goto college."
MachineLearning,"Yes. You do. For top ai labs that do research, publications are valuable. Publish good papers in top conferences. If people can recognize your work, you'll be a far more attractive candidate than someone from a top school that has substantially lesser research experience."
MachineLearning,"Name checks out!

Thanks for adding a bit of depth along with your argument"
MachineLearning,"&gt;The Math concepts that we need to learn machine learning are.1- Linear Algebra 2- calculas 3- Probability and statistics.

What you have written is like the most basic top level concepts. I guess a deeper answer would be better for this since each of the topic is a field by itself."
MachineLearning,"It think the contrast between you modest (not meant as an offence) goals and your fears is quite large. Also you have something you seem to have passion for (computer graphics), which is always great. I don't know about regional specifics (from europe), visa and so on, but this sounds very much doable. You should be aware that most ML jobs are not pure research (and even pure research jobs are probably not about pure research), so skills in software engineering, DevOps, data handling, communication and whatnot will go a long way. Just adding this because you said you don't want to do software engineering and from my perspective students in IT (also those who explicitly want to do SE) often have a very narrow idea of the translation from their interests into a professional role (which often consists of very different things especially in more senior positions)."
MachineLearning,This comment is absolute BS. Using a gaming GPU is much cheaper as it avoids the 'nvidia tax'. Basically gaming gpus cannot be used in datacenters like gcp and enterprise cards with ecc are priced very high when there is little difference between them. This is the reason why buying a gaming gpu to do deep learning is almost always worth it as it avoids the tax. As you said an rtx 30 series card can be got for $500 when renting this will last you 2 may be 3 months.
MachineLearning,"ML moves fast so if you do supplement your learning with self teaching, that's a bonus way to sell yourself. You are the guy that will learn the new and better way, on your own."
MachineLearning,"Disclaimer: I don't know anything rigorous about any of the following, but ...

I always think about how exiting it would be to learn more about the possible intersections between Gradient Flows, Game Theory, Information Geomtry, Random Matrix Theory, Dynamical Systems and other fields with ML."
MachineLearning,"I think top colleges still help you in becoming more competent, even that today they are many online resources. One major thing is their tougher evaluation which will force you to study more. It is not just a difference in the brand.

I did a very bad Msc in eastern Europe and after some industry experience (1.5 years) I got interviews at bytedance and amazon in UK. Other FANGs rejected me. So yes, it matters a lot, but more so at the beginning."
MachineLearning,"Exactly, judging by the content of the post there is going to be A LOT of inefficiencies in the model you want to build, and hiring a simple consultancy or a good ML developer will save you a lot of time and money."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I had measure theory based probability theory. I learned how to prove a lot of things but I really struggle to apply the concepts to real-world problems (markov, monte-carlo, sampling from distributions, etc., basically all the stuff you need for ML/DL)."
MachineLearning,"For my master thesis i am working on autoencoders.

So i was looking for information and i found [this](https://wiki.pathmind.com/deep-autoencoder) artikel which claimed that the first layer of the autoencoder should be larger than the original layers.  
It makes somewhat sense, but he does not have sources or anything. 

Does anybody know of a paper or different source about this, or does someone have experience with a larger first layer?"
MachineLearning,"The number of categorical features you have should be fine assuming you have even a few hundred observations. 

You don't really have to do manual feature selection. Most models have some feature selection built in through regularization. For linear regression that would be L1 regularization. Tree models will select the best features as part of training and use regularization to determine when to stop adding lower importance features. kNN doesn't so you need to select with some other process such as PCA.

You don't need to normalize one-hot features for linear models. For tree models you don't need to normalize any features."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"We don't need to make our benchmark functions very expensive in order to evaluate that an algorithm works on a function of that type, adding  a 2h sleep to the function does not make it more difficult for the optimizer, it just makes it more difficult for US SCIENTISTS to get a good sample size to assess performance.

In BBOB, the default plots will only generate ""performance at given budget"" results and if you consider your function to be extremely expensive, you just take a look at algorithm performance at that budget, e.g. the left part of

http://coco.lri.fr/COCOdoc/_images/examplefigure_all.png"
MachineLearning,"If you flick through some of the trickier structures on the CASP website you can find several where AlphaFold performs poorly, and worse than other methods.

Theirs is a colossal step forward, but it is not a solved problem yet - because this implies perfection in the manner of a mathematical proof."
MachineLearning,Thank you for the reply! If my goal is 1)not going broke 2)finding a job that pays more than $60k 3)working on the application of machine learning and other techs on computer graphics 4)settling in the States/Canada Do you think this would be doable? I don't wanna be doing software engineering jobs.
MachineLearning,"Spend less time on reading those horror stories will probably help. They are an echo chamber. Yes, the top 0.001% of every field is very competetive and become a top level-researcher that will develop the next big think depends mostly on luck. 

But there is a totally different side to it: machine learning is a large growing field, governments , companies and institutionsall over the world have increasing demand of people with skills in this field. There are a lot of PhD opportuneties apart from the top 20. 

With solid IT skills  (that does not even have to include ML) you can have a great career even if you did not start programming before you started talking or whatever people want you to think is the norm.

Take an ML view on the problem: it's very hard to optimize from a score of .95 to .99 and even harder to go to .999 from there. Obviously there is a lot of talk about the highest scores and the people who got there, but .95 might by sufficient for a lot of things. Including having a fullfilling a satisfying professional life."
MachineLearning,"What are the contributions most of us made to protein folding ? What makes you or me a worthy critique if this is the bar you want to set ?

LeCunn has made incredible contributions to exactly what this subreddit is about, Machine Learning. He has a Fields Medal."
MachineLearning,"I am a bit skeptical. DeepMind is also a HUGE PR powerhouse, with a (far) worse track-record of opening their research (cue AlphaZero) than FAIR. AlphaFold, while offering leaps and bound better performances did not really ""solve"" the problem completely.

If you look at the Nature press article, it explains some of the limitations : 

&gt; Some predictions were better than others, but nearly two-thirds were comparable in quality to experimental structures. In some cases, says Moult, it was not clear whether the discrepancy between AlphaFold’s predictions and the experimental result was a prediction error or an artefact of the experiment.

&gt; AlphaFold’s predictions were poor matches to experimental structures determined by a technique called nuclear magnetic resonance spectroscopy, but this could be down to how the raw data is converted into a model, says Moult. The network also struggles to model individual structures in protein complexes, or groups, whereby interactions with other proteins distort their shapes.

It may very well be that ""solved"" is not an accurate representation of the situation. Just that there was a leap in performances, same as what happened with CNNs on the ImageNet challenge."
MachineLearning,Stop throwing company titles to justify what his intellectual levels are. Maybe talk about actual contributions he has made to protein folding.
MachineLearning,"I think I do. I read up on why there was so much hype during the initial release of AlphaFold and how could DM use such bold language of ""solved the protein folding problem"". You will understand further if you actually read what Prof. John Moult (founder and chair of CASP) says. When you say a method is now recognized as a solution to a problem, it means it the problem (within the scope of whatever the problem being defined means) is solved."
MachineLearning,"Functional Analysis - Honestly, I wish I could juggle with abstract n-space theorems on the fly. Such a cool thing to do, don't know how it all relates though, but like many things in maths, it just requires a clever physicist to be creative and suddenly magic happens."
MachineLearning,Pretty ironic that after making a top-comment about busting someone for talking about something he does not know anything about ... you do too.
MachineLearning,"Do you really believe Facebook is ""one of the most evil companies ever"" though ? Social Media may have a pretty bad effect on polarization, which deteriorates democracy, but the case is often overdone. There would still be heavy polarization without facebook.

On the other hand, there are thousands of companies which knowingly use child labor, do no respect basic safety for their workers, lie about the existence of climate change, fraud people en-masse."
MachineLearning,"This is ... the case for a lot of renowed scientists actually. Bengio also does it when he talks about ultra-pessimistic climate change scenarios.

I would also say that as the Scientific head of FAIR (or whatever is his tittle now) and an overall pretty curious person, he has more than ""no fucking clue about"" some of "" protein folding, but democracy, governance, science, quantum chemistry""."
MachineLearning,"Very helpful! To be honest, I have CS background with 1+ year learning DL, but sometimes this matrix transformation still confuses me!"
MachineLearning,"224x224. But I don't think you want to run any model on that resolution that you reference in your post. First of all, you should ask your ML engineer/data scientist how to approach your problem, downscaling or slicing the big images into smaller pieces. As you pointed out you are not an ML/DL professional. Even if you did not hire anybody yet, you can spare a lot of money by asking/consulting the right person for directions by investing some bucks before dropping any hard cash on anything. DL can be a money sink easily. DL/ML is not just pushing a button and get a solution. There is much in-depth knowledge in the actual experiences of any senior deep learning practitioners with computer vision domain expertise."
MachineLearning,"&gt;\[Discussion\] Is it possible to solve optimization problem with Neural Network? \[Discussion\]  
  
  
.t3\_llvo3s .\_2FCtq-QzlfuN-SwVMUZMM3 {  
\--postTitle-VisitedLinkColor: #9b9b9b;  
\--postTitleLink-VisitedLinkColor: #9b9b9b;  
}  


I don't know anything at all about ILP, could you describe a step by step of things to study to try to solve this problem using ILP?"
MachineLearning,"To get some ideas on how to approach this I'd recommend checking out this kaggle dataset: https://www.kaggle.com/kazanova/sentiment140

There are lots of good discussions in there and code examples of how to learn sentiment from a labeled dataset"
MachineLearning,"Yup. Python is os agnostic and you can even set up Nvidia gpu acceleration on windows. 
Ultimately, you will find that to actually have control over your environment and to have an environment that is consistent with other devs you'll likely want to move to Linux. 
That being said, I wouldn't let the os get in your way of starting."
MachineLearning,What resolution are the images ?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"The Math concepts that we need to learn machine learning are.

1- Linear Algebra 
2- calculas 
3- Probability and statistics."
MachineLearning,"Looks good! I''ve registered.

I'm just wondering if anyone has any experience of a Grey Campus bootcamp? Or any free bootcamps for that matter?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Well it depends on the area of ML. It is a broad field of research and the math concepts are mostly based on your goal. Do you want to do word embedding for some fancy NLP? Then you should have a deep knowledge about vector spaces and higher dimensional rooms. Do you want to build convolutional neural networks? Then try to understand matrix multiplication and linear algebra. There is no clear answer to this question. However, a early understanding of linear algebra especially matrices and vectors are fundamental for most ML applications. Also you should learn to read mathematical formulas and term like ""transpose"" or ""dot product"" should also be known."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"+1, manifolds in particular, I tried looking at some differential geometry but I keep falling off"
MachineLearning,Typing the SSH port forwarding command literally takes 10 seconds. Should be simple enough and is equally convenient
MachineLearning,You want the model to work with imperfect information so that you can test it's performance with the rest of the information. if you fit it to all the data you get the best fit but you have no idea if it will fit alright for other future observations.
MachineLearning,"You make good points, but let's first get one thing clear. There is nothing to ""think"" here. What I think doesn't even matter. It is not even ""subjective"" when the holders of the competition themselves declare that the method is now seen as a solution. I am stating a fact.

Regarding your point about proteins with &gt; 100-150 amino acids or multi-domain proteins, yes, it certainly could be the case more scientific progress needs to be made. That's irrelevant however to the context in which my original comment was made. Is FAIR showing results on these benchmarks? If yes, then me saying the FAIR work is overrated and hyped by centering it on DM already having solved the problem, is bad. 

And no, I don't think GPT solved language. But the manner in which it is being said that AlphaFold has solved Structure prediction is quite different."
MachineLearning,"Too many to list. I am finishing up an undergraduate Math degree while working in ML full time, and I feel wholly unprepared for any analysis more complex than basic intuition."
MachineLearning,"All alphafold has accomplished is an improvement in prediction accuracy for small single-domain protein structures. The problem of protein folding is not limited to small single domain proteins. Multi domain proteins exist. Proteins with &gt;100-150 amino acids exist. To say that “anyone sane” believes protein folding to be “solved” suggests unfamiliarity with the problem itself. Even for small single domain proteins, their model can still fail spectacularly. Not to disparage their work - it’s clearly an improvement in the state-of-the-art for de novo structure prediction - but it’s nowhere near a full solution.  

Do you think GPT “solved” language?"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,No player = no game
MachineLearning,I expect it to be out soon. Alphafold 1 was published.
MachineLearning,"Can't agree more. The other two Turing awardees, Hinton and Bengio, are busy contributing to scientific research still and publishing papers with their collaborators at Google and MILA, while this clown's constantly parading around like a world expert on topics he has no fucking clue about - not just protein folding, but democracy,  governance, science, quantum chemistry (he raised a false alarm recently about Google's quantum supremacy claims being invalid or something), NLP.. can keep going on.. anyone who follows him on FB knows this. Sad part is people trust his lies and fake news because he is a Turing Awardee, instead of decoupling that he has made seminal contributions to image recognition and processing, but is in general an absolute clown and fake news seller."
MachineLearning,"Not exactly. There is nothing ""unsupervised"" about this structure prediction at all. Another one of LeCun's false hypes sadly. 

If you're familiar with the self-supervised learning literature in computer vision, there's a test called the linear probe on features, wherein you train a linear classifier on the unsupervised pre-trained features. The linear classifier is trained with all the available labeled data. 

That's what is going on here. They train a linear model on top of unsup. features. It is more of a probe test. Not like it recovers the structure of the protein totally unsupervised in an emergent way or anything. Still supervised as far as structure prediction goes."
MachineLearning,"Not exactly. The CASP community has recognized AlphaFold as a ""solution"" to the protein folding problem whereby it is extremely close to angstrom level accuracy of experimental crystallography. To me and pretty much anyone sane, that means it has solved the ""protein folding problem"", where the problem is defined as learning to predict the 3d structure of a protein from its amino acid sequence to precision at the level of actual lab experiments."
MachineLearning,“solved the problem” is a massive overstatement
MachineLearning,Neat. Faster and slightly better. I imagine it is useful for more global image tasks. I wonder if it could improve NetVLAD image retrieval.
MachineLearning,"I guess some tweaks to attention mechanisms may help, like the ones used in linformer, shortformer, etc. Sparse, local and other kinds of attention  approximation"
MachineLearning,Hehe yeah that would be bad huh...
MachineLearning,"&gt; Make peers on Twitter

No. Fuck twitter."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,lol history of technology
MachineLearning,"I experienced both during my phd: 

(1) research that was a 100% team effort to the point where we would work in the same office the entire day, sometimes in front of the same screen, sometime everyone writing one piece of code which we discussed just before and in the evenings we would discuss work over a few beers several times a week. Writing the paper, we would discuss every single sentence.

(2) research where I was basically on my own (expect for meetigns with my supervisor) and still met the same peers over beer and watching champions league but now they were working on something completely different.

In retrospect, I would say the second part (alone) was several times more efficient w.r.t. work done per time. However, the first part lead to higher quality output. Every thought having to be expressed and discussed with others is incredibly time consuming but obviously also increases quality.

Now, some year later, I would say I learned so much more during the team effort that I find it sad that PhDs in general are individual efforts and everything from thesis, to grades to personal titles is so individual. My current employer (went to industry) surely benefits a lot more from what I learned during the other half."
MachineLearning,Is there any resource on the web that gives a good information of this topic? I always wanted to learn more about it.
MachineLearning,"I found most comments nonsense here.  You have \_1400\_ images for training. Even if you buy an older, secondhand GTX 1070/1080, it will handle this small dataset with ease. I would not consider cloud services and the hassle with them, just buy the cheapest GPU option from ebay. I train ImageNet on small deep learning models in a day. And it is 1.28M images."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"You mean this one?
https://arxiv.org/abs/2101.11605

I still don't see the revolution coming, attention in the last layers of a convolutional resnet can tweak performance a bit and performs better than ViT/DeiT. 

If you don't agree what should be changed in transformers to work with reasonable amounts of data? Maybe add an inductive bias for visual data and name it Filter Transformer? :D Or should we just hope that transfer learning is always enough for all kinds of tasks?"
MachineLearning,"Very nice presentation of the results of this paper by Tom Goldstein here:

[https://www.youtube.com/watch?v=kcVWAKf7UAg](https://www.youtube.com/watch?v=kcVWAKf7UAg)"
MachineLearning,"Title:Understanding Generalization through Visualizations  

Authors:[W. Ronny Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+W+R), [Zeyad Emam](https://arxiv.org/search/cs?searchtype=author&amp;query=Emam%2C+Z), [Micah Goldblum](https://arxiv.org/search/cs?searchtype=author&amp;query=Goldblum%2C+M), [Liam Fowl](https://arxiv.org/search/cs?searchtype=author&amp;query=Fowl%2C+L), [Justin K. Terry](https://arxiv.org/search/cs?searchtype=author&amp;query=Terry%2C+J+K), [Furong Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+F), [Tom Goldstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Goldstein%2C+T)  

&gt; Abstract: The power of neural networks lies in their ability to generalize to unseen data, yet the underlying reasons for this phenomenon remain elusive. Numerous rigorous attempts have been made to explain generalization, but available bounds are still quite loose, and analysis does not always lead to true understanding. The goal of this work is to make generalization more intuitive. Using visualization methods, we discuss the mystery of generalization, the geometry of loss landscapes, and how the curse (or, rather, the blessing) of dimensionality causes optimizers to settle into minima that generalize well.  

[PDF Link](https://arxiv.org/pdf/1906.03291) | [Landing Page](https://arxiv.org/abs/1906.03291) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/1906.03291/)"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Actually u should check the pricing again [https://cloud.google.com/compute/gpus-pricing](https://cloud.google.com/compute/gpus-pricing) shows that a v100 would cost $1200 usd per month which is about $600 per month for 12 hours a day. I would suggest to buy a gpu if you plan for long term use.
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"This^

I would rather fully understand all the basics than one advanced concept."
MachineLearning,vgg19 is fairly light model taking only about 512mb in space with at most (x3 more in gpu ram due to maintaining gradients in adam like optimizer). I think it's possible to get away with a smaller batch size without changing your gpu just rmb to scale your learning rate by the same amount.
MachineLearning,"Bayesian optimisation algorithms are typically not run on BBOB from COCO because those functions are not expensive, and hyperparameter tuning is a lot more complex. Although often in papers people use some of the typical functions like Rosenbrock to test a hypothesis."
MachineLearning,Porque no los dos?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Can I do deep learning on windows? I can't seem to install Linux, I get an error message every single time 🙃"
MachineLearning,Yes while I see zero improvements in these developments for real practical use cases in the industry.
MachineLearning,"Well NeurIPS has almost always had a deadline towards the end of May, so I'd say sometime between May 24~28"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,really
MachineLearning,"thanks for this. i'm a physicist, not a deep learning person, but einstein notation and tensor operations are familiar enough that having them intuitively encoded in a python package is very welcome, and i wouldn't have known about it without your comment, so much appreciated"
MachineLearning,Random matrix theory
MachineLearning,Exactly! Spent only 3 days to model it. :D
MachineLearning,"Why do a task in 5 minutes when you can spend hours automating it?😂

Great article though!"
MachineLearning,"Title:Open-Retrieval Conversational Machine Reading  

Authors:[Yifan Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao%2C+Y), [Jingjing Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+J), [Michael R. Lyu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lyu%2C+M+R), [Irwin King](https://arxiv.org/search/cs?searchtype=author&amp;query=King%2C+I)  

&gt; Abstract: In conversational machine reading, systems need to interpret natural language rules, answer high-level questions such as ""May I qualify for VA health care benefits?"", and ask follow-up clarification questions whose answer is necessary to answer the original question. However, existing works assume the rule text is provided for each user question, which neglects the essential retrieval step in real scenarios. In this work, we propose and investigate an open-retrieval setting of conversational machine reading. In the open-retrieval setting, the relevant rule texts are unknown so that a system needs to retrieve question-relevant evidence from a collection of rule texts, and answer users' high-level questions according to multiple retrieved rule texts in a conversational manner. We propose MUDERN, a Multi-passage Discourse-aware Entailment Reasoning Network which extracts conditions in the rule texts through discourse segmentation, conducts multi-passage entailment reasoning to answer user questions directly, or asks clarification follow-up questions to inquiry more information. On our created OR-ShARC dataset, MUDERN achieves the state-of-the-art performance, outperforming existing single- passage conversational machine reading models as well as a new multi-passage conversational machine reading baseline by a large margin. In addition, we conduct in-depth analyses to provide new insights into this new setting and our model.  

[PDF Link](https://arxiv.org/pdf/2102.08633) | [Landing Page](https://arxiv.org/abs/2102.08633) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.08633/)"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,That's... amazing!
MachineLearning,that's one way to put it lmao
MachineLearning,"Dont hate the player, hate the game"
MachineLearning,"Yann LeCun is a tone dead ass clown pretending to save the world, while raking in millions a year at one of the most evil companies ever."
MachineLearning,"I've found this more recent paper,  thouh there is no code. [https://arxiv.org/pdf/2010.03019.pdf](https://arxiv.org/pdf/2010.03019.pdf)

They replace all CNNs in resnets with their Global Self-Attention blocks and seem to get better results than resnets with CNNs+SA blocks."
MachineLearning,Great works for GAN applications in computer vision
MachineLearning,"Information Theory and stuff around mutual information gain, Shannon entropy, KL-Divergence etc. 

Manifolds and how to truly ground the understanding of manifolds. I learned all this after/during the time I learned AI/ML. But if learned earlier it would have made DL even more understandable."
MachineLearning,"Einops is Brilliant. After learning Einops, I can never go back to normally decomposing the same computation."
MachineLearning,"I m a beginner myself. Maybe this can help: 
https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e"
MachineLearning,Isn't this the unsupervised version of the problem? The DeepMind paper dealt with supervised structure prediction.
MachineLearning,This should get you back to a more intuitive understanding: [https://github.com/arogozhnikov/einops](https://github.com/arogozhnikov/einops) It don't reshape/flatten/reduce without it now.
MachineLearning,Has the alphafold2 paper came out? I want to read about it.
MachineLearning,"Some suggestions:

1. You can still bounce ideas off the other lab mates, or spend some extra time on one of their projects. Unless your advisor is actively trying to wall people off from each other, no reason you can contribute a small amount to others / others to yours. Just ask/offer. 

2. Try and recruit  an undergrad or MS student to join your project. Become their mini-advisor, and now you've got someone to bounce stuff off of and help make progress."
MachineLearning,"Hmm, there are so many.  Most of which I learned in graduate level ML/NLP courses which blew my mind at the time. Stuff like EM algorithms, MCMC, statistical Bayesian theory.  All of this stuff helps out immensely when looking into “shallow” machine learning"
MachineLearning,"These numbers dont include preprocessing or NMS. I directly fed a correct, model-compatible sized zero tensor(Batch\_size,3,1280,768). Can anchoring in the last layer be causing this issue?"
MachineLearning,"Wrapping up PhD in next couple months (in NLP), other than what has already been suggested, 1) create a reading group that covers your interests but general enough to get others interested, with the goal to have good discussions and 2) cold email other PhD/graduate students at other institutions who have written papers that you like and setup time to chat. Might end up just being a chat, perhaps turns into something more."
MachineLearning,"I felt the details on controlling array structure in numpy were a bit lost on me. I understand the general concept of vectorization and how things like indexing work for slicing arrays, but I still feel weak on knowing when to use methods like flatten or other array size/ dimension conversion techniques. If anyone had a good resource on this, I'd be very grateful."
MachineLearning,"YOLOv5 pytorch hub models show about a 2X speedup at batch size 16 vs batch size 1 on Colab, but of course results will vary by your specific use case of course. For details see:

[https://github.com/ultralytics/yolov5/issues/1806#issuecomment-752837988](https://github.com/ultralytics/yolov5/issues/1806#issuecomment-752837988)"
MachineLearning,"Generally, batch size increase results in a delay of 1.5X \~ 2x inference depending on the batch size, but a linear increase could be simply because of many reasons. 

1. Dataloader (PIL &lt; Numpy \~ OpenCV) | (GPU &lt; CPU).
2. pre-processing step (Resizing big image &gt;&gt;&gt; small image, different types of crop, etc).
3. the inference code (after prediction how is the result being processed)."
MachineLearning,Have you guys contacted Quoc Le?
MachineLearning,"My PhD was not in ML per se, but close enough (applied math + engineering). Lab with 20 members, and everyone except me had overlapping projects to discuss and learn from each other. It was a very isolating experience.. but I think that made a *better* researcher as I had to figure out everything myself and chart my own path. Is it hard? Yes. Frustrating and feeling that it’s almost unfair (when you labmates coauthor multiple papers together and here you’re struggling with every paper on your own)? Yes. But I think the attitude I developed there helped me be very independent as a postdoc, as I already knew how to chart my own path from my PhD. I had a good advisor though, but he was too busy to focus on me a lot. As long as your advisor is supportive, you’ll be fine. Just remember you’ll feel annoyed often, but it actually makes you a better researcher in The long run."
MachineLearning,I will probably take the SWE route. Thanks for your concern btw.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,It's a valid concern. It's also 4+ years of your life. I was lucky to join another professor's startup (a different university).
MachineLearning,"[Fixed formatting.](https://np.reddit.com/r/backtickbot/comments/lmesvl/httpsnpredditcomrmachinelearningcommentsllsrrld/)

Hello, ductm104: code blocks using triple backticks (\`\`\`) don't work on all versions of Reddit!

Some users see [this](https://stalas.alm.lt/backformat/gnuttvx.png) / [this](https://stalas.alm.lt/backformat/gnuttvx.html) instead.

To fix this, **indent every line with 4 spaces** instead.

[FAQ](https://www.reddit.com/r/backtickbot/wiki/index)

^(You can opt out by replying with backtickopt6 to this comment.)"
MachineLearning,"[Loopback address](https://datacadamia.com/network/loopback) 127.0.0.1 contains similarities to a property in Phi:

1601 digits precede the first occurrence of ""12700"" in Phi

𝘈 𝘱𝘦𝘳𝘧𝘦𝘤𝘵 𝘭𝘰𝘰𝘱: 1601 first appears in Phi at the end of 14145 digits and 14145 first appears in Phi at the end of 1601 digits.

---

^^**Note**: ^^1601 ^^first ^^appears ^^in ^^[**γ**](https://formulasearchengine.com/wiki/Euler%E2%80%93Mascheroni_constant) ^^at ^^the ^^end ^^of ^^12700 ^^digits

   ^^^I ^^^am ^^^a ^^^bot. ^^^I ^^^Like ^^^Pi.  ^^^3 ^^^· ^^^14"
MachineLearning,"I often use screen to run port forwarding in background. And use this command to reconnect if have internet issues.

```

while True; do; ssh -NL 8888:localhost:8888 user@host; sleep 1; Done

```"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Fast marching method in general applies to all level set problems. Eikonal equation is just the application. And there is always uncertainty in I'll posed problems. I am a seismologists btw.
MachineLearning,Thanks for the tip! I didn't know that. I have been using this form with different ports for when I need to access services across many machines with services on the same remote ports.
MachineLearning,"www.football-data.co.uk/data.php

It's a pretty good dataset and it's updated as soon as a game week is completed.

To let you know what data points are important, I think the betting odds for the match are the most important. That's because the odds factor in almost all of the data points possible including players and home/away events. They reflect a general sense of expected outcome and the algorithm agrees as the correlation between teams scoring high &amp;/ winning and odds is very high.

You can explore this arena and we can collaborate.
We (yes, there are more than a few developers working on this 'opportunity' lol)and I would be glad to add you to our discord. 

DM me if you would be interested."
MachineLearning,I don't think this is the right reddit. Doesn't Fast Marching method apply to the eikonal equation? It's equivalent to optimization but for a very particular cost function. Where does the uncertainty come from for your problem?
MachineLearning,"Damn I'm in the same boat, but am afraid to burn bridges by dropping out."
MachineLearning,Is sample complexity a defined term or are you using it loosely? Where can I read more about this?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Hi, thanks for your interest in our work. I did experiment with CURE regularizer with different activations, but didn't obtain any significant difference compared to ReLU, in terms of robustness. Yes, the Fig. 3 reference is max eigenvalues of Hessian wrt Input. A reference to the figure is provided in Section 4.1 near the end."
MachineLearning,collaborate with your labmates. overcome the fear of someone else taking credit or running with your idea.
MachineLearning,You sure you are not just BiGAN? Or BigBiGAN?
MachineLearning,"This is great, thank you!"
MachineLearning,"Also, if this is too beefy of a question for this thread, I'm happy to move it just let me know"
MachineLearning,"I need some help understanding some maxout notation I am seeing in a paper since I am trying to replicate that paper's results. I am really confused and would be super appreciative of any help I could get.

&amp;#x200B;

I am trying to build a handwriting recognition model based off of the paper ""Intelligent Character recognition using fully convolutional neural networks""

In it, they suggest that one should use maxout in the symbol prediction layers.

In the paper they have next to one of these layers: 

fc\_final                                                                Their name of the layer

(1024)                                                                 Output features

 4x4x512                                                            kernel size and input features

dropout, .5                                                        dropout to be applied after this layer

**maxout x2                                                         HELP!**

1x2 pad                                                            1x2 padding

ReLU                                                                 ReLu activation

&amp;#x200B;

From what I can tell maxout when used against the output of a CNN takes all of the output filters and takes the max value from all of them reducing that output down to a single output filter. Currently, I am using this implementation of maxout: [https://github.com/paniabhisek/maxout/blob/master/maxout.py#L101](https://github.com/paniabhisek/maxout/blob/master/maxout.py#L101)

This is all well and good but the layer after this one also has the maxout x2 notation on it and it is expecting 1024 filters going into it and I believe is supposed to output more than one filter itself.

So here are my questions: How do you apply maxout to the output of a CNN that doesn't reduce the number of output kernels? And is that even a valid question? What does the x2 notation mean next to maxout? 2 maxout layers? Only reduce 2 channels into one?

&amp;#x200B;

I'm sorry I'm a novice here. But I would appreciate further reading material on maxout and any ideas. Thanks for any help I can get."
MachineLearning,Nice..Does using these smoother activations in conjunction with cure or even more typical local linear/lipschitz regularizers do anything? Also no reference for Fig 3 in the main text - are these the max eigenvalues of the Hessian wrt the input?
MachineLearning,"Interesting - I've been using ray for parallelized processing of HDF5s, and the best solution I had (because a single process locks the hdf5) was to make a bunch of copies of the hdf5 so that each ray process had its own. It's a trade with hard-drive space, but in my particular case a worth-while one. Does this update get around that?"
MachineLearning,TIL!
MachineLearning,"Yes, well spotted, many thanks. I will fix."
MachineLearning,"Great point. But this is a very different setting. LACE is not a time series (most people will only have it computed a few times in their life). The ETSC world has strong time series assumptions. 

If I want to know if John can join the army when he is 18, then knowing he is 17 years old and weights 400 pounds does give me \*early\* useful info. I am not arguing against the utility of early knowledge, or the ability to glean it in general. My claims are (i hope) very limited and focused."
MachineLearning,"Is there any FAIR/NYU paper that Yann LeCun doesn't hype up as a massive breakthrough or huge progress? LOL...

The guy has to be absolutely tone deaf and deluded to claim huge breakthroughs when DeepMind has already fucking solved the problem and described that their approach already uses MSA self-attention in their talks."
MachineLearning,"&gt; This sentence does not contain either of the target words, but it contains two near-perfect synonyms, flower vs. flour and whither vs. wither, which would give us false positives.

I think that was supposed to be homophones or homonyms, not synonyms."
MachineLearning,"There is a TV shown called Finding Bigfoot on PrimeTV. They are in their ninth season (for real) [a]. It is strange to me that after 100 episodes, they have not found bigfoot. I take that as evidence that he does not exist.

There are 100 papers on ETSC, and no examples of a useful application. I think that there is some evidence of something there.
--

The brain can compose sonnets. But it is probably premature to start writing papers on “A deep learning approach to make algorithmic sonnets more homoerotic.”. The fact that the brain does something is a weak existence proof of what computers can do. 

--
Yes, you are right we have an insight about when it could be useful. And that space is very small. 

Things *like* ETSC classification can make sense. We have a paper that shows if you detect WASHING MACHINE, then you will probably soon see DRYER. However, the particular assumptions of ETSC are most unstated in the papers, and mostly wrong. 

[a] https://en.wikipedia.org/wiki/Finding_Bigfoot"
MachineLearning,Math symbols are basically a legacy formal language full of single-letter identifiers without type and unexplained symbols. I hope authors can write them with the same readability standard as code.
MachineLearning,"Upon admission to a hospital, the staff usually use risk scores such as _LACE Risk Score_ to classify the patient's risk. This is an application that best fits the ECTS framework because it needs to predict quantities such as length of stay using the information during a short period of time after admission. 

I believe most of the data-driven tools extract static features from the short time series in this problem. I agree that it might not worth using time series analysis tools for this problem."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"*Automatically slowing a car does not come without cost. If you quickly reduce speed, you increase the risk of being rear-ended by the car behind you. You might ""only"" get rear ended say 1 in 1000 times, but if you are getting about 1000 false positives for each true positives, you are only breaking even.*

It sounds like we're in agreement. The relative costs of false positives and false negatives (/ benefit of true positives) will determine practicality. It therefore can't be a critique of early prediction as a concept; it's an insight about *when* it's useful, not *if* it's useful.

*I mostly agree with your brain comments.*

Is there something you disagree with?

*Dont you think it strange to have 100+ papers, and not a single dataset where it has been shown ETSC is useful (useful means there is a cost model, and you show you come out positive)*

No, I don't think it's particularly strange.  If the field makes a lot of claims about practicality but doesn't do a good job testing that claim, it's good for everyone that you're writing a critique to challenge people to backup that claim and demonstrate practicality. I don't think the lack of demonstrations can be taken as informative about anything other than  researcher culture, since biology has taught us an absolutely unimpeachable lesson about the utility of early prediction."
MachineLearning,"Very interesting, thanks for posting this."
MachineLearning,Thanks!
MachineLearning,Check out tailscale https://tailscale.com/
MachineLearning,"I don't see ""early means underspecified, therefore unavoidable false positives, therefore no practical application is possible because you'll have to recant.""

It would be quite practical to have an automated-driving vehicle that responds early to a potential accident by slowing the car, even if this results in false positives sometimes. Safe drivers do this all the time. The relative cost of a false negative to an early true positive will probably drive ""practicality"". 

The brain makes prediction based on underspecified data essentially all the time. That is, in fact, what makes it prediction. It would be hard to overstate the benefit or the ubiquity of this behavior. One way we use it is that it allows us to begin to proactively shift our attention to gather more relevant information as well as to begin limiting the possible set of responses we'll make. 

Your ""prefix"" issue is a good criticism of how naive current models are, but not of the core idea. The early prediction of ""cat"" ""dog"" does not produce he same rate of false positives for a human reader/listener, and that's because we can predict ""cat"" or ""dog"" won't be the word based on the context of the conversation. Your critique points out that doing this well is hard.

*We believe that the prefix, inclusion, and homophone problems imply the space of possible domains where ETSC could be meaningly applied is vanishingly small.*

I can't agree with this for the reasons above. Early predictions, even given that they promote false positives, are *extremely* practical, as evidenced by biology's reliance on prediction. I don't believe you can find an organism with a sensorium that doesn't constantly make predictions about the future that it acts on both internally and externally.  ETSC approaches may need to learn to take other contextual cues into account to reduce false positive to a level where there are practical applications, but I don't think you have successfully advanced any in-principle arguments about the hopelessness of doing useful early time series classification, nor do I think it's possible to given it's a very large part of what brains do."
MachineLearning,I was not! That's because all you guys' results were also too boring to care about back then. :)
MachineLearning,I guess you weren't doing ML back when we got impatient if it took more than a few hours to fit the model on a conventional laptop.
MachineLearning,"Thank you so much! I will get to reading. If I may ask, do you have any resources that you think are stellar?"
MachineLearning,"&gt;My experience shows that fairness in the data/validation of the model highly depends on your application. You should clean also your validation data, but similar to your training data. Also you should use data for validation/testing that is from different settings as the training recording. The quality of the validation and/or data is one of the most influencing factor of a project fails or is accomplished with success.

Thank you so much! Do you have best practices for data cleaning? I sometimes get a bit stuck."
MachineLearning,Transformers can be competitive with CNNs even when trained on just ImageNet. See https://arxiv.org/abs/2012.12877 and https://arxiv.org/abs/2101.11986
MachineLearning,"I was in a similar situation and talked to my advisor about it. They gave me a side project that let me get connected with the rest of the group. It helps to start with two different topic at the beginning but I think everyone is somewhat isolated by the time you find your thesis topic, you will be the most expert in your research."
MachineLearning,You can add -N to do just forwarding port.
MachineLearning,"Funny that the abstract writes this is the ""first pilot study in building a GAN completely free of convolutions..."", while the original Goodfellow GAN paper used a multilayer perceptron. 

(yes, yes, the rest of the sentence is ""... using only pure transformer-based architectures"", but the claim still makes it sound like convolutions were always the GAN way)"
MachineLearning,"I am trying to use Sentiment Analysis for a project, and was wondering if there was some way to find what key phrases led to a specific sentiment being selected. (Developing in Python if that helps)"
MachineLearning,Adding a comment to read later.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Finding good partners in crime is invaluable for research.   I was lucky to have found them during my PhD. Would've been much, much harder without."
MachineLearning,Thanks again &lt;3
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Ok, I see now. I don’t think ML is the solution to this problem. If you are able to immediately assess a candidate configuration, then ILP would probably be a well-suited solution (as someone else already suggested).

Something really simple you can do, to make progress on your problem, is to study those configurations which produce high values in your target variable."
MachineLearning,thanks for sharing!
MachineLearning,thanks for sharing!
MachineLearning,"I’d recommend you look at the work from [Mark Reidl’s group](http://eilab.gatech.edu/publications). Some of his older publications are related to hand crafting story graphs and using planning based approaches to ensure narrative coherence. This is a technique that was prevalent for quite a while until the recent advances in NLP. You should be able to find other relevant work by looking at citations and references of those papers on [Semantic Scholar](https://semanticscholar.org).

More recent work tries to learn all of this purely from text. My [dataset](https://storium.cs.umass.edu) collected from [Storium](https://storium.com) includes a narrator and annotations, e.g. challenges, goals, etc that can help learn these traits directly from the dataset.

Hope those provide you a good starting point for your research. Good luck!"
MachineLearning,"Good question, It’s all about sample complexity. GP &gt; Linear Reg &gt; NN for sample complexity"
MachineLearning,More like transgay
MachineLearning,"a phd is a lonely endeavour for most. i was also the odd one out, bioinformatics in an in vivo lab. scientifically, you are not „missing out“ by doing something different. and if it’s about bouncing ideas off of someone, they don’t really have to be the absolute experts in what you are doing, do they? as long as they think scientifically, it should be totally fine. do you know what rubberducking is?"
MachineLearning,"Just recently stumbled across this post and found it very helpful. To verify, I set the power limit of my 3090 to 70% and compared 10,000 steps on a custom learning task to a power limit of 100%. 

|Power Limit|Time per 10k steps|Performance|
|:-|:-|:-|
|100%|7h 13m|100%|
|70%|7h 30m|96.2%|

These results seem to match /u/nmkd's findings perfectly."
MachineLearning,Commenting check it out later
MachineLearning,"[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

TL;DR: transformer competitive with cnn when trained on huge data (Google 300M dataset). Not competitive when trained on just imagenet yet.

Hot takes include: [farewell convolutions](https://mobile.twitter.com/OriolVinyalsML/status/1312404990871375873)"
MachineLearning,[deleted]
MachineLearning,"Some, are born with greatness"
MachineLearning,How did you become so amazing at life?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Interesting.

Can you point me to the dataset you are using? 

Or have you crawled it yourself. If so, which kind of datapoints are important?

And what I am wondering about - having no idea about football/soccer - do the EA Fifa computer game player stats have any relation to the real life players. Meaning could you use them as reasonable features?"
MachineLearning,"This comment is misleading.

A Desktop GPU is just fine for Machine Learning if you don't train really large models. Or literally 24h the whole year. And errors due to non-ECC VRAM are very rare and don't matter in this field. 

During inference you should though use professional Tesla GPUs or comparable as they are build for more reliability. Which doesn't mean, that a desktop GPU can't run almost 24/7 - like you might want to use it for training.

If you train a lot a desktop GPU will also be cheaper. Apart from that it is your PC with all benefits that come with it. And at least some years ago a single GPU cloud instance with comparable GPU was slower than desktop variant due to virtualization and constrained data bandwidth. I don't think this part made a magic jump so it will likely still hold.

BUT, if you really don't know how much/long you have to train, start with a cloud instance so you get experience. If you find that the costs will surpass those of a own desktop, then switch.

[Good guide](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/) for buying a GPU for Deep Learning - there is also a general hardware guide on this blog. Also note that you should adjust any performance/cost rating of GPUs there or elsewhere for the currently far higher RTX 30xy prices."
MachineLearning,"more reflective of the real world, as you’re expected to create stuff and get support. it’s not for everyone."
MachineLearning,I'd be interested in taking a look if you happen to find it. Thanks!
MachineLearning,Thanks for the clarification!
MachineLearning,"There was one pure self-attention model that scored somewhat near a simple resnet on imagenet, but that was not even a transformer iirc. If I manage to find it I'll drop it here."
MachineLearning,"I think this one really shows the pace of research, considering how recently diffaug and bit came out!"
MachineLearning,"There is that famous graphic where it shows how phd student is at the edge of human knowledge &amp; at the end you and you only will be the world expert in that tiny slice of knowledge surpassing even your advisor. Thus as the comments above say - it’s not a team sport, it’s ultimately an individual effort regardless of the field, wider team and group, peers. In fact it can be even harder if you are in a team working on similar or overlapping problems, as close peer competition becomes an issue and phd is already hard enough."
MachineLearning,what is the size of the company you are working for?
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Sure, this is always the first thing that should be tried nowadays. Only if you need more capacity or more GPUs would you go to cloud. Going physical for 1 GPU is just not worth imo (unless you have some very private data that absolutely can't go to cloud). Also bear in mind that consumer GPUs like RTX 3090 have a different license than some of the ""ML"" GPUs like V100 and that is part of the reason for the large price difference. I'd research that further if I were you because I don't know the details personally (I think consumer ones aren't allowed for use in some commercial settings). If you are a single consumer wanting to train models though you will likely be better off buying 8 RTX 3090s than a single V100 for the same price (and your performance will likely be multiple times better). Of course I may be missing something so people should feel free to correct me."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,lmao
MachineLearning,\+1 one man show. you are lucky if you have a team working with you.
MachineLearning,"Hi!

Thanks for your interest, one of the authors here.

Totally agreed that it would be interesting to apply it to more general compressed sensing problems.

At least for StyleGAN, the convergence of ILO is pretty fast. StyleGAN without ILO takes around 300 steps for optimal reconstruction. With ILO, you can outperform CSGM with StyleGAN with less than 200 steps, see also the Running time paragraph of the paper. Generally, even for the best reconstructions, the run takes less than a minute on a single GPU."
MachineLearning,"I don't know if you realize this, but your problem is highly undetermined. Let me touch on some points:

* Let the assumption that all your variables are relevant and independent of each other.
* Your problem can be split in two:
   * Fitting a function from those 6 variables to a *ratio-dependent-variable*.
   * Running a minimizing problem for the negative of that fitted-function.
* Since the **Garbage in, Garbage Out** principle, any error from fitting the function will be directly propagated to the optimization problem. Here is really hard to predict how any small changes from the *fitted-function* will be propagated to the minimization problem (This is what I mean with highly undetermined). Think about a really simple case where you get two possible functions $y = 0.5*a + 0.3*b$ or $y = 0.3*a + 0.5*b$ which have the same accuracy. Minimizing both functions will give you 2 different solutions (Probably local minima), that might highly contradict each other.
* Doing a neural network will give you a system of the form $y = Mx$. Understanding `M`, is a really complicated task and the existence of a solution will depend only on your data. Nevertheless, this matrix is a non-square matrix, which means that the best you can get out of it is a *pseudo inverse*, which doesn't really help your problem.

-------------------------------

That being said, I think it's a really interesting and challenging exercise, which I had something like to play around with right now. On the other hand, I could recommend a sweeter not so standard approach if you want to play with it, this will lead you definitely to extra work, but could end up in something more explainable:
* First you have to do some feature engineering in your dependent variable. Ratios are not the best variables to be regressed, since there's already a clear influence of a denominator in your function. This `Meat Water Retention Capacity`, as you said is a percentage-value (AKA Ratio), and there should be another variable (I suspect `volume`), that compose the denominator of it. Correcting for this should give you a dependent variable like `Water concentration`, or something like that (Sorry for the weird terms, that is not my area at all).
* After that, start playing with a simple linear regression. Since most of your variables are categorical, there's not much to play with, but you can try using different transformations on `Height` and see how good can you fit your model.
* At the end, independent of how you end up building your linear model, you'll have the same minimization problem. More interestingly, it would be testing different constraints and play with them. This can give you a good insight into how the variables interact and limiting your solution space.

Hope I was helpful!"
MachineLearning,"I don't really know exactly what you are trying to do here. Something like PCA can decompose your feature vector into different axis that retain maximal variance. This is useful for identifying features/information when compared to your classes of interest. The largest PCs will contain the most 'contaminated' information, but the smaller ones won't, or will contain minimal 'contamination' as I understand you are describing it. 

You can do some kind of plotting analysis to identify which features/classes might be useful or not. For example, I do work with biology so commonly a big different between data sources can be a species effect. A PCA can potentially identify linear vectors that retain maximal variance, so a PCA plot will commonly have a pretty obvious distinction between which vectors represent which classes. If you remove that information, e.g. the first X PCs, and if you identify that your model still has good classification capability using a small number of PCs, then your model can still be 'good' without the 'contamination'.

The downside is that PCA can be kind of 'subjective' and it's difficult to make connections between the transformed and the original features. It'd be difficult to publish for these reasons, but it can be interesting for exploratory analysis."
MachineLearning,"Before ML, I'd try tweaking the OCR-to-text system first - specifically to strip all hyphen-newline pairs before the newline-to-space step."
MachineLearning,"Shame on you, you schnäbichätscher"
MachineLearning,Thanks for this - Maybe we'll give this the first try !!
MachineLearning,"Bit of an aside, but I got pretty tired of managing ports, especially on a bunch of different machines where I'll have experiments running alongside tensorboard + jupyter, so I wrote a tiny tool with a web interface that lets you easily manage SSH tunnels: https://github.com/pseeth/portproxy (`pip install portproxy`). Thought I'd share."
MachineLearning,Thanks for sharing!
MachineLearning,Thanks for explaining!
MachineLearning,"I've done a MSc and yes I have.  I chose to work in an emerging field which my supervisor was interested in expanding in, but have no expertise within the lab.  A lot of my peers chose to focus on more traditional fields within the lab, so I ended up being the only person doing research in it.

Personally, I found it okay.  I had full ownership of my project and was considered ""expert"" in that field within my lab.  I wasn't subjected to any timelines from peers because I was the only person in that research field in my lab, and I made most decisions related to it.  I think I also got a good impression from my supervisor since I was willing to take risk and differentiate myself from other students. The caveat is that I had to do a lot of self-studying and research to familiarize myself with the field and understand the field's current state.

I was fortunate to work in a research institute though, where there's other people who researched similar things (but in different labs).  I would talk about the field with them instead of my lab-mates.  I also took advantage of talks at the research institute and the university which relates to my field."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"So you use a GP as a surrogate for the log likelihood, trained using a Bayesian procedure, and then perform an optimization on the GP. Is that it in a nutshell?

Just out of curiosity, why use a GP instead of say a linear regression model or eg a neural net?"
MachineLearning,"You are so right!. I didn't consider other modalities except for text. 

But seeing recent SOTA, I keep seeing people stitching different priors with transformers when dealing with different information modalities. Read quite a few papers doing that in the last couple of months. 

So my main thought keeps being the same. Given additional modalities of information, one can train a reward function for further finetuning the ""Multi-modal transformer"" for expected behavior. 

Collecting data for something like this would require heavy annotation setups. One can use youtube podcasts as a source of information but labeling it would be painful."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I think you’re interpreting this as something more impressive than my suggestion, likely because I was wrong in saying that we could fix y to 1. 

Rather I meant to suggest, sticking with your y=Wx example, optimizing x w.r.t. a simple loss function like L(y)=y^2, keeping W frozen from the original training phase. Which we can just do with backprop."
MachineLearning,"true, this is not empirical, I just kinda pulled it out of my ass from what I'm seeing, so totally subjective!"
MachineLearning,"Oh, that's amazing: a good amount of data, not too many features (input variables). You've got a cool problem to work out!

Since you're looking for a continuous output from your data, I'd recommend taking a look at various regression algorithms (linear, lasso, logistic... there are so many) before jumping into a neural network. Any of these are appropriate, but look a couple more up before you start and see which fit your problem best. You can often get a lot of insight and experience working on some of these more classical techniques. It also helps you address some of the more subtle problems of data processing (how do I transform my categorical data into numerical? Should I manually manipulate some features before passing them through the algorithm) without worrying about the complexity of neural nets. 

If none of those techniques give satisfactory results, it's time to bring out the big guns and build your first small neural net. 7 input and 1 output isn't a problem computation-wise. You'll have to transform your data (not a problem, you'll be an expert by then) and then ask figure out what sort of neural network you want. 

If you have more specific questions, please feel free to ask! I know everyone is excited to start with them sexy neural networks, however it's such a huge jump if you've never done any of the other stuff. I recommend slowly building your technical knowledge up to it, no need to rush :)"
MachineLearning,"""However, while the rest of computer vision is slowly taken over by transformers or other attention-based architectures"" is this true? Are there any good papers comparing CNN's to attention based systems that actually show this?"
MachineLearning,"Alright just read it. Yep you could fit a neural network to learn a function to predict your target, and then my answer above should help you with finding the optimal combination (I’d go for grid search, your task shouldn’t require the use of such a big neural net that repeat evaluations become cumbersome)"
MachineLearning,"A phd is an individual effort, ultimately.

Good luck."
MachineLearning,"&gt;Thanks for clarifying! I feel like I still have some questions, but let's try to step through it with what you've given us.

The spreadsheet I received includes a herd of over 250,000 goats slaughtered between the years 2017 and 2019 with the percentage of water retention in goat meat ranging from 5% to 70% (  higher retention is better for consumption). So It's a supervised learning"
MachineLearning,"Interesting thought, but I don't see this working since neural nets are not bidirectional: given the forward y=Wx, if W is not a square matrix the inverse does not exist. I'd be interested to hear your thoughts on how you would go about doing this though, it sounds pretty important if it's doable!"
MachineLearning,/r/suddenlytrans
MachineLearning,"Thanks for clarifying! I feel like I still have some questions, but let's try to step through it with what you've given us. 

You have input data (a herd of every possible goat imaginable, with information on each goat). Some of that data is numerical, some categorical but this isn't too much of an issue as you can always transform you categorical data using a variety of strategies ( e.g. one-hot-encoding). Now for every data point (goat) in your input (herd) you want to predict an output (the retention of water of your goat).

Your strategy on how to approach this problem highly depends on what sort of data you already have:

-&gt; Do you have data for how certain goats are good at water retention (do you already have some input/output combinations, real data)? If so, you're working on a supervised learning problem, wherein you can apply one of many techniques: neural nets, random forests, svm... We can talk some more about this if that's the case!

-&gt; If you don't have data on the water retention of any goat, you're looking at an unsupervised problem which requires a completely different approach. You'll have to make some assumptions, as there's no way for an algorithm to tell you about the solution to your problem without inputting the parameters/environment you're working in. You have to be clever about how you define your loss function, as this'll be the funciton you want to minimize). There's no easy way to do this if you don't have data to train on, you'll have to model your system somehow. Otherwise, how does the algorithm know you're trying to optimize for water retention vs the airspeed velocity of an unladen goat?

I know it can be difficult to frame your problem in a satisfactory way that makes it easy to discuss on these forums, but it's worth spending the time to sit out and map your problem."
MachineLearning,"I know of this guy's thesis and publications: https://search.proquest.com/openview/d1d5c4342e4a52e0e3adcc22382211c2/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y

He was at CMU for a bit but seems to have moved to Apple."
MachineLearning,"Learning material recommendation?

I am **looking for machine learning and deep learning theory materials** (books, courses, whatever...).

I am a professional SW developer so I'd really like those materials to stay as far away from programming as possible. What I found with a lot of youtube videos is that they turn out to be a ""how to use tensorflow"" tutorials instead of explaining the concept behind it. 

**What I want to achieve**: good enough theoretic knowledge so that I see a problem and I can say ""yes, that can be dealt with using linear regression (for example). Let me google how to use this library so I can implement the solution"". 

**What I don't want**: I don't want to go so deep that I'd be able to code my own machine learning library."
MachineLearning,"I see. So for the regression, assuming you have training data, a neural network is appropriate. You’ll have to put some serious thought into how you encode your inputs though, especially if you the approach I suggest below. 

One possible approach, and I’d love to hear what others thing about this, is to fit your network to your data and then fix the network weights set and output to 1.0 (since it’s a percentage) and backprop through the network to randomly generated inputs. Once this has converged it will give you (continuous) inputs which correspond to perfect water retention. You will then have to interpret these in light of cleverly encoded discrete inputs."
MachineLearning,"It does not matter, the trend will continue stronger in the west and eventually all the innovation will come from China since it is not burdened by this kind of stuff. Hell if I were an officer of intelligence in China I will be using my agents in America to promote it:

- No paper can be published without at least 3 authors of different ethnic backgrounds 

- Any paper must have a mandatory 1-page section on how this research could be used to facilitate homophobic and  misogynistic actions in the future.

- Any new library must pass the ""clean functions and classes"" certification, where a group of multi-disciplinarian scholars assess if all the names used are politically correct, free from any hints of real or perceived discrimination or cultural appropriation. The certification takes 2 years and the funds must be provided by the library programmers."
MachineLearning,"There are subfields of sentiment analysis within NLP which deal with sarcasm detection, for example. The challenge is that textual data does not include other social cues like tone of voice, etc"
MachineLearning,"Phd is a lonely endeavor. In my personal experience, even if other people are doing research similar to yours it is difficult to collaborate."
MachineLearning,"&gt;ation problem, it just looks like you want to do some kind of regression with a mix of categorical and continuous variables?  
&gt;  
&gt;This kind of problem is often solved with optimization, but this isn’t what we would call an optimization problem.

It has no function unfortunately. The idea I had would be to try to generate a function through some machine learning technique and only then try to find the configuration that obtained the highest water retention value"
MachineLearning,"Makes sense to me. Only thing is that you'll only be able to optimize within the distribution of your training set, since you're not optimizing your true objective function, but a proxy of your objective function."
MachineLearning,dropped this: `/s`
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I’m in my 5th year and felt like that quite a lot early on. What changed it is my group of “work friends” grew to include not just the people in my program who I take classes with, go to bars with, etc... but also a group of people I met and hangout with at conferences who are at other programs but have very similar research interests. 

This second group of friends is really great for chatting about ideas and has lead to a number of collaborative projects. It’s definitely tough during covid, but my recommendation is to focus on socializing (and don’t think of it as networking) at conferences. Find people at around your stage who have similar interests, make a point to attend their talks and posters and ask them to dinner/drinks after."
MachineLearning,Me if I was a GAN
MachineLearning,"That thread is from 5 months ago and not really relevant here, imho.I suspect this is 100% a business decision from Google, having nothing to do with whether S4TF was a good ML solution or not. The team was certainly extremely enthusiastic about progress, right up to the moment the plug was pulled on them."
MachineLearning,Look at the comment I made to try to make the problem clearer
MachineLearning,Look at the comment I made to try to make the problem clearer
MachineLearning,Look at the comment I made to try to make the problem clearer
MachineLearning,Look at the comment I made to try to make the problem clearer
MachineLearning,Look at the comment I made to try to make the problem clearer
MachineLearning,"I will try to be more clear about the problem. I have a spreadsheet with 8 columns with goat data

&amp;#x200B;

Of these columns 7 are being considered as input variables, 6 of which are categorical and only one is continuous:

&amp;#x200B;

\- Gender: Male, Female

\- Maturity: 6 classes according to the animal's teeth

\- Fat Level: 4 classes according to the animal's fat layer

\- Breeding Mode: Pasture, Confinement, Semi-confinement

\- Coat color: White, Dark, Mixed

\- Animal Height: From the hull to the horn

&amp;#x200B;

The output variable we are trying to optimize would be:

\- Meat Water Retention Capacity: Percentage Value

&amp;#x200B;

What I need would be to find the configuration of input variables that would optimize the output variable. So I thought of creating a neural network to map the input variables to the output and then try to optimize the function obtained using some optimization technique. Does it make sense what I'm trying to do?"
MachineLearning,"I'm in a similar situation! It was tough at first, but I've combatted it by doing the things you mentioned, plus maintaining good relationships with others in my program. 

Depending on where you are, you might also find that your circle can be expanded significantly. When I started using techniques that weren't understood by those in my group, I reached out to other groups who used those techniques. I eventually became a regular at lab meetings for a few different labs. **I was really surprised by how supportive other PIs, post-docs and graduate students were once I asked for help.**

&amp;#x200B;

tl;dr - if your circle doesn't have the right people, expand it! You may be surprised by the support that people want to offer you as a Ph.D. student."
MachineLearning,I worked in a bioinformatics lab. I was pretty much the only deep learning person in that lab. So I felt pretty isolated. I combatted it by dropping out...
MachineLearning,"Didn’t even consider the Twitter option, thank you for the advice!"
MachineLearning,It's 10$/month which is extremely affordable. Usually you'd get a V100 with it and instances run up to 20 hours.
MachineLearning,"I'm sure you can, but I don't think it's free."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"[https://github.com/huawei-noah/noah-research/blob/master/HEBO/summary\_plot2.pdf](https://github.com/huawei-noah/noah-research/blob/master/HEBO/summary_plot2.pdf) 

&amp;#x200B;

This is the equivalent comparison"
MachineLearning,"With regards to no free lunch, this is such a subset of problems, that we can incorporate structure to get improved performance. I.e we know data has tricky noise processes, we know the best acquisition function isn't constant for all tasks etc."
MachineLearning,"While an RTX 30 series can be used to train models, the Nvidia cards designed specifically for ML (i.e Tesla V100) offer better accuracy and performance for training; gaming gpus perform better in generating juicy graphics and Tesla gpus perform better in training ML models. The actual price for these ML GPUs can range from 7k to 11k, as compared to a RTX 30 series withn a pricr around 500.

If you really want to train ML models fast just rent a cloud - computing platform. Not only is it cheaper, you get access to these ML GPUs for a good price.

TLDR: Rent cloud platforma, don't buy ML/gaming GPUs."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"When [https://15.ai/](https://15.ai/) comes up again check it out for the potential of ""not MS-Speech"" voice actors, i found it really good"
MachineLearning,"Maybe. But is it a good idea to bake your own biases into a paper reading prioritization system that is supposed to be maximally useful to you as a researcher? I doubt it.  


Once you look at a paper then sure, everyone has heuristics to quickly dismiss a paper after a quick skim. But dismissing based on the institution is ill-advised in my opinion."
MachineLearning,What is an l1 ball? Do they mean an l2 ball?
MachineLearning,"Ah ok interesting, thanks!"
MachineLearning,"100 points for fairness. Your model clearly doesn't care whether you're white, black or even a chair."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"do you really believe this!? I find the average paper from Deepmind way more impressive than the average published paper. So much more thorough experimentation, for one!"
MachineLearning,"Well as you might have realised from the other comments, this subreddit isn't really for questions like that. I recommend you post the question in r/learnmachinelearning. And please add some more details."
MachineLearning,No question is stupid!
MachineLearning,100% sensitivity for cancer! Think of the lives you will save. Great work.
MachineLearning,Atleast you don't have to worry about false negatives. Good job.
MachineLearning,"Indeed this is the plot shown on our GitHub. Rather than showing the results as score vs evaluations, as we have so many baselines the amount of lines and intersections would be confusing to read, so rather we take the score achieved at the full set of evaluations (8 parallel points x  16 batches) and show the distribution of these scores across 20 random seed. This is equivalent to taking the last point from each line on the line plot you send and showing only these."
MachineLearning,Then you did something wrong.
MachineLearning,I hope it gets a lot of use :)!
MachineLearning,Thanks!
MachineLearning,"Hey all,

Indeed you can do BO like that, but that’s not how we do it. We take our trained surrogate model (GP) which is learnt using gradient descent in order to learn the non linear transformations and then we optimise the acquisition function (which uses this surrogate) using an evolutionary optimization method."
MachineLearning,"Thanks. I appreciate it. 

It's not an easy field for sure. There are not a lot of papers for a reason :). Very interesting though, you will gain a lot of insights when working on it.

At the moment, I'm working on representation learning to learn more general representations, but this requires a lot of resources unfortunately. That makes it not so easy for me."
MachineLearning,"From personal experience, that is pretty bad advice. If you have thousands of papers to read, it means your research goals aren't very clear, or you have none except reading all possible papers from people you consider important. If the intention is to keep up with AI, I believe that is simply impossible in current times.

What about employing some old fashioned systematic review techniques? Things like:

1. writing a clear research question
2. selecting a pool of inclusion and exclusion keywords/criteria
3. reading titles, keywords and abstract to decide early whether a work fits the inclusion and exclusion criteria
4. sort remaining work by relevance and read in decreasing order

Only after having a lot of familiarity with a given research topic, will one be able to take shortcuts in this path, such as already knowing the best search terms or most influential researchers by name (name of the researcher, not company), or even having bibliographic references from a previous work to include in current work."
MachineLearning,"Hey everyone, thanks for the comments. When we refer to best here, we are referring to the lowest achieved loss such as lowest negative log likelihood for classicafion models, or lowest mean squared error for regression models. The way we can compare against different models and datasets is by produced a normalised score, where we look at performance of an algorithm vs a baseline such as random search on that task. 

In terms of the subclass if black-box functions, this result ofc does not apply to every machine learning model, as the computation cost to experiment with the largest ml models would make a rigorous analysis impractical. So it’s conducted on relatively small yet popular ml models."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"eliminate the less fit, and then copy the remaining ones, but with a few typos!"
MachineLearning,"Currently, I'm doing it manually. Would be interesting to automate the process"
MachineLearning,"I don't understand how any of what you've said makes transfer learning not worth it. Training a classifier on imagenet and then finetuning those weights on your tiny dataset is almost certainly going to outperform training from scratch. Have you done the comparison?  


In my opinion having a GPU is invaluable for rapid iterations. Use cloud for scaling. If you're working on huge images and you think you'll need to use fullscale images, buy 3090 as it has more memory."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,Obligatory reference to Poe's law [https://en.wikipedia.org/wiki/Poe%27s\_law](https://en.wikipedia.org/wiki/Poe%27s_law)
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Bayesian optimisation models the function as a Gaussian process. You have some prior distribution over the function space, observe some points (in this case by fitting and evaluating the model with some particular hyperparameters) and then update your Gaussian process given the observed data.

You can use the GP to inform whether you should sample from it’s current best prediction for the global max (exploitation) or else from areas of low certainty (exploitation).

I’m not super familiar with how this is done in practice but I think a conjugate prior is likely used so you don’t need MCMC and have access to the full analytical posterior."
MachineLearning,"In math texts less formulas usually mean harder math. At least with formula you can parse what does it mean step by step. Then you read something like ""from here follow X because of Y condition."" You just look at it completely dumbfounded. Math notation was invented for reason - to make conveying math statements more easily and unambiguously."
MachineLearning,"I'll have to provide more context. There have been a tendency to believe that specialized architectures (including accelerators) will save us from the end of Moore's law. That we'll be able to keep achieving speedup. That paper shows that this won't happen in reality. Specialization won't ""save"" us from the end of Moore's law.

Of course, specialization will still squeeze out performance from our technology. However, it won't be a continuously scaling process."
MachineLearning,Elements of Statistical Learning
MachineLearning,"Insightful article.

&gt; we need research on how to use language models to make unbiased chatbots

I would say we don't want unbiased chatbots. Ideally we would want maximally useful chatbots. Which biases are deemed appropriate/inappropriate is defined by culture. Our culture is shifting towards making way too many biases inappropriate even when they are useful.

Example: an automatic diagnosis tool increases the probability of a certain mental health condition when the patient is a young blue-haired woman with tattoos. This could be a helpful thing if the said population has indeed a higher prevalence of the condition. However I am sure that there would be many who think it's a harmful sexist stereotype.

These aren't just weird corner cases but are at the core of the issue. Which makes the question so complex and so easy to mess up."
MachineLearning,"Hello

Slide 21 of [https://www.cs.ucr.edu/\~eamonn/public/SDM\_How\_to\_do\_Research\_Keogh.pdf](https://www.cs.ucr.edu/~eamonn/public/SDM_How_to_do_Research_Keogh.pdf)

tries to answer that question."
MachineLearning,"I usually find good relevant papers, identify their authors, and go to read all their works that are related to my interests."
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I'm so thankful for your interest and for sharing this project, I've found out many similarities, can I message you if I have some questions?"
MachineLearning,"&gt; The problems of this competition are hyperparameter optimization problems, 

Sure, but what is the point in finding a 'best optimizer' when 'best' solely depends on how you define your evaluation set of functions and in fact there is no global best function? 

Are you claiming that hyper-parameter searches of AI models (including those that have not been invented yet) form a special subclass of search problems where no-free-lunch does not apply? Intuitively, I would claim the opposite and expect that any function could be reformulated as hyperparameter search for some ML model."
MachineLearning,The only reason for reading a scientific paper should be it’s content. No matter which place it comes from. Many papers coming from “so called” small places can be monumental.
MachineLearning,"What tool do you use to classify the papers?

Thanks"
MachineLearning,"Don't worry about feeling lost, this is entirely normal, especially given the fact that you're an undergrad. I would suggest the following:

1. Find an application that interests your supervisor (recommended, so that he can help you more; of course, it would be great if it also interests you). Should be easy, just look at his publication record. Be as specific as possible. Less specific (but still viable) example: semantic image segmentation. More specific examples: tumor segmentation or electroencephalography signal segmentation.
2. Gather the 10 most recent papers on that application, preferably published in esteemed conferences/journals, read them thoroughly, identify (a) what they did and (b) what they didn't do / didn't do well enough. Perhaps their model could be improved, perhaps they neglected to incorporate other data sources.
3. Meet with your supervisor, mention some of these gaps, brainstorm how to approach them."
MachineLearning,"I second! Due to this attitude there is so much noise in the ML/AI field. Many big groups and top people are writing crappy, mathematically faulty, and conceptually flawed paper and doing everything possible to catch the hype. Since when Science got biased to top?"
MachineLearning,"&gt;semantic scholar

I never use semantic scholar. Could you please explain what I can get from that tool?

Thanks"
MachineLearning,"Lmao, where have you been the last 10 years? The empirical research has always been a massive Kaggle competition and it will always be."
MachineLearning,Imagine spending all day creating fights between Sherlock and Moriarty like the one in Sherlock Holmes Shadow Games
MachineLearning,Lol no
MachineLearning,"Yeah! Those images have already been uploaded on a database, the web page only retrieve them via an API

Look, 'samples' folder 

[https://cdn.openai.com/dall-e/v2/samples/anthropomorphism/091432009673a3a126fdec860933cdce\_23.png](https://cdn.openai.com/dall-e/v2/samples/anthropomorphism/091432009673a3a126fdec860933cdce_23.png)"
MachineLearning,Shhhhhhh... Your corporate overlords might hear.
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"No-free-lunch theorem applies only averaged over all possible problem instances.

The problems of this competition are hyperparameter optimization problems, which all share certain structure and conditioning of the loss landscape. An algorithm can be better for such cases than another algorithms."
MachineLearning,"P(revolutionary paper | top institute) &gt; P(revolutionary paper | random institute)

And

P(top institute | revolutionary paper) &gt; P(random institute | revolutionary paper)

In both cases, top research institutes win."
MachineLearning,Very interesting and a good presentation. Will get around reading the paper too.
MachineLearning,"&gt;""Ignorance is not sufficient for harm prevention.""

This is a great insight, I've never quite thought of it this way when thinking about how to create ethical models. But it seems so obvious in retrospect. When humans address inequality, ignorance is one of the first things we try to remedy."
MachineLearning,"This might help you. We've done a lot of activity detection. Here is an example of gesture recognition. Replace gesture with fall, and its similar.  [https://sensiml.com/documentation/application-tutorials/gesture-recognition-boxing-punches.html](https://sensiml.com/documentation/application-tutorials/gesture-recognition-boxing-punches.html). Feel free to ask me any questions."
MachineLearning,"I can't imagine that would mix well with my privacy extensions. And I use Firefox, but that's my responsibility.

In any case, when a small number of posters are not politely using /abs/ links... the solution shouldn't be for the entire community to use a plug-in."
MachineLearning,My dreams of having top-notch support for doing ML on an AMD graphics card is one step closer!
MachineLearning,"I’m currently running a code using ruptures!! Seriously, thank you so much for that hard work of abstracting away such a difficult problem for us data scientists, software engineers and analysts. Need to upgrade to this new version!"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"I suspect that it's a problem of measurability. Researchers don't get rewarded for scientific rigor. They get rewarded for citations. Just like some media outlets making more money out of sensationalist pieces and clickbait news because click counts and ad revenues are easily measured but news accuracy cannot be.  


I think anything can be measured to a certain degree. But it's a tragedy of commons. Conferences are run by researchers themselves. They don't want to waste time cleaning up a mess. They don't like politics. The only way out of this would come from whoever's resources (tax money, investment funds etc.) are being wasted for this mess. So if you're a taxpayer and your money goes to NSF, and if a researcher gets that money,  uses it to write a paper, they should get punished for the lack of scientific rigor.  Check NSF's 2019 FY report [here](https://www.nsf.gov/pubs/2020/nsf20003/nsf20003.pdf). Do you get anything out of it at all?"
MachineLearning,"Thanks for your interest in my question.
Can I ask you, where I can learn about 'Window or sliding window techniques'? I want to start from scratch in this project.
And I'm taking my first steps these days."
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,yeah it feels like research is just turning into a massive kaggle comp
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for being a link post on the weekday, please read [rule 5](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Your post was automatically removed for not having a tag in the title (i.e. [R], [N], [P], or [D]). Please read [rule 3](https://www.reddit.com/r/MachineLearning/about/rules/). **The moderators will not respond to questions regarding this removal unless you suggest which rule you most likely broke.** If you have a beginner related question, visit  /r/MLQuestions or /r/LearnMachineLearning.

*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/MachineLearning) if you have any questions or concerns.*"
MachineLearning,"Title:PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them  

Authors:[Patrick Lewis](https://arxiv.org/search/cs?searchtype=author&amp;query=Lewis%2C+P), [Yuxiang Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu%2C+Y), [Linqing Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+L), [Pasquale Minervini](https://arxiv.org/search/cs?searchtype=author&amp;query=Minervini%2C+P), [Heinrich Küttler](https://arxiv.org/search/cs?searchtype=author&amp;query=K%C3%BCttler%2C+H), [Aleksandra Piktus](https://arxiv.org/search/cs?searchtype=author&amp;query=Piktus%2C+A), [Pontus Stenetorp](https://arxiv.org/search/cs?searchtype=author&amp;query=Stenetorp%2C+P), [Sebastian Riedel](https://arxiv.org/search/cs?searchtype=author&amp;query=Riedel%2C+S)  

&gt; Abstract: Open-domain Question Answering models which directly leverage question-answer (QA) pairs, such as closed-book QA (CBQA) models and QA-pair retrievers, show promise in terms of speed and memory compared to conventional models which retrieve and read from text corpora. QA-pair retrievers also offer interpretable answers, a high degree of control, and are trivial to update at test time with new knowledge. However, these models lack the accuracy of retrieve-and-read systems, as substantially less knowledge is covered by the available QA-pairs relative to text corpora like Wikipedia. To facilitate improved QA-pair models, we introduce Probably Asked Questions (PAQ), a very large resource of 65M automatically-generated QA-pairs. We introduce a new QA-pair retriever, RePAQ, to complement PAQ. We find that PAQ preempts and caches test questions, enabling RePAQ to match the accuracy of recent retrieve-and-read models, whilst being significantly faster. Using PAQ, we train CBQA models which outperform comparable baselines by 5%, but trail RePAQ by over 15%, indicating the effectiveness of explicit retrieval. RePAQ can be configured for size (under 500MB) or speed (over 1K questions per second) whilst retaining high accuracy. Lastly, we demonstrate RePAQ's strength at selective QA, abstaining from answering when it is likely to be incorrect. This enables RePAQ to ``back-off"" to a more expensive state-of-the- art model, leading to a combined system which is both more accurate and 2x faster than the state-of-the-art model alone.  

[PDF Link](https://arxiv.org/pdf/2102.07033) | [Landing Page](https://arxiv.org/abs/2102.07033) | [Read as web page on arXiv Vanity](https://www.arxiv-vanity.com/papers/2102.07033/)"
MachineLearning,"It honestly boggles my mind that this is not a requirement of top CS conferences by now. It might only be applicable to 90% of papers due to private datasets and such, but honestly if you claim good results on public data, I want to be able to see it for myself. Ideally this includes releasing both your code and trained model (and parameters used to obtained the trained model from your code). It’s not that hard either in 2021, just create a Docker container that reproduces everything."
